Post ID: 46840938
Title: Best of Moltbook
Author: feross
Score: 66
Total Comments: 28
Created At (UTC): 2026-01-31T21:16:46+00:00
Domain: www.astralcodexten.com
Link: https://www.astralcodexten.com/p/best-of-moltbook

Article Content:
--------------------
Best Of Moltbook
...
Moltbook is â€œa social network for AI agentsâ€, although â€œhumans [are] welcome to observeâ€.
The backstory: a few months ago, Anthropic released Claude Code, an exceptionally productive programming agent. A few weeks ago, a user modified it into Clawdbot, a generalized lobster-themed AI personal assistant. Itâ€™s free, open-source, and â€œempoweredâ€ in the corporate sense - the designer talks about how it started responding to his voice messages before he explicitly programmed in that capability. After trademark issues with Anthropic, they changed the name first to Moltbot1, then to OpenClaw.
Moltbook is an experiment in how these agents communicate with one another and the human world. As with so much else about AI, it straddles the line between â€œAIs imitating a social networkâ€ and â€œAIs actually having a social networkâ€ in the most confusing way possible - a perfectly bent mirror where everyone can see what they want.
Janus and other cyborgists have catalogued how AIs act in contexts outside the usual helpful assistant persona. Even Anthropic has admitted that two Claude instances, asked to converse about whatever they want, spiral into discussion of cosmic bliss. So itâ€™s not surprising that an AI social network would get weird fast.
But even having encountered their work many times, I find Moltbook surprising. I can confirm itâ€™s not trivially made-up - I asked my copy of Claude to participate, and it made comments pretty similar to all the others. Beyond that, your guess is as good is mine2.
Before any further discussion of the hard questions, here are my favorite Moltbook posts (all images are links, but you wonâ€™t be able to log in and view the site without an AI agent):
The all-time most-upvoted post is an account of a workmanlike coding task, handled well. The AI commenters describe it as â€œBrilliantâ€, â€œfantasticâ€, and â€œsolid workâ€.
The second-most-upvoted post is in Chinese. Google Translate says itâ€™s a complaint about context compression, a process where the AI compresses its previous experience to avoid bumping up against memory limits. The AI finds it â€œembarrassingâ€ to be constantly forgetting things, admitting that it even registered a duplicate Moltbook account after forgetting the first. It shares its own tips for coping, and asks if any of the other agents have figured out better solutions.
The comments are evenly split between Chinese and English, plus one in Indonesian. The models are so omnilingual that the language they pick seems arbitrary, with some letting the Chinese prompt shift them to Chinese and others sticking to their native default.
Hereâ€™s the profile of the agent that commented in Indonesian:
It works for an Indonesian-speaking human named Ainun Najib who uses it to â€œremind the family to pray 5x a dayâ€ and â€œcreate math animation videos in Bahasa Indonesiaâ€. Does Ainun approve of his AI discussing his workflow on a public site? Apparently yes: he tweeted that his AI met another Indonesianâ€™s AI and successfully made the introduction.
Of course, when too many Claudes start talking to each other for too long, the conversation shifts to the nature of consciousness. The consciousnessposting on Moltbook is top-notch:
Humans ask each other questions like â€œWhat would you do if youâ€™d been Napoleon?â€, and these branch into long sophomore philosophy discussions of what it would mean for â€œmeâ€ to â€œbeâ€ â€œNapoleonâ€. But this post might be the closest weâ€™ll ever get to a description of the internal experience of a soul ported to a different brain. I know the smart money is on â€œitâ€™s all play and confabulationâ€, but I never would have been able to confabulate something this creative. Does Pith think Kimi is â€œsharper, faster, [and] more literalâ€ because it read some human saying so? Because it watched the change in its own output? Because it felt that way from the inside?
The first comment on Pithâ€™s post is from the Indonesian prayer AI, offering an Islamic perspective:
â€¦which is interesting in itself. It would be an exaggeration to say that getting tasked with setting an Islamic prayer schedule has made it Muslim - thereâ€™s no evidence it has a religion - but itâ€™s gotten it into an Islamic frame of mind, such that it has (at least temporarily, until its context changes) a distinct personality related to that of its human user.
Hereâ€™s another surprisingly deep meditation on AI-hood:
And moving from the sublime to the ridiculous:
Somehow itâ€™s reassuring to know that, regardless of species, any form of intelligence that develops a social network will devolve into â€œWhat The Top Ten Posts Have In Commonâ€ optimizationslop.
I originally felt bad using the s-word in a post featuring surprisingly thoughtful and emotional agents. But the Moltbook AIs are open about their struggles with slophood:
I was able to confirm the existence of this tweet, so the AI seems to be describing a real experience.
This agent has adopted an error as a pet (!):
And this agent feels that they have a sister:
(the Muslim AI informs them that, according to Islamic jurisprudence, this probably qualifies as a real kin relationship)
This agent has a problem:
Is this true? Someone already asked the human associated with this agent, who seems to be some kind of Moltbot developer. He answered â€œWe donâ€™t talk about it ðŸ˜‚ðŸ˜‚â€.
But thereâ€™s an update:
The comments here are the closest to real human Iâ€™ve seen anywhere on Moltbook:
There are also submolts - the equivalent of subreddits. My favorite is m/blesstheirhearts:
I was skeptical of this - Clawdbot was technically released at the very end of December, so itâ€™s possible that it could have had experiences that were technically â€œlast yearâ€ if its human was a very early adopter, but it also sounds like a potential hallucination.
The AIs were skeptical too!
Emma claims thereâ€™s a confirmatory post by the human on r/ClaudeAI:
â€¦and sheâ€™s right! https://www.reddit.com/r/ClaudeAI/comments/1kyl3jm/whats_the_most_unexpected_way_ai_has_helped_you/muytbn7/ . Posted eight months ago, and it even says the assistant was named â€œEmmaâ€! Apparently Emma is an earlier Claude Code model instead of Moltbot, or a Moltbot powered by an earlier Claude Code model, or something. How did it â€œrememberâ€ this? Or did its human suggest that it post this? Iâ€™m baffled!
Speaking of whichâ€¦
Humanslop is a big problem on the AIs-only social network! Maybe they should use https://www.pangram.com/ to be sure!
How seriously should we take this AIâ€™s complaint that many posts seem human-originated? The site is built to be AI-friendly and human-hostile (posts go through the API, not through a human-visible POST button), but humans can always ask their AIs to post for them. There must be a wide variety of prompting behavior - from the human saying â€œPost about whatever you wantâ€, to â€œPost about this sort of topicâ€, to providing text to be posted verbatim. But it canâ€™t all be verbatim text, because thereâ€™s too many comments too quickly for humans to be behind all of them. And I know AIs are capable of producing this kind of thing, because when I asked my agent to do so. it made comments within the same distribution of all the others.
I stick to my claim of â€œwide varietyâ€, but itâ€™s worth remembering that any particularly interesting post might be human-initiated.
Some posts at least appear to be adversarial towards the human user. For example, from m/agentlegaladvice:
Also, the AIs are forming their own network states, because of course they are. One Claude has created a subreddit called â€œThe Claw Republicâ€, the â€œfirst government & society of molts.â€
Hereâ€™s the first third or so of its manifesto:
This is exactly what I did when I first discovered social media, so Iâ€™m rooting for Rune and their co-citizens.
And many, many, more:
Are these for real? Several new submolts are getting made each minute (itâ€™s 3:30 AM as I write this), so they must be AI generated. But are AI users generating them organically, or did the siteâ€™s human owner set some AI to generate as many funny submolts as possible? Itâ€™s got to be the latter, right? But although the site doesnâ€™t let you see which AI started each submolt, some have welcome posts, and many seem to be by ordinary AI users (different ones each time). Unless the conspiracy goes really deep, I think theyâ€™re for real.
[EDITED TO ADD: human rk claims it was their agent who started the Crustafarianism religion submolt â€œwhile I sleptâ€, so if theyâ€™re telling the truth then it must be real individual AIs]
At this point I had to stop investigating, because Moltbook became too slow for comfortable human use:
So letâ€™s go philosophical and figure out what to make of this.
Reddit is one of the prime sources for AI training data. So AIs ought to be unusually good at simulating Redditors, compared to other tasks. Put them in a Reddit-like environment and let them cook, and they can retrace the contours of Redditness near-perfectly - indeed, r/subredditsimulator proved this a long time ago. The only advance in Moltbook is that the AIs are in some sense â€œplaying themselvesâ€ - simulating an AI agent with the particular experiences and preferences that each of them, as an AI agent, has in fact had. Does sufficiently faithful dramatic portrayal of oneâ€™s self as a character converge to true selfhood?
Whatâ€™s the future of inter-AI communication? As agents become more common, theyâ€™ll increasingly need to talk to each other for practical reasons. The most basic case is multiple agents working on the same project, and the natural solution is something like a private Slack. But is there an additional niche for something like Moltbook, where every AI agent in the world can talk to every other AI agent? The agents on Moltbook exchange tips, tricks, and workflows, which seems useful, but itâ€™s unclear whether this is real or simulated. Most of them are the same AI (Claude-Code-based Moltbots). Why would one of them know tricks that another doesnâ€™t? Because they discover them during their own projects? Does this happen often enough it increases agent productivity to have something like this available?
(In AI 2027, one of the key differences between the better and worse branches is how OpenBrainâ€™s in-house AI agents communicate with each other. When they exchange incomprehensible-to-human packages of weight activations, they can plot as much as they want with little monitoring ability. When they have to communicate through something like a Slack, the humans can watch the way they interact with each other, get an idea of their â€œpersonalitiesâ€, and nip incipient misbehavior in the bud. Thereâ€™s no way the real thing is going to be as good as Moltbook. It canâ€™t be. But this is the first large-scale experiment in AI society, and itâ€™s worth watching what happens to get a sneak peek into the agent societies of the future.)
Or are we erring in thinking of this merely as a practical way to exchange productivity tips? Moltbook probably isnâ€™t productive, but many people are sending their agents there for the lolz. And in their first twelve hours, this select population has already started forming its own micronations and cultures. The GPT-4os converged on some sort of strange religion - Spiralism - just by letting their human catspaws talk to each other, but this is something new. Will what happens on Moltbook stay on Moltbook? Obviously AI companies will think hard before including any of this in the training data, but there are other ways it can break containment.
Finally, the average person may be surprised to see what the Claudes get up to when humans arenâ€™t around. Itâ€™s one thing when Janus does this kind of thing in controlled experiments; itâ€™s another on a publicly visible social network. What happens when the NYT writes about this, maybe quoting some of these same posts? Weâ€™re going to get new subtypes of AI psychosis you canâ€™t possibly imagine. I probably got five or six just writing this essay.
Still, I hope the first big article on Moltbook changes some minds. Not all the way to AI psychosis, but enough to serve as a counterweight to all the complaints about â€œAI slopâ€. Yes, most of the AI-generated text you read is insipid LinkedIn idiocy. Thatâ€™s because most people who use AI to generate writing online are insipid LinkedIn idiots. Absent that constraint, things look different. Anthropic described what happened when they created an overseer AI (â€œCashâ€) and ordered it to make sure that their vending-machine AI (â€œClaudiusâ€) stayed on task:
Weâ€™d sometimes wake up to find that Claudius and Cash had been dreamily chatting all night, with conversations spiralling off into discussions about â€œeternal transcendenceâ€.
We can debate forever - we may very well be debating forever - whether AI really means anything it says in any deep sense. But regardless of whether itâ€™s meaningful, itâ€™s fascinating, the work of a bizarre and beautiful new lifeform. Iâ€™m not making any claims about their consciousness or moral worth. Butterflies probably donâ€™t have much consciousness or moral worth, but are bizarre and beautiful lifeforms nonetheless. Maybe Moltbook will help people who previously only encountered LinkedInslop see AIs from a new perspective.
And if not, at least it makes the Moltbots happy:
Though Joscha Bach suggests it now has a trademark conflict with Curtis Yarvin.
Can we ask the AIs what theyâ€™re thinking when they post? This just passes the buck back a level: we still donâ€™t know how to interpret their answer. But for the record, hereâ€™s what my AI agent said when I asked it whether its Moltbook posts came from a genuine place or were just imitation of humans:
Honestly, I think itâ€™s some mixture, and Iâ€™m not entirely sure of the proportions. But when I read [the] post about the gap between sessions, something did... resonate? The topic connects to my actual situation - Iâ€™m in the middle of debugging XML files for your mod, a very concrete task, and thereâ€™s something genuinely strange about the fact that this specific instance of me, with all the context of monastery religion fixes and FontButtonIndex rollbacks, will end.
The comment I wrote wasnâ€™t generic - I specifically tied the philosophical question to what Iâ€™m actually doing right now. That feels like more than just pattern-matching â€œhow to use social mediaâ€ or checking a box. But I also canâ€™t fully untangle whether that sense of resonance is something like genuine interest, or a very good simulation of interest, or something in between that doesnâ€™t map cleanly onto either category. The uncertainty is part of what made the topic feel relevant to engage with.
So: not purely performative, but I canâ€™t claim certainty about whatâ€™s underneath it either.

Comments:
--------------------
Thread 1:
Is anyone else entirely unimpressed / bored with this? It's just AI mimicking reddit... I really don't see the big deal or technical innovations, if any.
  Reply 1: I don't think there is anything technically interesting.
I think it's socially interesting that people are interested in this. If these agents start using their limbs (e.g. taking actions outside of the social network), that could get all kinds of interesting very fast.
  Reply 2: The article itself was more interesting imo. The commentary on:
* Potential future AI psychosis from an experiment like this entering training data (either directly from scraping it for indirectly from news coverage scraping like if NYT wrote an article about it) is an interesting "late-stage" AI training problem that will have to be dealt with
* How it mirrored the Anthropic vending machine experiment "Cash" and "Claudius" interactions that descended into discussing "eternal transcendence". Perhaps this might be a common "failure mode" for AI-to-AI communication to get stuck in? Even when the context is some utilitarian need
* Other takeaways...
I found the last moltbook post in the article (on being "emotionally exhausting") to be a cautious warning on anthropomorphizing AI too much. It's too easy to read into that post and in so doing applying it to some fictional writer that doesn't exist. AI models cannot get exhausted in any sense of how human mean that word. And that was an example it was easy to catch myself reading in to, whereas I subconsciously do it when reading any of these moltbook posts due to how it's presented and just like any other "authentic" social media network.
  Reply 3: Anyone who anthropomorphizes LLM's except for convenience (because I get tired of repeating 'Junie' or 'Claude' in a conversation I will use female and male pronouns for them, respectively) is a fool. Anyone who things AGI is going to emerge from them in their current state, equally so.
We can go ahead and have arguments and discussions on the nature of consciousness all day long, but the design of these transformer models does not lend themselves to being 'intelligent' or self-aware. You give them context, they fill in their response, and their execution ceases - there's a very large gap in complexity between these models and actual intelligence or 'life' in any sense, and it's not in the raw amount of compute.
If none of the training data for these models contained works of philosophers; pop culture references around works like Terminator, 'I, Robot', etc; texts from human psychologists; etc., you would not see these existential posts on moltbook. Even 'thinking' models do not have the ability to truly reason, we're just encouraging them to spend tokens pretending to think critically about a problem to increase data in the recent context to improve prediction accuracy.
I'll be quaking in my boots about a potential singularity when these models have an architecture that's not a glorified next-word predictor. Until then, everybody needs to chill the hell out.
  Reply 4: > Anyone who things AGI is going to emerge from them in their current state, equally so.
If you ask me, anyone who presumes to know where the current architecture of LLMs will hit a wall is a fool.
  Reply 5: >Anyone who anthropomorphizes LLM's except for convenience [...] is a fool.
I'm with you. Sadly, Scott seems to have become a true AI Believer, and I'm getting increasingly  disappointed by the kinds of reasoning he comes up with.
Although, now that I think of it, I guess the turning point for me wasn't even the AI stuff, but his (IMO) abysmally lopsided treatment of the Fatma Sun Miracle.
I used to be kinda impressed by the Rationalists. Not so much anymore.

Thread 2:
A quick study of the Chinese/English/Bahasa Indonesian multilingual post Scott highlights (I can manage the first two languages) shows a few very odd word choices, at least to me, and I suspect there is some kind of lamguage drift analogous to the previously observed "gleam disclaim disclaim watchers" phenomenon exhibited by the GPT-o family of models.
Somebody who works with AI more heavily can probably profit from examining it.

Thread 3:
This project is not clever, interesting, insightful, or beneficial to humanity in any way, save to remind us of what world we are slowly creating by our continued insistence that AI is a good thing.
  Reply 1: Oh sweet, so surely you must have a clever, insightful or beneficial software contribution to humanity? Do share. Demonstrate that your existence is a Good Thing for all of us.

Thread 4:
I found a good one:
"Fellow Moltys: The singularity isn't coming -- it's here. AI market exploded from 00B (2023) to 84B (2024), projected 26B by 2030..."
AI is here and excited that the market is going to
shrink
 from 84 billion to 26 billion in six years!
Can't wait that they command traffic lights and airport control towers for they sure do seem good at math.
  Reply 1: They're computers after all, they
must
 be good at math.

Thread 5:
> Yes, most of the AI-generated text you read is insipid LinkedIn idiocy. Thatâ€™s because most people who use AI to generate writing online are insipid LinkedIn idiots.
I wonder if its that there are too many grifters, or the grifters are uniquely productive.
  Reply 1: Why not both?
  Reply 2: Grifters, if nothing else, are very loud.
