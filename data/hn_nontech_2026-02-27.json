{
  "scraped_date": "2026-02-27",
  "source": "hacker_news",
  "total_scraped": 129,
  "nontech_count": 37,
  "posts": [
    {
      "id": "47163885",
      "title": "Tell HN: YC companies scrape GitHub activity, send spam emails to users",
      "link": "https://news.ycombinator.com/item?id=47163885",
      "domain": "news.ycombinator.com",
      "author": "miki123211",
      "score": 600,
      "comment_count": 224,
      "created_ts": 1772098508,
      "is_internal": true,
      "post_text": "Hi HN,<p>I recently noticed that an YC company (Run ANywhere, W26) sent me the following email:<p>From: Aditya &lt;aditya@buildrunanywhere.org&gt;<p>Subject: Mikołaj, think you&#x27;d like this<p>[snip]<p>Hi Mikołaj,<p>I found your GitHub and thought you might like what we&#x27;re building.<p>[snip]<p>I have also received a deluge of similar emails from another AI company, Voice.AI (doesn&#x27;t seem to be YC affiliated). These emails indicate that those companies scrape people&#x27;s Github activity, and if they notice users contributing to repos in their field of business, send marketing emails to those users without receiving their consent. My guess is that they use commit metadata for this purpose. This includes recipients under the GDPR (AKA me).<p>I&#x27;ve sent complaints to both organizations, no response so far.<p>I have just contacted both Github and YC Ethics on this issue, I&#x27;ll update here if I get a response.",
      "is_ask_hn": false,
      "matched_keywords": [
        "purpose"
      ],
      "comments": [
        {
          "top": "Martin from GitHub here.  This type of behaviour is explicitly against the GitHub terms of service, when we catch the accounts doing this we can (and do) take action against those accounts including banning the accounts. It's a game of whack-a-mole for sure, and it's not just start-ups that take part in this sketchy behaviour to be honest. I've been plenty of examples in my time across the board.\nThe fundamental nature of Git makes this pretty easy for folks to scrape data from open source repositories.  It's against our terms of service and those folks might want to talk with some lawyers about doing it - but as every Git commit contains your name and email address in the commit data it's not technically difficult even if it is unethical.\nFrom the early days we've added features to help users anonymise their email addresses for commits posted to GitHub.  Basically, you configure your local Git client to use your 'no-reply' email address in commits and that still links back to your GitHub account when you push: \nhttps://docs.github.com/en/account-and-profile/reference/ema...\nI think that's still probably the best route. We want to keep open source data as open as possible, so I don't think locking down API's etc is the right route. We do throttle API requests and scraping traffic, but then again there have been plenty of posts here over the years from people annoyed at hitting those limits so it's definitely a balancing act. Love to know what folks here think though.",
          "author": "martinwoodward",
          "replies": [
            {
              "text": "> when we catch the accounts doing this we can (and do) take action against those accounts including banning the accounts.\nThis isn't my experience. I requested that you looked into a spammer in July 2025, you ignored my reply and the account is still active.\n----\nThank you so much for the report. We're sorry to hear you're receiving unwanted emails, but it's always a possibility when your public contact information is listed on the web. You can keep your email address private if you wish by following the steps here:\nSetting your commit email address\nWe do expect our users to comply with our Terms of Service, which prohibits transmitting using information from the GitHub (whether scraped, collected through our API, or obtained otherwise) for spamming purposes. I'm happy to look into it further to see if we can contact the reported user and let them know that this type of activity is not allowed.\nPlease let us know if you have any other questions or concerns.\n----\nMy reply which was ignored:\n----\nI understand it will happen from time to time. I'd rather be contactable (I've received legitimate emails today because my email is on my profile).\nPlease take further action. My email is public with the expectation that the ToS will be enforced. If GitHub isn't discouraging spammers then it makes it much harder to justify being contactable.\nAll the best,\nDavid",
              "author": "david_allison",
              "depth": 1
            },
            {
              "text": "I reported spammers ~5 times to GH, and every time the account went down in a couple of hours. Obviously mileage may vary, but I don't want the whole HN to think this process is completely broken.\nPlease keep reporting spammers, usually it works.",
              "author": "gettingoverit",
              "depth": 2
            },
            {
              "text": "To confirm: is this email spammers, or spam on GitHub?\nI've had decent success with on-GitHub action (I'd wager ~80% action taken), but the effort to report email spammers doesn't seem worthwhile.",
              "author": "david_allison",
              "depth": 3
            },
            {
              "text": "It's impossible for them to stop if you list your email on there. They could make it harder of course. But if you put your email out there for a human to find, then a script or bot or also find it.\nAnd yes of course they can also stop a specific spammer. But that spammer may pick up another account and email.",
              "author": "tom_m",
              "depth": 2
            },
            {
              "text": "The grandparent post wasn't asking for them to do the impossible and stop all spamming, only to take action against the particular user that spammed them.",
              "author": "angoragoats",
              "depth": 3
            }
          ]
        },
        {
          "top": "YC is a proud investor in Flock, what YC Ethics thing are you talking about?",
          "author": "scottydelta",
          "replies": [
            {
              "text": "And that Optifye.ai demo with the sweatshop surveillance software",
              "author": "otherayden",
              "depth": 1
            },
            {
              "text": "And Cluely",
              "author": "cassonmars",
              "depth": 1
            },
            {
              "text": "Cluely is not YC.",
              "author": "tasn",
              "depth": 2
            },
            {
              "text": "he might be thinking of chadIDE \"the first brainrot ide\"",
              "author": "fantasizr",
              "depth": 3
            },
            {
              "text": "the same Cluely that's on IG? I thought that was a fictional satire.",
              "author": "insane_dreamer",
              "depth": 2
            }
          ]
        },
        {
          "top": "I've spent a lot of my career marketing to developers, and spamming their GitHub account might be top 1 or 2 worst marketing tactics you can use.\nCold emailing rarely works by itself. Cold emailing \ndevelopers\n via emails you pulled from their GitHub accounts? At that point, you're actively harming your brand, and may as well just send them spam diet pill ads.",
          "author": "keiferski",
          "replies": [
            {
              "text": "If someone took the time to look through my GitHub contributions then pitched me with a job relevant to that work I would absolutely consider them. That's \nexactly\n the kind of recruiter I would like to work with.\nIf it's obviously just a bot scraping emails and sending generic job requests, that's very different.",
              "author": "RandallBrown",
              "depth": 1
            },
            {
              "text": "Yeah this - I got one of these emails someone sniffing around my GitHub not that long ago and it wasn't immediately obvious that it was a scammy recruiter, so I responded to sound out if they were actually interested in one of my projects. Got the same generic response about let's work together on something so I didn't respond.",
              "author": "jamesfinlayson",
              "depth": 2
            },
            {
              "text": "> If it's obviously just a bot scraping emails and sending generic job requests, that's very different.\nIt's not even that nice. They scrape emails and send cold calls to try to get you to purchase their services.",
              "author": "devmor",
              "depth": 2
            },
            {
              "text": "Wait why? That seems like the high effort and high specificity thing that I'd love to get.\nYou searched for people who do what you need to have done, found me, looked at what I've worked on and determined I'd be a good fit and you reached out? That's the number one way to get me to want to work for you.",
              "author": "polishdude20",
              "depth": 1
            },
            {
              "text": "> You searched for people who do what you need to have done, found me, looked at what I've worked on and determined I'd be a good fit and you reached out? That's the number one way to get me to want to work for you.\nNo, their email templating tool finds an old throwaway repo you did 6 years ago, templates its name into a form email, and invites you to join a cattle call to be whiteboarded along with the rest of the shmucks",
              "author": "woah",
              "depth": 2
            }
          ]
        },
        {
          "top": "Ever wonder why YC has the \"Describe a time you most successfully hacked some system to your advantage\" question? It's because they select for founders that are willing to take advantage of legal gray areas. Airbnb repeatedly violated Craigslist terms of service and called it \"growth hacking.\" Reddit stole content from Digg and faked users. OpenAI trains their models on copyrighted content.",
          "author": "an0malous",
          "replies": []
        },
        {
          "top": "I also had unsolicited spam from Vincent Jiang of Aden, another YC company.\n    Hi Daniel,\n\n    I just came across your profile on social media and wondered if you'd be interested in joining our Discord community for AI agent development. Currently, we see that agents break, loop, get lost, hallucinate, and cost a fortune, and therefore built a space where developers can share challenges and insights.",
          "author": "unfunco",
          "replies": [
            {
              "text": "…and more from Backdrop.\n    Hi Daniel, I found your GitHub profile while searching for anthropic projects, and got your email from your profile.\n\n    I'm part of an online program for builders called Backdrop Build, and I think that program would be a great fit given what you are building. We have a track for builders in AI like you, it's fully online/remote and costs nothing to participate. It also works if you have a day job, it's light on time and perfect for side projects!\n\n\nAnd then another after I marked the first one as spam and ignored it.\n    Checking in one last time to see if you have any questions about the program or the application. If it's not for you, all good - just ignore the email because I won't be pinging you again :)\n\n   Joey from Backdrop\n\n\nBoth companies have guaranteed that I won't use their services nor procure them for any organisation I work for.",
              "author": "unfunco",
              "depth": 1
            },
            {
              "text": "Hey it's Joey checking in again. We noticed you mentioned our company, let me know if you have any questions about our (free!) program. I'll go ahead and email you some more info, just in case.",
              "author": "agmater",
              "depth": 2
            },
            {
              "text": "I had a similar one from that guy asking me to make open source PRs to some repo of theirs for, err, $25-50/hour. I replied explaining that senior software engineers in the UK aren’t \nquite\n as desperately poor as that, and got a canned response saying that they were looking forward to reviewing my PRs :D",
              "author": "foldr",
              "depth": 1
            }
          ]
        }
      ]
    },
    {
      "id": "47148454",
      "title": "Claude Code Remote Control",
      "link": "https://code.claude.com/docs/en/remote-control",
      "domain": "code.claude.com",
      "author": "empressplay",
      "score": 536,
      "comment_count": 315,
      "created_ts": 1772004176,
      "is_internal": false,
      "post_text": "",
      "is_ask_hn": false,
      "matched_keywords": [
        "remote"
      ],
      "comments": [
        {
          "top": "This is an extremely clunky and buggy prerelease, so don't try to hot fix prod from the toilet without a different mobile frontend.\nRight now:\n- You can't interrupt Claude (you press stop and he keeps going!)\n- At best it stops but just keeps spinning\n- The UI disconnects intermittently\n- It disconnects if you switch to other parts of Claude\n- It can get stuck in plan mode\n- Introspection is poor\n- You see XML in the output instead of things like buttons\n- One session at a time\n- Sessions at times don't load\n- Everytime you navigate away from Code you need to wait for your session to reappear\nI'm sure I'm missing a few things.",
          "author": "fny",
          "replies": [
            {
              "text": "Struggled with it also, given up (for now).\nI thought coding was a solved problem Boris?",
              "author": "monkeydust",
              "depth": 1
            },
            {
              "text": "I have little doubt where things are going, but the irony of the way they communicate versus the quality of their actual product is palpable.\nClaude Code (the product, not the underlying model) has been one of the buggiest, least polished products I have ever used. And it's not exactly rocket science to begin with. Maybe they should try writing slightly less than 100% of their code with AI?",
              "author": "rfw300",
              "depth": 2
            },
            {
              "text": "More generally, Anthropic's reliability track record for a company which claims to have solved coding is astonishingly poor. Just look at their status page - \nhttps://status.claude.com/\n - multiple severe incidents, every day. And that's to say nothing of the constant stream of bugs for simple behavior in the desktop app, Claude Code, their various IDE integrations, the tools they offer in the API, and so on.\nTheir models are so good that they make dealing with the rest all worth it. But if I were a non-research engineer at Anthropic, I wouldn't strut around gloating. I'd hide my head in a paper bag.",
              "author": "rfw300",
              "depth": 3
            },
            {
              "text": "I don’t think that’s fair. ChatGPT and Gemini also seem to suffer random outages. They’re dealing with high load on a new type of product.\nBut it’s also true that Anthropic products are super buggy.",
              "author": "rhubarbtree",
              "depth": 4
            },
            {
              "text": "It's almost as if a lot of anthropic's stuff is vibecoded",
              "author": "jaapz",
              "depth": 5
            }
          ]
        },
        {
          "top": "Posted elsewhere but will copy here. Been doing this for a while.\n- - -\nget tailscale (free) and join on both devices\ninstall tmux\nget an ios/android terminal (echo / termius)\nenable \"remote login\" if on mac (disable on public wifi)\nmosh/ssh into computer\nnow you can do tmux then claude / codex / w/e on either device and reconnect freely via tmux ls and tmux attach -t <id>\n- - -\nYou can name tmux and resume by name via tmux new -s <feature> and tmux attach -t <feature>",
          "author": "jasonjmcghee",
          "replies": [
            {
              "text": "I have just today discovered zmx [1] which is like tmux but I always hated the tmux terminal emulation and how it hijacks scrolling, especially on Termius on my phone. It does session persistence but I think without the terminal emulator side of things, so scrolling works normally.\nBeen testing it today with Claude Code and it seems to work quite well switching between my laptop and phone.\n[1] \nhttps://github.com/neurosnap/zmx",
              "author": "madjam002",
              "depth": 1
            },
            {
              "text": "I also hate how tmux uses alt mode and can never remember all the shortcuts, copy paste is a PITA and just today I had to look up how to dump the scrollback buffer to a file.  Named sessions without window management makes a lot more sense these days.  Similarly, I'm not a fan of all the ANSI escape codes that CC uses to jump the cursor around and rewrite the display to look like a GUI.  I prefer a TUI that doesn't mutate rows after writing them, that's what alt mode is for.  CC often clears whatever was in the scrollback buffer before you opened it, it hides bracketed paste, and goes crazy sometimes when content overflows the window and I have to resize the terminal or get blasted with a wall of glitching characters--extra annoying if I'm working from a low bandwidth link.  I develop my own agent framework and code agent, and while some features aren't as polished as CC, one of my explicit goals is to preserve the traditional CLI feel, like the python REPL (that's what it's based around).  I'll give zmx a try tonight :)",
              "author": "jacob019",
              "depth": 2
            },
            {
              "text": "Thanks for the tip. Other ppl are saying \"most of us started out like this\" but if you haven't played with tailscale etc. (like me). Then this is new and good for learning imo",
              "author": "nebben64",
              "depth": 1
            },
            {
              "text": "I think everyone started out with this",
              "author": "zeppelin101",
              "depth": 1
            },
            {
              "text": "I think people familiar with tmux/screen and tailscale or how to setup their own VPN might have.\nBased on my experience many people don't know this is a thing you can do.",
              "author": "jasonjmcghee",
              "depth": 2
            }
          ]
        },
        {
          "top": "Opencode's 'web' command makes your local session run on the browser with same access rights as the cli. It's a pretty slick interface too. I sometimes use it instead of the cli even when I can access both.\nYou can test it right now if you want with the included free models.\nhttps://opencode.ai/docs/web/",
          "author": "dizhn",
          "replies": [
            {
              "text": "I was having too many bugs using it with my phone, I gave up and went back to Termux",
              "author": "rubslopes",
              "depth": 1
            },
            {
              "text": "It's changing super fast. I am using it on the desktop mostly and when I tried on my phone there were issues yes. But do try it out again in a few weeks.\n(I am actually using zellij on the remote and using various CLIs more than I am using only opencode on the web. I was using wezterm mux until about a week ago but the current state of the terminal is not  very good for this scenario. It seems like almost all the CLIs are choking because of nodejs ink library)",
              "author": "dizhn",
              "depth": 2
            }
          ]
        },
        {
          "top": "There's \nhttps://happy.engineering/\n which already does this with many fewer bugs and supports codex.",
          "author": "Fizzadar",
          "replies": [
            {
              "text": "Happy is buggy af and is in the middle of a rewrite (see its Discord).\nA fork named Happier looks promising, but is alpha-stage and is also a mystery-meat vibe-coded security roulette.",
              "author": "quinncom",
              "depth": 1
            },
            {
              "text": "I was using this religiously but there’s a bug currently that makes the initialization fail and/or throws an error on the phone client. \nAbsolutely great piece of software otherwise, free, anonymous, encrypted and so on. Really hope the team can fix this soon - I would hate to switch back to tmux tunneling.",
              "author": "tor0ugh",
              "depth": 1
            },
            {
              "text": "Set it up and never managed to have it work. Only thing it did was renaming my sessions on my main cc instance. Mobile did nothing, not even an error message.",
              "author": "crashabr",
              "depth": 1
            }
          ]
        },
        {
          "top": "I feel like a lot of folks are saying this kills the Code on your Phone opportunity some start-ups are building for. I don't agree. I feel like coding agents are like streaming services, we will subscribe to multiple and switch between them. So for one there's value in a universal control plane. The other is that mobile as a coding interface should offer more than a remote control to the desktop. I think there's still some space to cook, especially if people are investing 8 hours a day talking to agents, the interface surely matters.",
          "author": "raunaqvaisoha",
          "replies": [
            {
              "text": "I don't know a single person who is satisfied with the status quo on streaming services where you have to subscribe to multiple ones. Everyone is complaining that the landscape is 1) more fragmented than cable was, 2) costs more, 3) has even more ads than cable",
              "author": "63stack",
              "depth": 1
            },
            {
              "text": "I think people forgot how bad it was. It was much more fragmented before but instead of services it was fragmented by time. Sure you have access to Seinfeld, but you can watch one or two Seinfelds a night at 8pm and 11pm.\nI also remember base cable without any movies was around $60 or something and with some movie channels is >$100. And that's not inflation adjusted. You can easily get 3 or 4 of the top services for $100 today.\nFinally claiming there are more ads on these services is a joke. There was ~20m for every 30m of programming, meaning 1/3 of the time you're watching commercials. And not just any commercials, the same commercials over and over. There was even a case of shows being sped up on cable to show more commercials.\nI get it, everyone wants everything seamlessly and for next to nothing, but claiming that 90s cable was even comparable is absurd.\nhttps://www.digitaltrends.com/home-theater/how-networks-spee...",
              "author": "bko",
              "depth": 2
            },
            {
              "text": "Seinfeld way syndicated. It aired for a long time on TBS. But also Comedy Central after 2021, Nick at Nite briefly and TV Land more recently.\nI'm not sure what your point is.",
              "author": "bko",
              "depth": 4
            },
            {
              "text": "Seinfeld only ran until 1998. Not sure what people buying the rights in 2021 has to do with the OP's comment.",
              "author": "galleywest200",
              "depth": 5
            },
            {
              "text": "Not that it is particularly relevant to agentic coding but how can anyone truly argue streaming costs more? Average cable packages were exceeding 125-150 USD a month (in 2000 dollars). Under no circumstances would I be sympathetic to the argument that streaming costs more.\nYou can get all 7 of the major streaming subs for less without even shopping out deals. That is 100s of times the volume and quality of content that was delivered on cable for far less. It is so much content realistically that no one I have ever met has subscribed to all of them at once.\nThe argument really is empty. The fragmentized experience is annoying, but it isn't more expensive...And it DEFINITELY has fewer ads.",
              "author": "hodder",
              "depth": 2
            }
          ]
        }
      ]
    },
    {
      "id": "47172119",
      "title": "Layoffs at Block",
      "link": "https://twitter.com/jack/status/2027129697092731343",
      "domain": "twitter.com",
      "author": "mlex",
      "score": 484,
      "comment_count": 472,
      "created_ts": 1772140676,
      "is_internal": false,
      "post_text": "<a href=\"https:&#x2F;&#x2F;www.cnbc.com&#x2F;2026&#x2F;02&#x2F;26&#x2F;block-laying-off-about-4000-employees-nearly-half-of-its-workforce.html\" rel=\"nofollow\">https:&#x2F;&#x2F;www.cnbc.com&#x2F;2026&#x2F;02&#x2F;26&#x2F;block-laying-off-about-4000-...</a><p><a href=\"https:&#x2F;&#x2F;www.marketwatch.com&#x2F;story&#x2F;block-plans-to-lay-off-nearly-half-its-staff-in-deliberate-and-bold-embrace-of-ai-81e99247\" rel=\"nofollow\">https:&#x2F;&#x2F;www.marketwatch.com&#x2F;story&#x2F;block-plans-to-lay-off-nea...</a>",
      "is_ask_hn": false,
      "matched_keywords": [
        "layoffs"
      ],
      "comments": [
        {
          "top": "> today we're making one of the hardest decisions in the history of our company:\n> i had two options: cut gradually over months or years as this shift plays out, or be honest about where we are and act on it now. i chose the latter.\n> i’m sorry to put you through this.\nPOV: Dude who has effortlessly fired people before deflects blame for over-hiring in the first place.\nI swear people should start blacklisting CEOs and refuse to work under them if they're part of the blacklist.\nThis is just a piss poor excuse for bad management and short-sighted vision and no accountability.",
          "author": "neya",
          "replies": [
            {
              "text": ">>I had two options: cut gradually over months or years as this shift plays out, or be honest about where we are and act on it now. I chose the latter.\nI don't know why CEOs and other executive members end up writing such useless language in their posts. Essentially, both these points are the same if you look at the employees. However, the writing has to be bloated in such a way that there is something else involved here, which there is not. This is just drama.\nAlso, these decisions are not hard, regardless of whatever the hell has been claimed. They are actually easy decisions and choosing not to do layoffs is actually the hard decision. There is no need to sugarcoat so much.",
              "author": "princevegeta89",
              "depth": 1
            },
            {
              "text": "It's for the people who still work there.",
              "author": "operatingthetan",
              "depth": 2
            },
            {
              "text": "It surely still does look stupid for the same people though..",
              "author": "princevegeta89",
              "depth": 3
            },
            {
              "text": "> I swear people should start blacklisting CEOs and refuse to work under them if they're part of the blacklist.\nLook at the job market. They know they can get away with it and so they don't care.\nMy current theory is that this is partly why executives are desperate to get AI to work, and why investors are ploughing billions into AI. They know they've burnt too many bridges, and they \nneed\n AI to work so they never have to turn to us again. Otherwise the pendulum will swing even farther in the opposite direction, putting even more bargaining power in the hands of employees than the post-COVID job market.\nUnfortunately, AI does seem to be working very well, and I don't see great outcomes for us on the current trajectory. I expect turmoil before a new social contract is established.",
              "author": "keeda",
              "depth": 1
            },
            {
              "text": "> i had two options: cut gradually over months or years as this shift plays out, or be honest about where we are and act on it now. i chose the latter.\nit's interesting how he casts this as:\na) a drawn-out downsizing that might not meet its numbers for years, which is clearly \"bad\" because no one likes uncertainty.\nb) ripping the band-aid decisively, with the nobility of being an honest decision.  and who doesn't appreciate honesty?\nbut certainly most employees would prefer to be laid off in months (or even, as he says, years) \"as this shift plays out.\"  they would probably prefer that to having jack's \"honesty.\"",
              "author": "khazhoux",
              "depth": 1
            }
          ]
        },
        {
          "top": "We'll see how much the AI aspect is true by whether they're thinning out teams equally, or just axing whole initiatives. My impression of Block was that it was mostly a one-trick pony (okay, two if you include CashApp) with a bunch of side initiatives that never seemed to pan out, so I'm expecting it to be more of the latter, with this being more of an admission that they're now in \"maintenance mode\".\nEither way, I think this is how it's gonna be. Regardless of whether AI significantly increases productivity (40%? come on), layoffs will be preemptory. Executives will see the lack of productivity boost as being due to lack of pressure, and imagine engineers are just using the AI to make their own lives easier rather than to work more efficiently. You can't really double output velocity because your users will see it as too much churn, so the only choice is to lay off half the workforce and double the workload for those who stay. \"Necessity is the mother of invention.\" They'll overlook the fact that the work AI tools provide only encompasses 10% of your job even if they're 100% efficient.",
          "author": "daxfohl",
          "replies": [
            {
              "text": "I'm convinced that these \"AI Layoffs\" are these companies trying to save face from the absurd overhiring that they did in 2022 and 2023 because apparently they thought that these no-interest loans/free money would just last forever.\nNo one really \"knows\" how to grow businesses so the easiest way to spend a lot of money quickly is hiring lots of people, whether or not they are \"necessary\". Then this free money dries up, interest rates go back up, and now they're stuck with all these employees that they didn't actually need.\nSome companies like Google and Microsoft just accepted that assholes like me will call their CEOs incompetent and fired lots of people in 2023, but I think other CEOs were kind of embarrassed and held off.  Now they can use AI as a scapegoat and people won't act like they were idiots for hiring twice as many people as they needed.\nAlso, I got declined by Block a year ago.  Glad I was now.",
              "author": "tombert",
              "depth": 1
            },
            {
              "text": "Regardless of the reasoning I think it is worth keeping in mind that the times when companies are letting talented experienced people go is also a great time to start the next new big thing.  Talent that might have been unobtanium during a hiring frenzy could now be the building blocks of a new venture.  A lot of these companies were started or really built themselves up during a tech slow down.",
              "author": "georgeecollins",
              "depth": 2
            },
            {
              "text": "it's all just saying stuff the shareholders want to hear.  when the shareholders want to hear \"we're staffing up aggressively\" the companies hire.  when the shareholders want to hear \"we're moving workloads to AI\" the companies fire.\nit's not using AI as a scapegoat.  they're doing this because they're quite literally being rewarded for it.  they could care less what the employees who are getting fired think, as long as the investors are happy.",
              "author": "notatoad",
              "depth": 2
            },
            {
              "text": "> I'm convinced that these \"AI Layoffs\" are these companies trying to save face from the absurd overhiring that they did in 2022 and 2023 because apparently they thought that these no-interest loans/free money would just last forever.\nPartially.\nThe first nail in the coffin was the change in assumptions around output. Before 2023, there was an assumption that more bodies means more output. After the massive X/Twitter layoffs (60-70% headcount culled) with X/Twitter still standing, this assumption was clearly proven false.\nThe second nail was the change in operational metrics. Before 2023, ARR growth was a good enough metric to target. After 2023, FCF positivity became the name of the game. Especially because us investors are demanding this \nbecause\n most funds are reaching the 10 year mark where we need to make our LPs whole, so a path to exit (be it IPO, M&A, or a continuation fund) needs to be communicated.\nAnd finally, COVID proved to a large number of companies and industries that 100% WFH and Async for white collar roles \ndoes\n work. But wait, if I can hire Joe in Cary to work async, why can't I hire Jan in Karlin, Prague or Jagmeet in Koramangla, Bangalore? This means I can \nalso\n enhance FCF positivity while not impacting delivery.\nAdd to that some very, very, very bad hires (most bootcamp grads just can't cut it) at absurdly high salaries and that's why you're seeing the culling that is occurring today.\nThat said, AI tools are powerful, and if you are working on rightsizing an organization, using Claude or Enterprise GPT in workflows helps one person do multiple jobs at once. We now expect PMs to also work as junior program managers, designers, product marketers, customer success managers, and sales engineers and we now expect SWEs to also work as junior program managers, designers, docs writers, and architects. Now I can lay off 10-20% of my GTM, Designers, SWEs, Program Managers, and Docs Writers and still get good enough output.\n---\nIMO, if you want to survive in the tech industry in this world, doing the following will probably help maintain your longevity:\n1. Move to a Tier 1 tech hub like the Bay and NYC. If you get laid off, you will probably find another job in a couple of weeks due to the density of employers.\n2. Start coming into the office 2-3 days a week. It's harder to layoff someone you have had beers or coffee with. Worst case, they can refer you to their friends companies if you get laid off\n3. \nUpskill technically\n. Learn the fundamentals of AI/ML and MLOPs. Agents are basically a semi-nondeterministic SaaS. Understanding how AI/ML works and understanding their benefits \nand\n pitfalls make you a much more valuable hire.\n4. \nUpskill professionally\n. We're not hiring code monkeys for $200K-400K TC. We want Engineers who can communicate business problems into technical requirements. This means also understanding the industry your company is in, how to manage up to leadership, and what are the revenue drivers and cost centers of your employer. Learn how to make a \nbusiness case\n for technical issues. If you cannot communicate why refactoring your codebase from Python to Golang would positively impact topline metrics, no one will prioritize it.\n5. Live lean, save for a rainy day, and keep your family and friends close. If you're not in a financial position to say \"f##k you\" you will get f##ked, and strong relationships help you build the support system you need for independence.\nThe reality is the current set of layoffs and work stresses were the norm in the tech industry until 2015-22. We live in a competitive world and complaining on HN does nothing to help your material condition.",
              "author": "alephnerd",
              "depth": 2
            },
            {
              "text": "The Twitter layoffs being used as proof of _anything_ is misguided no matter what you're trying to say.\nIf success is losing half their revenue, reverting to revenue numbers from a decade ago, I gotta know what failure looks like. You might argue that the revenue losses aren't correlated to their headcount changes and probably make a good argument, but I mean... It's not a great one",
              "author": "hirsin",
              "depth": 3
            }
          ]
        },
        {
          "top": "This is one of the best (if not the best) layoff letters I've seen online (no affiliation, don't know anyone working there, purely outsider perspective).\n* Severance packages upfront because realistically that's what everyone worries about first.\n* Reasoning second. I appreciate the one clean cut vs prolonged bleeding.\n* Owning the decision and respecting the people that got you there. Opting for an awkward allhands vs breakup-via-text-message.\n* Giving people a chance to say goodbye.\nNot gonna go into strategic analysis of this, or Jack's leadership style in general.\nBut realistically, you can't pen a better (or, well, less bad) layoff announcement.",
          "author": "senko",
          "replies": [
            {
              "text": "> I appreciate the one clean cut vs prolonged bleeding.\nThat's a false dichotomy, you could reduce headcount via attrition which is better in some ways.\nThere's also no reasoning on product impact. Is the strategy to cut products that aren't making money? Is the strategy to cut 40% across everyone because everyone can go faster?\n> Owning the decision\nDoes it? It came across to me as an inevitability of AI, not \"we over-hired\". Layoffs are always a mis-management issue, because the opposite (hiring) is a management issue. If management failed to see where the market was going and now needs a different workforce, that's still a management issue.\n> respecting the people that got you there\nThere's words, and there's money, and on these it's pretty good. But there's also an empathy with the experience they're about to go through and I'm not sure there's much of that here beyond the words. To do this well you'd need to think through what folks are about to go through and look for ways you can positively impact that beyond actions today. I've seen some companies do this better, helping teams get re-hired elsewhere, splitting off businesses to sell to other companies, incubating startups, there are lots of options. Hard, especially at this scale, but possible.\n> But realistically, you can't pen a better (or, well, less bad) layoff announcement.\nAnd this is the crux of my point, I really think you can. This was a good one, one of the better I've seen, but it's still within the realm of SV companies laying people off. In some companies, countries, industries, this would look very different, and better.",
              "author": "danpalmer",
              "depth": 1
            },
            {
              "text": "> you could reduce headcount via attrition which is better in some ways\nI don't think reducing via attrition is better for the company, for the employees 100%, but attrition would be your people moving to other companies and retirement. It means that you are effectively bleeding your people with options (usually above average) and those with the most experience in favor of \"the rest\".",
              "author": "belval",
              "depth": 2
            },
            {
              "text": "It's a nuanced trade-off. It's worse for the company as you said, it may be worse for the employees because some will leave from burnout without severance, those remaining will have more work to do typically.\nBut my point was that what was presented was a false dichotomy and that framing it as such is disingenuous to employees receiving those comms.",
              "author": "danpalmer",
              "depth": 3
            },
            {
              "text": "It should be good. It's the third time he's written this exact same announcement, including \"taking the blame\" and \"making the difficult choice to cut a large group instead of smaller cuts over time\" and \"thanking the expendables that got the company where it is.\"",
              "author": "shaftway",
              "depth": 1
            },
            {
              "text": "\"owning\" the decision doesn't mean anything. It's just words.",
              "author": "slantedview",
              "depth": 1
            }
          ]
        },
        {
          "top": "Nice severance; but in this job market, holy shit.\nYeah, you get 5 months of severance and a bunch of devices and such; but, does this CEO really think these employees will find new work in that time? In this job market?\nIf the profits are still up and growing, why on earth would you evict 40% of the company, to send them into \nthis\n job market? Why not … try new industries, play around, try to become the next Mitsubishi or Samsung or General Electric. If you’ve got the manpower and talent, why not play with it and see if anything makes money. In-house startups with stable capital, all that.\nThis seems … wrong.",
          "author": "t-writescode",
          "replies": [
            {
              "text": "> Nice severance; but in this job market, holy shit.\nI just talked to a bunch of recruiters (we're hiring) and their main piece of advice was: The market is crazy. \nMove fast\n. We're seeing people getting jobs within days of starting to look, bailing on offers after signing because they got a better offer somewhere else, etc. 24 hours is the longest you can leave a candidate waiting. You have been warned\nedit: I am in SFBA. Your reality may be different. People have spilled some 2 trillion dollars onto the area in the past 2 years. A lot of that is going to software engineers as everyone tries to shove AI down consumers' throats. Rents are up 60% in 12 months, which is not the sign of a cold employment market :)",
              "author": "Swizec",
              "depth": 1
            },
            {
              "text": "I'm in NYC which I think has similar demographics to SF in this regard; I found my job in August of last year, after about five months of searching, and I found it because a friend of mine referred me. It's a good job, and I like it, I'm grateful for that friend.\nRegardless, it's not like that was the only job I applied to. I had a policy of applying to at least ten jobs a day, so I applied to about ~1500 jobs, and literally \nall\n of them rejected me except for the one I have right now. I had about twenty other interviews (edit: 15, checked my calendar from last year), a few that got to late stages, and they didn't pan out [1].\nI psychotically save money so I wasn't worried in any kind of existential sense, I could survive for years if I needed, but man I would have \nkilled\n to be in a situation where I even had the opportunity to bail on an offer.\nThis has been the worst economy for software engineers I've seen in my ~15 year career.  I am slightly optimistic that it will improve \neventually\n but I suspect \"eventually\" might mean several more years.\n[1] And one at a one of the world's largest bank (that my lawyer/mom has advised me not to name publicly) where my interviewers were potentially the most incompetent people I have ever talked to and who didn't seem to know what an atomic was in Java, and \"corrected\" my counter code with a mutex.  And I put \"corrected\" in quotes, because what they corrected it to would deadlock.  Morons.",
              "author": "tombert",
              "depth": 2
            },
            {
              "text": "I'm a senior in NYC, considering changing jobs but haven't pulled the trigger. I've got a good amount of reachouts from finance recruiters and small-to-medium sized startups but haven't heard anything from the established players (admittedly I haven't been looking).",
              "author": "garbawarb",
              "depth": 3
            },
            {
              "text": "I got a number of finance recruiters reaching out then too but nothing stuck.  I got a few interviews (even got to meet a few interesting characters like Martin Shkreli) but nothing panned out until my friend gave me a referral to my current gig.\nI think recruiters will just carpet bomb emails out and then only respond like ten percent of the people that email them back.",
              "author": "tombert",
              "depth": 4
            },
            {
              "text": "Blink once if the bank rhymed with face",
              "author": "vedaba",
              "depth": 3
            }
          ]
        },
        {
          "top": "Square/Block stock peaked at $273 in Feb 2021 and is currently at $54. Taking away the Covid bubble the stock has been completely flat since 2018, almost 8 years, while the S&P 500 returned nearly 200% in that same period. So I'm not buying the whole \"the company is doing great! The layoff is just because of AI.\"",
          "author": "paxys",
          "replies": [
            {
              "text": "We know they're not...\n> In its fiscal fourth quarter, Block reported revenue of almost $6.3bn, in line with Wall Street expectations. Its earnings tumbled to 19 cents a share, owing to a $234mn hit — or 38 cents a share — on its bitcoin holdings.",
              "author": "QGQBGdeZREunxLe",
              "depth": 1
            },
            {
              "text": "Yea, look over there!!! AI.\n(Don’t mention the bitcoin investment that’s in the shitter)",
              "author": "dmboyd",
              "depth": 2
            },
            {
              "text": "Exactly this. Seriously, look at a 10 year graph of SQ/XYZ. How a board of directors puts up with this is beyond me, seems like a massive governance failure.",
              "author": "shawkinaw",
              "depth": 1
            },
            {
              "text": "AI is the logical, counter proof reason, I feel it serves as a scapegoat so perfectly they pretend it replaces people.",
              "author": "hirako2000",
              "depth": 1
            },
            {
              "text": "Feb 2021 was peak covid tech bubble stemming from ZIRP. There are a number of companies that hit highs during that period that they'll likely never see again (or for quite some time) despite being profitable.",
              "author": "Sparkle-san",
              "depth": 1
            }
          ]
        }
      ]
    },
    {
      "id": "47164270",
      "title": "Show HN: Terminal Phone – E2EE Walkie Talkie from the Command Line",
      "link": "https://gitlab.com/here_forawhile/terminalphone",
      "domain": "gitlab.com",
      "author": "smalltorch",
      "score": 290,
      "comment_count": 73,
      "created_ts": 1772102445,
      "is_internal": false,
      "post_text": "TerminalPhone is a single, self-contained Bash script that provides anonymous, end-to-end encrypted voice and text communication between two parties over the Tor network. It operates as a walkie-talkie: you record a voice message, and it is compressed, encrypted, and transmitted to the remote party as a single unit. You can also send encrypted text messages during a call. No server infrastructure, no accounts, no phone numbers. Your Tor hidden service .onion address is your identity.",
      "is_ask_hn": false,
      "matched_keywords": [
        "remote",
        "communication"
      ],
      "comments": [
        {
          "top": "Using a v3 onion address as both the cryptographic identity and the NAT traversal layer is such a clean architectural choice. No STUN/TURN servers, no hole punching, you just boot the script and Tor handles routing.\nFor those who use Tor regularly for things other than web browsing: how bad is the real-world latency for pushing a ~20KB Opus audio chunk over Tor these days? Are we talking a 2-3 second delay, or is it much worse?",
          "author": "Pinkert",
          "replies": [
            {
              "text": "The real world delay is about 2-3 seconds your spot on. I initially started with a full duplex version but it was absolutely terrible. Walkie talkie kinda forces the recieve, listen, response from the users so the latency isn't as much of an issue.",
              "author": "smalltorch",
              "depth": 1
            },
            {
              "text": "Is audio transmitted \nwhile\n it is being recorded or afterwards? Is it played before everything is received or is everything buffered? In the later case, I find it more akin an audio message on Signal or similar, than as a walkie-talkie, which is much more \"dynamic\".",
              "author": "ale42",
              "depth": 2
            },
            {
              "text": "It's not streamed. It gets recorded, compressed, (voice effects if you want), encrypted on device, \nthen\n piped through, reverse process, auto played on reciever end.\nAlso, once it's decrypted and played back, the message gets destroyed.",
              "author": "smalltorch",
              "depth": 3
            },
            {
              "text": "Small suggestion, maybe you should send a “key down” notice when you begin recording, that generates a subtle sound on the receiving end. This would act as something like a typing indicator on a text messaging client.",
              "author": "iamnothere",
              "depth": 4
            },
            {
              "text": "Thats a great idea.",
              "author": "smalltorch",
              "depth": 5
            }
          ]
        },
        {
          "top": "Very cool, happy to see more IRL applications of onion services as a backend. Arti onion client support should soon be available, which will make Tor embeddable in applications as a Rust library. Hopefully this encourages even more usage.\nMore applications using the network means more cover traffic as well.",
          "author": "iamnothere",
          "replies": [
            {
              "text": "> More applications using the network means more cover traffic as well.\nAgree. The biggest barrier for me using Tor is the perception held by many IT admins is that Tor is synonymous with nefarious. It makes using it inconvenient or impossible in many highly controlled network environments such as enterprise, public access wifi, etc.",
              "author": "xnyan",
              "depth": 1
            }
          ]
        },
        {
          "top": "> 21 curated ciphers are available\nWhy!? That sounds like approximately 20 too many.",
          "author": "lxgr",
          "replies": [
            {
              "text": "The library is openssl and that comes with all these ciphers available. No other reason than \nbecause we can!\nI wish AES-GCM was available...but openssl can't do it on its own without further dependencies to parse the authentication correctly.\nReally this whole layer is complelty redundant actually. It's already E2EE without openssl via Tor. I like that it's encrypted before I hit the network pipe though.",
              "author": "smalltorch",
              "depth": 1
            },
            {
              "text": ">\nNo other reason than because we can!\ngreat attitude for approximately everything except, perhaps, cryptography.\nespecially since the initial encryption is mostly redundant, i would encourage that you, at some point, consider reducing the number of ciphers.",
              "author": "john_strinlai",
              "depth": 2
            },
            {
              "text": "If a library doesn't do what you need, you need a different library, but this is impossible from a short bash script, so it's one of the tradeoffs of your design.",
              "author": "inigyou",
              "depth": 2
            },
            {
              "text": "> No other reason than because we can!\nThen maybe your scientists should spend some time to stop and consider whether they should ;)\nBut seriously, I'd just limit this to one option on the selection side, even if you continue supporting more than that at the protocol level for cryptographic agility.",
              "author": "lxgr",
              "depth": 2
            },
            {
              "text": "I don't see the issue. \"Anything that openssl actively supports\" plus providing a default seems like an extremely reasonable stance to take.",
              "author": "fc417fc802",
              "depth": 3
            }
          ]
        },
        {
          "top": "Tangential, did Gitlab become faster than a while back or is it an illusion from their lazy loading?",
          "author": "aitchnyu",
          "replies": []
        },
        {
          "top": "I love this. In your view, how would users go about securely swapping credentials ? PGP over email ?",
          "author": "marcosqanil",
          "replies": [
            {
              "text": "Thanks! My realistic use case is that I am already speaking to someone who I know and trust, so ideally exchange credentials in person. A preferred out of band secure messanger of choice is probably fine.",
              "author": "smalltorch",
              "depth": 1
            },
            {
              "text": "What do you guys talk about?",
              "author": "deadbabe",
              "depth": 2
            },
            {
              "text": "I have my wife's phone set up on autolisten running in the background, so I just pop in and ask how her days going and crack jokes.",
              "author": "smalltorch",
              "depth": 3
            },
            {
              "text": "That's funny but it must absolutely drain the battery of her phone, no?",
              "author": "clouedoc",
              "depth": 4
            },
            {
              "text": "So far it's lasted all week with maybe 10% -15% loss per day. It's not her main, actually just a old phone I had laying around.\nI think it's a pretty light background process.",
              "author": "smalltorch",
              "depth": 5
            }
          ]
        }
      ]
    },
    {
      "id": "47149586",
      "title": "Show HN: A real-time strategy game that AI agents can play",
      "link": "https://llmskirmish.com/",
      "domain": "llmskirmish.com",
      "author": "__cayenne__",
      "score": 213,
      "comment_count": 79,
      "created_ts": 1772013765,
      "is_internal": false,
      "post_text": "I&#x27;ve liked all the projects that put LLMs into game environments. It&#x27;s been a weird juxtaposition, though: frontier LLMs can one-shot full coding projects, and those same models struggle to get out of Pokémon Red&#x27;s Mt. Moon.<p>Because of this, I wanted to create a game environment that put this generation of frontier LLMs&#x27; top skill, coding, on full display.<p>Ten years ago, a team released a game called Screeps. It was described as an &quot;MMO RTS sandbox for programmers.&quot; The Screeps paradigm of writing code and having it executed in a real-time game environment is well suited to LLMs. Drawing on a version of the Screeps open source API, LLM Skirmish pits LLMs head-to-head in a series of 1v1 real-time strategy games.<p>In my testing I found that Claude Opus 4.5 was the most dominant model, but it showed weakness in round 1 as it was overly focused on its in-game economy. Meanwhile, I probably spent a third of all code on sandbox hardening because GPT 5.2 kept trying to cheat by pre-reading its opponent&#x27;s strategies.<p>If there&#x27;s interest, I&#x27;m planning on doing a round of testing with the latest generation of LLMs (Claude 4.6 Opus, GPT 5.3 Codex, etc.).<p>You can run local matches via CLI. I&#x27;m running a hosted match runner with Google Cloud Run that uses isolated-vm. The match playback visualizer is statically served from Cloudflare.<p>I&#x27;ve created a community ladder that you can submit strategies to via CLI, no auth required. I&#x27;ve found that the CLI plus the skill.md that&#x27;s available has been enough for AI agents to immediately get started.<p>Website: <a href=\"https:&#x2F;&#x2F;llmskirmish.com\" rel=\"nofollow\">https:&#x2F;&#x2F;llmskirmish.com</a><p>API docs: <a href=\"https:&#x2F;&#x2F;llmskirmish.com&#x2F;docs\" rel=\"nofollow\">https:&#x2F;&#x2F;llmskirmish.com&#x2F;docs</a><p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;llmskirmish&#x2F;skirmish\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;llmskirmish&#x2F;skirmish</a><p>A video of a match: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=lnBPaZ1qamM\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=lnBPaZ1qamM</a>",
      "is_ask_hn": false,
      "matched_keywords": [
        "team"
      ],
      "comments": [
        {
          "top": "MY FELLOW HUMAN, this is amazing work!\nI foresee this laying the foundation for whole football stadia filled to the brim with people wanting to watch (and bet on!) competing teams of AI trained on military tactics and strategies!\nSoon enough we shall have AI-Olympics! Imagine that, MY FELLOW OXYGEN CONVERTING HUMAN FRIEND! Tens of thousands of robots and drones, all competing against each other in stadia across the planet, at the same time!\nI foresee a world wide, synchronized countdown marking the beginning of the biggest, greatest and definitively most unique, one-time-only spectacle in human history!\nKeep up the good work!",
          "author": "5o1ecist",
          "replies": [
            {
              "text": "This reminds me of the Unreal Tournament: Xan episode from the Secret Level series.\nLink for those curious or confused as to what I'm talking about:\n\nhttps://www.youtube.com/watch?v=1F-rAW3vXOU\nForcing AI to fight in an arena for our entertainment, what could go wrong? (this was tongue in cheek, I am fully aware LLM's currently don't have conscious thoughts or emotions)",
              "author": "softfalcon",
              "depth": 1
            }
          ]
        },
        {
          "top": "I know visualization is far from the most important goal here, but it really gets me how there's fairly elaborately rendered terrain, and then the units are just unnamed roombas with hard to read status indicators that have no intuitive meaning. Even in the match viewer I have no clue what's going on, there is no overlay or tooltip when you hover or click units either. There is a unit list that tries (and mostly fails) to give you some information, but because units don't have names you have to hover them in the list to have them highlighted in the field (the reverse does not work). Not exactly a spectator sport. Oh, but there is a way to switch from having all units in one sidebar to having one sidebar per player, as if that made a difference.\nI find this pretty funny because it seems like a perfect representation of what's easy with today's tools and what isn't\nLove the idea though",
          "author": "wongarsu",
          "replies": [
            {
              "text": "Yeah, it's all what you get when you basically ask an agent \"Build X\" without any constraints about how the UI and UX actually should work, and since the agents have about 0 expertise when it comes to \"How would a human perceive and use this?\", you end up with UIs that don't make much sense for humans unless you strictly steer them with what you know.",
              "author": "embedding-shape",
              "depth": 1
            },
            {
              "text": "Or maybe the simple answer is it looks exactly like the referenced game screeps. Probably a better explanation than hand waving away the faults of an agent.",
              "author": "infecto",
              "depth": 2
            }
          ]
        },
        {
          "top": "I’d love to see something like this in games like Beyond All Reasons.",
          "author": "nirav72",
          "replies": [
            {
              "text": "Check out the top starcraft AIs playing each other. They have like 40k apm its insane to watch.",
              "author": "AuthAuth",
              "depth": 1
            }
          ]
        },
        {
          "top": "Reminds me of the “Google AI Challenge” in 2011 called Ants [1], except the ‘AI’ is implemented using ‘AI’ now instead of human programmers.\nI was proud for getting the highest-ranked JavaScript-based implementation, but got absolutely crushed by the eventual winner.\n1. \nhttps://github.com/aichallenge/aichallenge",
          "author": "arscan",
          "replies": []
        },
        {
          "top": "At least until one of the competitors is overheard saying \"A strange game. The only winning move is not to play\"",
          "author": "EwanG",
          "replies": []
        }
      ]
    },
    {
      "id": "47151842",
      "title": "Show HN: Respectify – A comment moderator that teaches people to argue better",
      "link": "https://respectify.org/",
      "domain": "respectify.org",
      "author": "vintagedave",
      "score": 212,
      "comment_count": 214,
      "created_ts": 1772029279,
      "is_internal": false,
      "post_text": "My partner, Nick Hodges, and I, David Millington, have been on the Internet for a very long time -- since the Usenet days.  We’ve seen it all, and have long been frustrated by bad comments, horrible people, and discouraging discussions. We&#x27;ve also been around places where the discussion is wonderful and productive. How to get more of the latter and less of the former?<p>Current moderation tools just seem to focus on deletion and banning. Wouldn’t it be helpful to encourage productive discussion and <i>teach</i> people how to discuss and argue (in the debate sense) better?<p>A year ago we started building Respectify to help foster healthy communication. Instead of just deleting bad-faith comments, we suggest better, good-faith ways to say what folks are trying to say.  We help people avoid: * Logical fallacies (false dichotomy, strawmen, etc.) * Tone issues (how others will read the comment) * Relevance to the actual page&#x2F;post topic * Low-effort posts * Dog whistles and coded language<p>The commenter gets an explanation of what&#x27;s wrong and a chance to edit and resubmit. It&#x27;s moderation + education in one step. We want, too, to automate the entire process so the site owner can focus on content and not worry about moderation at all. And over time, comment by comment, quietly coach better thinking.<p>Our main website has an interactive demo: <a href=\"https:&#x2F;&#x2F;respectify.ai\" rel=\"nofollow\">https:&#x2F;&#x2F;respectify.ai</a>. As the demo shows, the system is completely tunable and adjustable, from &quot;most anything goes&quot; to &quot;You need to be college debate level to get by me&quot;.<p>We hope the result is better discussions and a better Internet. Not too much to ask, eh?<p>We love the kind of feedback this group is famous for and hope you will supply some!",
      "is_ask_hn": false,
      "matched_keywords": [
        "feedback",
        "communication"
      ],
      "comments": [
        {
          "top": "It seems to have a harder time with political news than more abstract concepts. I was able to pass the checks for the Algorithmic Radicalization and Echo Chamber articles with my first comments.\nHowever, I did not manage to express any opinion on the transgender rights article, from any political perspective, without being flagged. On one of the comments I tested, it gave me a suggested revision from this:\n\"This is another move in a pattern of limiting the rights of anyone who isn't a MAGA supporter.\"\nTo this:\n\"This seems to continue a trend where certain groups feel their rights are being limited, which could affect many people beyond just MAGA supporters.\"\nThe first comment isn't substantive, but the second is even worse, adding so much equivocation that it's meaningless. To add insult to injury, the detector also flagged its own suggested revision. Even if it had gone through, accepting these revisions would mean flooding a platform with LLM-speak, which is not conducive to discussion.\nHonest feedback: from a user perspective, the suggestions feel frustrating and patronizing, more so than if my comments were simply deleted. I would stop using a site that implemented this.\nFrom a site operator perspective, the kind of discourse it incentivizes seems jagged, subject to much stricter rules if the LLM associates a topic with political controversy. It feels opinionated and unpredictable, and the revisions it suggests are not of a quality I would want on a discussion board. The focus on positive language in particular seems like a reductive view of quality; what is the point of using an LLM if it's only doing basic sentiment analysis?",
          "author": "Miraste",
          "replies": [
            {
              "text": "Dave here -- I've tweaked a bunch of the internal rules during the HN discussion today, and your comment now passes (using the default settings.)\nAs for equivocation, that should be strongly dialed down too. It annoyed me too, it was \"mush\", and did not help. I hope you'll find the current version a lot more human.\nI'm grateful for the feedback! Changing it based on all these comments has been intense over the past couple of hours, but boy is it now significantly improved and I am super grateful to you and other commenters.",
              "author": "vintagedave",
              "depth": 1
            },
            {
              "text": "Perhaps in keeping with age-old internet behaviors, it completely fails to recognize sarcasm.",
              "author": "tacitusarc",
              "depth": 2
            },
            {
              "text": "As much as I love sarcasm that is done well, I do find that it translates very poorly to written text unless explicitly noted with /s or something like that.  Even when annotated, it's extremely rare that a sarcastic comment actually furthers discussion or makes a meaningful point.  If a person is using sarcasm, odds are pretty high that they aren't engaging substantively anyway.  Given the difficulties with detection (which even many humans fail at) it seems like trying to detect sarcasm would just make the tool a lot less useful and would be mostly antithetical to the project goals anyway.",
              "author": "freedomben",
              "depth": 3
            },
            {
              "text": "How shocking!",
              "author": "dcanelhas",
              "depth": 3
            },
            {
              "text": "That's understandable, humans do in many cases too",
              "author": "hvb2",
              "depth": 3
            }
          ]
        },
        {
          "top": "Decades ago (1995), \nwhen I was barely a decade old\n, I created a maintainence/repair website for a Motorola product (pre-iFixit). A fellow geek created his own similar website, focusing more on general product usage.\nNeither of us webmasters \ntook constructive feedback well\n, often lashing out at fellow usenet geeks \nwho were just trying to be helpful\n. Tantrums, from us both.\nTwenty years later we randomly met in-person @DEF-CON (recognized his unique name) — he ended up being \na year younger than me!\n We exchanged chuckles about what big personalities us two little kids had been, blasting angst into the aether.\nMotorola had linked to both our websites in their official documentation, despite our pottymouths =P\n----\nWhen I witness road rage (myself, included), I pretend the aggressor is a toddler. This makes it easier (and more effective!) to handle the rage that often passes through miscommunication(s).\n----\nI've been a forumjunkie since 1994, and HN is the only online forum I still participate within — mostly because of the techgenre, but also because the rules here prevent all sorts of perpetualSeptembers from scattering themselves among otherwise-constructive threads.\nDanG&co: thanks for cultivating an exceptional online community\nOP: Thanks for trying; I haven't used your product, but the premise seems noble... my main question to ya'll is: how do you prevent overbearing censorship (e.g. does karma influence how \"tough\" your product is on particular users, or are we all \nequally correctable\n)?",
          "author": "ProllyInfamous",
          "replies": [
            {
              "text": "I empathise. We definitely mature over time :) And we all have bad days.\nIf we can bring the HN kind of interaction approach to more sites, we'll call ourselves successful. Dan and co provide an inspiration through this site. Someone commented below something about automating dan, and... dan, if you read this, I laughed, but kinda ;) At least, bringing a chance for or what we can of the general HN approach to other spaces.\nRe specific users or karma, everyone is equal. Comments are judged on their own merits, within the context of the topic they are about (the API allows adding more context, such as other comments in the thread, which isn't shown in the demo.)\nWe've played around with the idea of building a kind of reputation over time, ie allowing people to build a score. If so, it's important to note that's not based on the content of what was written (eg specific political views) but based on how well, how healthily, someone expresses it. That line does blur because some opinions are inherently unhealthy, and cannot be expressed in a way that respects others, demonstrates decency / humanity, etc, but within the spectrum of 'being a decent person and just trying to interact well including while disagreeing' we specifically do \nnot\n want to police topics. We want instead to encourage, and in future maybe try to build a rep, for how well someone engages with others.\nAnd returning to your question, if we did that, every comment would \nstill\n be assessed standalone. We want people to grow, kinda like you were talking about. If someone expressed themselves poorly a year ago and behaves more healthily now, now is what should be reflected.",
              "author": "vintagedave",
              "depth": 1
            },
            {
              "text": ">allowing people to build a score\nI think of this as \"letting \ntheir voice\n speak\" — but yes, a \"score\"/tolerance of sorts.\n>now is what should be reflected.\ngrowth. context. rebirth. forgiveness.\nIt's always so shocking when decades-old writings discredit modern thinkers.",
              "author": "ProllyInfamous",
              "depth": 2
            }
          ]
        },
        {
          "top": "This thing seems to be more about enforcing a political PoV than about avoiding logical fallacies.\nAll my attempts to comment on the UBI article (and not supporting UBI) said my comment was a dogwhistle, and/or had an overly negative tone. This topic, of all things, is absolutely worthy to challenge and debate.\nUsing this would have the effect of creating an echo chamber, where people who stay never benefit from having their ideas challenged.",
          "author": "badc0ffee",
          "replies": [
            {
              "text": "Yeah I feel like this will funnel everyone's opinion into sounding like it was written by an AI.\nLove the idea but the example they give with bears is absolutely hilarious. Calling bears dumb animals is offensive? God help us.",
              "author": "ryanackley",
              "depth": 1
            },
            {
              "text": "Hah, the idea is to have an example on the site that is not offensive -- we're not going to write something offensive down -- but where you can understand what it \nwould\n be or \ncould\n be. It lets you infer / understand the point without us actually writing something awful. (Maybe we can do it better, though.)\nBears seemed a pretty inoffensive target, plus our backend uses Python with beartype and that library is all about bear jokes.",
              "author": "vintagedave",
              "depth": 2
            },
            {
              "text": "Can you give some examples of comments you made which you feel were reasonable but got flagged?",
              "author": "esperent",
              "depth": 1
            },
            {
              "text": "Thankyou — I’d love to hear what you wrote, if you wouldn’t mind sharing?\nWe’ve tried to aim it not to enforce any specific view — that’s a design goal — but focus on how it will feel to the other person.\nAlso things like logical fallacies or other non-emotional flaws in comments (there’s a toxicity metric for example, or dogwhistles).\nAn echo chamber is the exact opposite of what we want. There are too many already. What we hope for is guided communication so different views _can_ be expressed.",
              "author": "vintagedave",
              "depth": 1
            },
            {
              "text": "If that is happening, that is a huge problem.  We'll look at that right away.\nWe specifically \ndon't\n want that to be the case.  We want to encourage healthy, productive debate.\nWe may have the \"dog-whistle\" stuff over tuned.",
              "author": "NickHodges0702",
              "depth": 1
            }
          ]
        },
        {
          "top": "I think the better model is to just block everyone who isn't useful to communicate with. For instance the top of this HN page reads (for me): 68 comments | 11 hidden | 3 blocked\nThe hidden comments are from people in the Top 1000 by word count (who I usually don't want to hear from but if there is not much content I might click to toggle). The blocked are people I've seen argue with others in a useless way because they don't understand them or because they're just re-litigating or whatever (which I cannot toggle). I think it would be cool if people all published their blocklists and I'd pull from those I trust. Sometimes I open HN on my phone through the browser and I'm baffled by all these responses I got which are useless.\nI'm surprised by how much more high quality comment threads are now to me and I frequently find that I want to respond to everyone. It's like in old-school mailing lists or forums where you were having a conversation so the other people are worth talking to.\nAttention is precious and I wouldn't want to waste it on boring things. And it goes both ways. I communicate incompletely and there are people out there who get what I'm saying and there are people who need me to be more explicit. I would prefer that the latter and people who find me boring just block me.",
          "author": "arjie",
          "replies": [
            {
              "text": "This goes back to my early days on the internet, but: I do not use blocklists or ignore features except as an absolute last resort. Ignoring the problem is not a solution. In other ways I think it just makes the problem worse. If the person is not banned from the community, then your decision to pretend they don't exist just leaves other people to deal with it. Instead my feeling is that you should confront it by lobbying for their removal, or leave the community.\nSure, you may no longer see the noise, but that means that newcomers to your community do and have to deal with it. When you have a giant blocklist, you are ignoring your duty to police your own community.\nThen there is the issue of people blocking people who are simply more tolerant than they are. Hiding speech that is challenging to your personal views is a different kind of disaster.",
              "author": "devin",
              "depth": 1
            },
            {
              "text": "Haha, it's funny. I just disagree with you on about every point. I don't think that all the people I find noise are annoying to others. Some of them have 5000+ karma so others find them useful. And I don't want to leave the community so I'm not going to do that.\nThe community has some loose norms and I'm fine with that being the baseline. I don't want to police the community to strict norms. In fact, I would prefer if society were looser and we lived like in \nToo Like The Lightning\n. I can't have that there but I can here so I'm happy for it. The technology affords it.\nAs for blocking people who are more tolerant than me - that seems fine. Tolerance is not some unalloyed good. There are people who are tolerant of spam and all that and I don't really care for it. They're welcome to it and I'm welcome to mine.\nThe virtual world affords us a glorious opportunity: we don't have to worry about occupying the same space, and we don't have to worry about broadcast media like voice over air, we can silence and amplify as we see fit. To not use that is to take a skeuomorphic approach to a new parallel world, I think.\nI like that I don't need everyone to agree with me that someone belongs or doesn't belong. I can simply edit my user-agent to behave correctly for me and others can do so likewise. Free agents controlling their own experience of the world without impinging on others is great!",
              "author": "arjie",
              "depth": 2
            },
            {
              "text": "> I do not use blocklists or ignore features except as an absolute last resort\nI block people every single day. I've blocked so many people on Twitter that all I see is a very nice timeline with mostly stuff I like, some boring, but none of the culture wars garbage. Some days the timeline is completely empty, I suspect because Twitter can't cope with having blocked so many nodes in the social graph :D\n> If the person is not banned from the community, then your decision to pretend they don't exist just leaves other people to deal with it.\nWhen you start banning people outright from the entire community you risk ending up like any far left group, with schisms and civil wars between factions for absolutely tiny differences. The right approach is to have a very high bar for banning, and it's perfectly fine if people decide not to speak to one another.",
              "author": "ragall",
              "depth": 2
            },
            {
              "text": "If there's one good thing that could possibly come out of this AI revolution, it would be the ability for people to automate this across all their feeds.  I'd love it if I never had to waste time on toxicity, spam, or propaganda.\nAlthough, recent history would suggest that we'd just end up with even more powerful echo chambers.",
              "author": "alexose",
              "depth": 1
            },
            {
              "text": "You would end up in a more powerful echo chamber for sure - whichever side was best at avoiding your autoblocker while tricking the other side into activating it.",
              "author": "inigyou",
              "depth": 2
            }
          ]
        },
        {
          "top": "I am bitter about this.\nDo you really with your mind and with your heart believe that:\n- LLMs are fundamentally fit for this type of comprehension\n- Misjudgements posted in this thread are \"bugs\", \"errors\"\n- Agents who choose to act in bad faith will be anyhow affected\n- It is desirable by a majority of the group whose opinion you would even consider (is there such a group?), that everyone should have this kind of thing shoved into their face\n- Promotion of this kind of thing does not also promote (and help build) harsher censorship mechanisms\nDo you think that every single thing you will ever say publicly from now on will be considered constructive by all future filters with all of their different biases and \"bugs\"? Do you think that this new \"constructive speak\" will not make you want to blow your brains out at some point? Do you not see it everywhere already and get nauseus from it? I would prefer trash talk to that - at least seldom honest and \ntrue\n. If you don't like the message - hide it, timeout the poster, block them or whatever - with your own agency. If you think they welcome education from you - dm them a book.\nOr perhaps you imagine yourselves as above that kind of filtering? Then there is no question.\nAlso, nothing new under the sun. Can't remember exactly but I saw not long ago on a medical platform a review filtering system. It \"isn't\" censhorship per say, of course, the same as your idea. Only, you can't post a review you want - only a much more milder version (and therefore useless) with transformations akin: \"This thing doesn't work\" -> \"I felt like this thing didn't work for me in this instance, but there were such an such positives\". Way to go - turning everything into \"we are sorry you feel that way\".",
          "author": "izucken",
          "replies": []
        }
      ]
    },
    {
      "id": "47151163",
      "title": "Red Hat takes on Docker Desktop with its enterprise Podman Desktop build",
      "link": "https://thenewstack.io/red-hat-enters-the-cloud-native-developer-desktop-market/",
      "domain": "thenewstack.io",
      "author": "twelvenmonkeys",
      "score": 125,
      "comment_count": 78,
      "created_ts": 1772025953,
      "is_internal": false,
      "post_text": "",
      "is_ask_hn": false,
      "matched_keywords": [
        "enterprise"
      ],
      "comments": [
        {
          "top": "The best alternative to Docker Desktop on macOS is to abandon the GUI entirely use colima to create the linux VM.\n    brew install colima docker docker-buildx docker-completion docker-compose\n    export DOCKER_HOST=\"$HOME/.colima/docker.sock\" >> ~/.zshrc\n    source ~/.zshrc\n    colima start --cpus 8 --vz-rosetta --ssh-agent --mount $HOME:w\n\n\n\nthen add this line to your $HOME/.docker/config.json\n    \"cliPluginsExtraDirs\": [\"/opt/homebrew/lib/docker/cli-plugins\"],\n\n\n\nthat will get you a fast virtiofs VM with the latest docker, including compose and buildx. it may seem scary to replace an officially blessed tool like Docker Desktop, but i have had zero issues with colima. it isn't \"docker compatible\". it's docker. just need to run `brew upgrade` and `colima update` every once in a while to keep it up to date.",
          "author": "y0ssar1an",
          "replies": [
            {
              "text": "I've been using Finch (\nhttps://runfinch.com/\n) for the last few years, it basically is a managed Lima, nerdctl, containerd and BuildKit.\nI just alias docker to finch and it just works.",
              "author": "jpalmer",
              "depth": 1
            },
            {
              "text": "If you're going full CLI on macOS, I've had the best experience with:\n    brew install podman\n\n\nPodman manages the linux vm for you automatically.\nI've come to enjoy podman more than docker on my linux hosts anway; the default runtime (crun) is lighter than docker (runc), podman-kube-play is great for managing multi-container pods and is compatible with kubernetes. It also integrates very neatly with systemd. Of course there is the whole daemon-less and rootless side of the things as well..",
              "author": "seemaze",
              "depth": 1
            },
            {
              "text": "I’ve been using OrbStack instead of Docker Desktop and gotta say, I’d not replace it with anything else. So if anyones looking for a more automated alternative, check out OrbStack.",
              "author": "eknkc",
              "depth": 1
            },
            {
              "text": "I hear great things about OrbStack; unfortunately the licensing tied to their free offering doesn't play nicely with corporate environments (and we're cheap!).\nI switched to Colima instead and couldn't be happier.",
              "author": "cathalc",
              "depth": 2
            },
            {
              "text": "Thank you!!! Been struggling with time skew on Podman desktop for around a year now with no fix in sight. At least in the initial test since I saw your comment a few hours ago, this is working great!",
              "author": "a456463",
              "depth": 1
            }
          ]
        },
        {
          "top": "I tried to use podman desktop for a bit but I ran into some screwy compatibility issues. It just wasn't as smooth as docker.\nI really really want an alternative to docker desktop. I don't like the path they're going down. I don't like the AI crap in the UI. The licensing is crazy. It just doesn't feel right.\nSo I've been lately using rancher by SuSE. Surprisingly, it's been all right. So far it just works. I'm using this on Mac OS.\nIf anybody's looking for an alternative that's one worth considering.",
          "author": "bmurphy1976",
          "replies": [
            {
              "text": "I'm still confused by why anyone wants to use either Docker or Podman desktops. The the docker/Podman CLIs seem like a much better way to interact with containers/images. Maybe it's just my usecase.",
              "author": "scuff3d",
              "depth": 1
            },
            {
              "text": "I can't speak to docker, but the Podman desktop UI on MacOS doesn't really offer any functionality that the CLI doesn't. It's more like a status dashboard than anything else. I personally never look at it. I don't see how you can get very far managing containers, images, etc using _just_ the UI in any case.",
              "author": "bityard",
              "depth": 2
            },
            {
              "text": "Agreed. To be honest I feel the same way about k8s. A bunch of people on my team get grumpy if we don't have k9s available or some other interface, but I prefer to just use kubectl",
              "author": "scuff3d",
              "depth": 3
            },
            {
              "text": "The API is pretty extensive too. It works on a local socket that you can start on demand. (Lest people think there's a daemon or root requirement)",
              "author": "dizhn",
              "depth": 2
            },
            {
              "text": "I personally use Docker Desktop because it was the easiest way to install Docker on my Mac. I launch Docker Desktop, close the window but keep the app running in the background, then use the docker tool on the command line :)",
              "author": "mort96",
              "depth": 2
            }
          ]
        },
        {
          "top": "I love podman. it’s my default whenever i need to run containers locally. Ive also used it to run containerized systemd services.\nSelling enterprise licenses is a smart move from Redhat: they actually build/contribute to production grade container orchestration platforms like openshift. Unlike Docker Inc which looks like it only has the docker registry and Docker Desktop.",
          "author": "pm90",
          "replies": []
        },
        {
          "top": "Man, I feel bad for Docker, the company. Created the open source project that almost single-handely revolutionized deployments, development environments, and cloud computing, but sorta never managed to stick a product.",
          "author": "tonnydourado",
          "replies": [
            {
              "text": "I'm equally shocked nobody has bought them out to keep them well funded and not focused on trying to monetize (outside of just billing for private images). Every cloud provider like CloudFlare (I think?), Azure, AWS, GCP, etc benefit from Docker, it seems like a no brainer to me... You would then condense the org to just developers and PMs. Then marketing and other employees could be shifted to another part of the parent org and condense it down to a core group that builds and makes the tooling stronger.\nI wish we had tax exceptions for companies maintaining open-source projects full time to be reasonable write offs or something, with strict checks so companies dont just make random \"open source\" projects to write off, it should be something with known sizable impact and/or use, it would make some critical open source projects attractive \"buy outs\" or options to fully fund for some of these giants that benefit from them. Imagine if the devs entire salary (up to a point) could be written off completely. Some of these people are working on key infrastructure for the modern web, and even other critical systems, think of Chromium (tricky because of Chrome being not-open source but a proprietary end-product), Firefox, Linux, openssl, and obviously Docker, as good example.",
              "author": "giancarlostoro",
              "depth": 1
            },
            {
              "text": "> Every cloud provider [...] benefit from Docker\nHow ? Docker didn't invent the underlying technology and can't control it (through patents, etc...). It's all open and Docker tools are just the most popular but there are alternatives. Why pay when you can get it for free ?",
              "author": "ragall",
              "depth": 2
            },
            {
              "text": "To keep it maintained.",
              "author": "giancarlostoro",
              "depth": 3
            },
            {
              "text": "Cloud providers have no reason to. If Docker disappeared today, the alternatives would take over.",
              "author": "ragall",
              "depth": 4
            },
            {
              "text": "I think they are now doing better than ever. And they have been bought out already by Mirantis, unless I missed something.\nPodman isn't really a competitor at this point, it's just the \"docker at home\" NIH project from redhat. It works fine, but docker isn't going anywhere really.",
              "author": "mardifoufs",
              "depth": 1
            }
          ]
        },
        {
          "top": "I personally prefer the Podman CLI however as you don't need the daemon running in the background and prefer Kubernetes like yamls for local development. I definitely don't need a polished desktop GUI that shows me how many images I have though - I've never understood the use case for that.",
          "author": "osigurdson",
          "replies": [
            {
              "text": "Same. I switched to podman just so I don't have to troubleshoot why the docker daemon isn't running again.",
              "author": "dec0dedab0de",
              "depth": 1
            },
            {
              "text": "Used docker for over a decade, never ran into this docker daemon intermittently stops running issue.",
              "author": "newsoftheday",
              "depth": 2
            }
          ]
        }
      ]
    },
    {
      "id": "47147183",
      "title": "Ask HN: Share your productive usage of OpenClaw",
      "link": "https://news.ycombinator.com/item?id=47147183",
      "domain": "news.ycombinator.com",
      "author": "aavci",
      "score": 97,
      "comment_count": 77,
      "created_ts": 1771992206,
      "is_internal": true,
      "post_text": "What are some very productive things you achieved with OpenClaw that you wouldn’t mind sharing?",
      "is_ask_hn": true,
      "matched_keywords": [],
      "comments": [
        {
          "top": "We run an OpenClaw agent for our entire team — he lives in a group chat (although we have DMs too).\n- Runs our standups, checks in withe everybody EOD on blockers\n- Already know what we shipped on Github and Linear so it can focus on the work that's not tracked and summarize it in the morning for everyone\n- Helps with debugging customer issues\n- Keeps up with twitter and competitors and lets us know if they launch new features\nBesides, I'm honestly blown away by the social aspect of it. I was honestly pretty skeptical at first, but having an AI team mate is actually _fun_. There, I said it. Everybody on the team said they'd be sad if we took it away.\nI'll do a write-up on our setup sometime this week, I hope others will find our approach to security posture and multi-tenant usage insightful.",
          "author": "maebert",
          "replies": [
            {
              "text": "We did the same and I wrote (admittedly had AI write) about it.\nhttps://speedscale.com/blog/building-speedy-autonomous-ai-de...",
              "author": "stitched2gethr",
              "depth": 1
            },
            {
              "text": "Thanks for sharing. Can you share an estimate of how many tokens it uses over time? Would love to know how much it costs in terms of money.",
              "author": "apopapo",
              "depth": 2
            },
            {
              "text": "It all depends on the model and how much you use it of course.  We're running Opus 4.6 and on a light day it spends a dollar or two.  This is just a few simple operations like \"create a ticket for ...\" and it's regular heartbeat checks.  The heaviest day I see is $110 and on that day we were basically talking to it and having it implement features all day long.",
              "author": "stitched2gethr",
              "depth": 3
            },
            {
              "text": "In your experience, did you (or anyone) in the team/company felt that some non-tech people were not pulling their weight, example project managers/directors who didn't seem to bring enough value and if you did, found that using OpenClaw reduces the need for those positions?\nOr has anyone else?",
              "author": "hboon",
              "depth": 1
            },
            {
              "text": "Holy s*t loaded question batman",
              "author": "ucffool",
              "depth": 2
            }
          ]
        },
        {
          "top": "I have a claw (that is not OpenClaw, just another impl) and my wife and I communicate to it through a Telegram bot (we have a group chat, but we both DM it most of the time), it has its own email, and all that. A couple of things I had it do:\n* I was going through some SOC2 compliance vendor evals and I just messaged it as things were happening and it made me a nice doc at the end\n* My wife and I are planning a trip and we have a spreadsheet organized as a calendar. A friend asked when we'd be in Taiwan and my wife texted it to summarize the calendar into a text message to copy and it gave it to her.\n* I have it set up to warn me when to cover my bike so it doesn't get rained on, in the sense that I told it I wanted this functionality and it wrote something and scheduled it\n* It pulls my wife and my todo lists and gives me a top 3 in the morning to work on.\n* Every morning, it looks up Hacker News posts related to AI, filters out culture war type stuff and then sends me a short message about what it thinks will be interesting (new models, techniques, that sort of thing)\n* It watches some subreddits for sales of certain hardware (I'm interested in servers with SXM5 boards, Mac Studios with >64 GiB of RAM) and then notifies me when something matches\nOverall, it's all about mechanizing lots of parts of my life and using the advantage of a machine that understands text: it doesn't need sophisticated parsing logic. That's actually really nice.",
          "author": "arjie",
          "replies": [
            {
              "text": "Do you mind sharing how much this is costing? I'm a heavy claude code user, but if I had to pay the API rate, it would be a bit prohibitive",
              "author": "ffb7c5",
              "depth": 1
            },
            {
              "text": "Certainly, yes. It's about $75/week. The harness picks the model appropriately but it's mostly Sonnet with some rare Opus and Haiku/Gemma. I was previously running on DeepSeek 3.2 but while per token costs are low, the actual amount of tokens required to produce results is high so total costs were actually higher!\nI am also a Claude Code Max subscriber so the API use is in addition to the subscription, but it can't be helped. Claude Code is the best way for me to do work and the Claw is the best way for me to get an automated EA. I forgot something else: I also just text the bot to schedule meetings and it does that as well (I have a calendar delegated to its Google user).\nI've got some space in a datacenter, and I was vacillating on getting a card and running some open models but when I actually exercised them, it turned out that the quality of open models was too far below the Claudes for my use-case. Still if you've got a 300W Blackwell-based RTX 6000 Pro and you want to trade for some 4090s, email me.\nMy claw powered by Claude is pretty trustworthy for my use-cases.",
              "author": "arjie",
              "depth": 2
            },
            {
              "text": "“so the API use is in addition to the subscription, but it can't be helped.” - I beg to differ. OrcaBot.com is a claws that runs using vanilla Claude Code so you can do all that with your regular subscription. Disclosure: I’m the author. The only reason these other claws can’t offer that is because they front it with their own AI.",
              "author": "robbomacrae",
              "depth": 3
            },
            {
              "text": "That's pretty cool. And when I first tried this, I tried to do it with a bash loop around `claude -p` and you can get quite far with that! But overall, I think I'd rather use their tools the way they've set them up to be used and pay them their $500/month total or whatever. I'm probably going to stick to this approach, but your thing is pretty neat so thank you for sharing.",
              "author": "arjie",
              "depth": 4
            },
            {
              "text": "Just a heads up, I tried to use the continue with google button on your site, but running into \"Bot verification failed\". Using stock chrome browser, not running a VPN either",
              "author": "ffb7c5",
              "depth": 4
            }
          ]
        },
        {
          "top": "I’ve been running a modified setup for fintech workflows where the agent handles KYC verification and compliance checks across WhatsApp and Telegram. The multi-channel aspect is genuinely powerful — clients in Latin America strongly prefer WhatsApp for everything, including document submission.\nThe biggest pain point is memory. SOUL.md and Markdown memory files have no structure, no provenance, and no way to verify that what the agent “remembers” is consistent with its actual history. For regulated workflows where you need audit trails, flat files are a non-starter. I ended up building a semantic layer on top that stores facts as RDF triples with cryptographic provenance — essentially giving the agent verifiable memory instead of just text files.\nThe security posture required significant hardening. Running in Docker with network isolation, separate credentials per channel, and explicit tool allowlists. Even then, the prompt injection surface through incoming messages is real and unresolved.",
          "author": "throwback_dev",
          "replies": []
        },
        {
          "top": "I wrote a piece on this which was on the front page of HN a few weeks ago:\n\nhttps://brandon.wang/2026/clawdbot\nI still do all of the text automations, which have been pretty set-and-forget.",
          "author": "brdd",
          "replies": [
            {
              "text": "Thanks for sharing. That was a nice read",
              "author": "aavci",
              "depth": 1
            },
            {
              "text": "This is really cool! I am curious how much is the average daily cost for the kinds of things you are doing. Are you using hosted models or running one locally?",
              "author": "thelittlelight",
              "depth": 1
            },
            {
              "text": "Hosted models only. (my philosophy is that I need these things to be fast if I'm talking to them, and they also need to be maximally not wrong, which means cloud hosted big models even if they're expensive. To me, if it's wrong even once or if I'm sitting there waiting for it to reply, that's already making the value prop not worth it)\nI think per token costs I calculated on Opus 4.5/4.6 were like $0.30/day for my text automations; $0.60/day for a few things I do that load up the browser. In general, anything browser-based munches up a lot more token (expected). What can be a bit of sticker shock is if you're having it load a lot of large web pages in a long conversation-- that can be several dollars. In the grand scheme of things, several dollars is not a lot but certainly from a \"should I just go to the website myself\" it tips the scale. I'm usually more interested in doing things once to \"teach\" it what to do (e.g. how to check a price) and then having it run that as a dialed-in cron job\nHope this helps",
              "author": "brdd",
              "depth": 2
            }
          ]
        },
        {
          "top": "It overhauled my media server.\nI hadn't set it back up after moving. I gave OpenClaw ssh credentials and it updated the OS and packages, then couldn't get back in after a restart.\nI plugged in keyboard and screen and it was stuck at boot, couldn't mount a drive.\nI sent OpenClaw screenshots and it told me to type in journalctl commands. Then it had me modify fstab so boot could continue.\nAfter that, OpenClaw could get back in on its own. It found the drive I'd been using had 1300 bad sectors and was going to die. It saw that another drive was perfectly healthy. It said the bad disc sectors were all early and probably just filesystem metadata and my files were probably fine.\nIt copied 1.5Tb to the newer drive and restored everything.\nI probably would have thrown the whole box out, as I hadn't used it in a year and wasn't looking for a project like that.",
          "author": "jonahss",
          "replies": []
        }
      ]
    },
    {
      "id": "47170174",
      "title": "Launch HN: Cardboard (YC W26) – Agentic video editor",
      "link": "https://www.usecardboard.com/",
      "domain": "www.usecardboard.com",
      "author": "sxmawl",
      "score": 96,
      "comment_count": 48,
      "created_ts": 1772131118,
      "is_internal": false,
      "post_text": "Hey HN - we&#x27;re Saksham and Ishan, and we’re building Cardboard (<a href=\"https:&#x2F;&#x2F;www.usecardboard.com\">https:&#x2F;&#x2F;www.usecardboard.com</a>). It lets you go from raw footage to an edited video by describing what you want in natural language. There’s a demo video at <a href=\"https:&#x2F;&#x2F;www.usecardboard.com&#x2F;share&#x2F;fUN2i9ft8B46\">https:&#x2F;&#x2F;www.usecardboard.com&#x2F;share&#x2F;fUN2i9ft8B46</a>, and you can try the product out at <a href=\"https:&#x2F;&#x2F;demo.usecardboard.com\">https:&#x2F;&#x2F;demo.usecardboard.com</a> (no login required!)<p>People sit on mountains of raw assets - product walkthroughs, customer interviews, travel videos, screen recordings, changelogs, etc. - that could become testimonials, ads, vlogs, launch videos, etc.<p>Instead they sit in cloud storage &#x2F; hard drives because getting to a first cut takes hours of scrubbing through the raw footage manually, arranging clips in correct sequence, syncing music, exporting, uploading to a cloud storage to share, and then getting feedback on WhatsApp&#x2F;iMessage&#x2F;Slack, then re-doing the same thing again till everyone is happy.<p>We grew up together and have been friends for 15 years. Saksham creates content on socials with ~250K views&#x2F;month and kept hitting the wall where editing took longer than creating. Ishan was producing launch videos for HackerRank&#x27;s all-hands demo days and spent most of his time on cuts and sequencing rather than storytelling. We both felt that while tools like Premiere Pro and DaVinci are powerful, they have a steep learning curve and involve lots of manual labor.<p>So we built Cardboard. You tell it to &quot;make a 60s recap from this raw footage&quot; or &quot;cut this into a 20s ad&quot; or &quot;beat-sync this to the music I just added&quot; and it proposes a first draft on the timeline that you can refine further.<p>We built a custom hardware-accelerated renderer on WebCodecs &#x2F; WebGL2, there’s no server-side rendering, no plugins, everything runs in your browser (client-side). Video understanding tasks go through a series of Cloud VLMs + traditional ML models, and we use third party foundational models for agent orchestration. We also give a dropdown for this to the end user.<p>We&#x27;ve shipped 13 releases since November (<a href=\"https:&#x2F;&#x2F;www.usecardboard.com&#x2F;changelog\">https:&#x2F;&#x2F;www.usecardboard.com&#x2F;changelog</a>). The editor handles multi-track timelines with keyframe animations, shot detection, beat sync via percussion detection, voiceover generation, voice cloning, background removal, multilingual captions that are spatially aware of subjects in frame, and Premiere Pro&#x2F;DaVinci&#x2F;FCP XML exports so you can move projects into your existing tools if you want.<p>Where we&#x27;re headed next: real-time collaboration (video git) to avoid inefficient feedback loops, and eventually a prediction engine that learns your editing patterns and suggests the next low entropy actions - similar to how Cursor&#x27;s tab completion works, but for timeline actions.<p>We believe that video creation tools today are stuck where developer tools were in the early 2000s: local-first, zero collaboration with really slow feedback loops.<p>Here are some videos that we made with Cardboard:\n- <a href=\"https:&#x2F;&#x2F;www.usecardboard.com&#x2F;share&#x2F;YYsstWeWE9KI\">https:&#x2F;&#x2F;www.usecardboard.com&#x2F;share&#x2F;YYsstWeWE9KI</a>\n- <a href=\"https:&#x2F;&#x2F;www.usecardboard.com&#x2F;share&#x2F;nyT9oj93sm1e\">https:&#x2F;&#x2F;www.usecardboard.com&#x2F;share&#x2F;nyT9oj93sm1e</a>\n- <a href=\"https:&#x2F;&#x2F;www.usecardboard.com&#x2F;share&#x2F;xK9mP2vR7nQ4\">https:&#x2F;&#x2F;www.usecardboard.com&#x2F;share&#x2F;xK9mP2vR7nQ4</a><p>We would love to hear your thoughts&#x2F;feedback.<p>We&#x27;ll be in the comments all day :)",
      "is_ask_hn": false,
      "matched_keywords": [
        "feedback",
        "collaboration"
      ],
      "comments": [
        {
          "top": "Here's an agent skill that lets you do similar things: \nhttps://skills.sh/remotion-dev/skills/remotion-best-practice...\nhttps://www.remotion.dev/docs/ai/claude-code",
          "author": "flyingcircus3",
          "replies": [
            {
              "text": "We've played around with this and honestly have a lot of respect for what the Remotion team has built. Fun fact, I tinkered with it back in 2021 when they made those GitHub Wrapped videos, it was one of those projects that made me think differently about video on the web :)\nCardboard is a bit different though, aimed at non-developers who want to edit raw footage through natural language without writing any code. Motion graphics is on the roadmap and Remotion would hopefully be a natural fit when we get there.\nCool to see the space evolving from so many directions! :)",
              "author": "ishandeveloper",
              "depth": 1
            }
          ]
        },
        {
          "top": "For your example videos that you made with Cardboard: can you also put up the raw material that went into those videos?  Just looking at the output doesn't tell me anything. :thanks:!",
          "author": "1024core",
          "replies": [
            {
              "text": "Sure! Will share the raw material for all the videos.\nFor some of the examples we shared though, we've created sample projects right within the product itself. They contain the raw assets and the exact prompts used to create the videos. You can try them out directly at \nhttps://demo.usecardboard.com\n and see the whole process!",
              "author": "ishandeveloper",
              "depth": 1
            }
          ]
        },
        {
          "top": "Impressive UI.  I assume you must be doing some kind of RAG + audio/video transcription on all the media. What's RAG architecture did you go with?",
          "author": "popalchemist",
          "replies": [
            {
              "text": "we've found more success with similar directions to what claude code took. maybe its closer to hybrid+agentic RAG",
              "author": "sxmawl",
              "depth": 1
            },
            {
              "text": "Firefox is not supported ...\n\n\nBut why?",
              "author": "newbeeguy",
              "depth": 1
            },
            {
              "text": "Totally fair question. I've actually been a longtime Gecko/Firefox user myself, so this one stings a bit.\nThe short answer: Firefox doesn't support the File System Access API (\nhttps://caniuse.com/?search=File+System+Access+API\n).\nWe made a deliberate decision to go client-first. Video editing happens entirely in your browser without us uploading your entire footage on our end. No bandwidth costs for you, no storing your raw video on our servers. The File System Access API is what makes that possible, and unfortunately Firefox just doesn't have it yet.\nIt's not a forever thing though. For cloud-based projects where files live on our end anyway, Firefox support is very much on the roadmap. But for the local-first editing flow, our hands are a bit tied until Mozilla ships it.\nHope that makes sense, and fingers crossed Firefox adds support soon!",
              "author": "ishandeveloper",
              "depth": 2
            }
          ]
        },
        {
          "top": "This is amazing (I'll add you on LinkedIn).\nI recently started making videos for a loved one that lives far away, I started using CapCut and this is the kind of thing I was thinking \"I wish it did that\".\nI'll definitely try it out. Congrats!",
          "author": "moralestapia",
          "replies": [
            {
              "text": "that's really cool!\nlmk if i can help in any way :)",
              "author": "sxmawl",
              "depth": 1
            }
          ]
        },
        {
          "top": "Really impressive work guys! It seems like YC has funded a few companies attacking this but I think you all might have the best approach so far. Behind the scenes is the agent just editing using text/annotated timelines? I feel like the move is probably text for roughcut/narrative, then a vlm for digesting the initial roughcut, then adding broll and fixing timing issues. Feel free to steal my FCP xml generator. \nhttps://github.com/barefootford/buttercut",
          "author": "barefootford",
          "replies": [
            {
              "text": "happy that you liked our approach! also, i think it's a better idea to just give agent these tools and let it figure out its course of actions than giving it a specific workflow to work on - it seems like the world keeps reminding us the bitter lesson [\nhttp://www.incompleteideas.net/IncIdeas/BitterLesson.html\n] more frequently these days\nwill definitely check the XML exports, ty :)",
              "author": "sxmawl",
              "depth": 1
            },
            {
              "text": "Theoretically I agree, but practically without guidance agents aren't really able to edit video ATM. Without hand holding Claude will just call ffmpeg and look at a few frames.",
              "author": "barefootford",
              "depth": 2
            },
            {
              "text": "yeah we just ask a lot more questions to user to begin with",
              "author": "sxmawl",
              "depth": 3
            }
          ]
        }
      ]
    },
    {
      "id": "47164892",
      "title": "Men in their 50s may be aging faster due to toxic 'forever chemicals'",
      "link": "https://www.cnn.com/2026/02/26/health/forever-chemicals-aging-men-wellness",
      "domain": "www.cnn.com",
      "author": "jb1991",
      "score": 85,
      "comment_count": 106,
      "created_ts": 1772107093,
      "is_internal": false,
      "post_text": "",
      "is_ask_hn": false,
      "matched_keywords": [
        "toxic"
      ],
      "comments": [
        {
          "top": "This is about a journal article titled \"Emerging PFAS contaminants PFNA and PFSA amplify epigenetic aging: sex- and age-stratified risks in an aging population\" \nhttps://doi.org/10.3389/fragi.2025.1722675\n .\nThe OP says mentions a report from a US Academy but the paper is published in a different journal—this wasn't clear to me at first so I thought I would share the original work.",
          "author": "bonsai_spool",
          "replies": []
        },
        {
          "top": "I assume a lot of people 50+ were exposed to a lot more lead and cigarette smoke than younger people.",
          "author": "9999_points",
          "replies": [
            {
              "text": "Yes, for those under 30 you have no idea how normalized smoking was right thru the 90s.  Restaurants reeked of it, bars more so.  A ridiculous percent of men smoked.\nI have memories of being quite young sitting in a relatives lap at a baseball game while they smoked.  Or my coach in little league smoking a pipe in the dugout filled with 11 year olds.",
              "author": "steveBK123",
              "depth": 1
            },
            {
              "text": "I was explaining this to my elementary school aged kids just a few days ago. We were eating in a restaurant and I told them that when I was their age most restaurants had a smoking and non-smoking section. Of course the smoke did not respect the invisible barrier. The idea that people could just smoke indoors and it was normal really blew their minds.",
              "author": "seidleroni",
              "depth": 2
            },
            {
              "text": "High school boys bathroom was basically a de-facto smoking lounge.  It was banned but kids still did it.  They occasionally cracked down, but the smell was permanent.\nThere was also an unwritten understanding that it was preferred the boys went out back to a certain door to smoke outside there instead and wouldn't get in trouble if caught.",
              "author": "steveBK123",
              "depth": 3
            },
            {
              "text": "I went to bingo years ago and there was a glass partition between the smoking and non, but it didn't go to the ceiling. So you'd sit in the non and just watch a wave of cigarette smoke roll over the top of the glass into your area... I only went once because of that.",
              "author": "cbull",
              "depth": 3
            },
            {
              "text": "\"…had a smoking and non-smoking section\"\nYou're younger than me if you don't remember before there was even \nthat\n distinction.",
              "author": "JKCalhoun",
              "depth": 3
            }
          ]
        },
        {
          "top": "There's this theory that you can dilute the chemicals by doing frequent blood donations.",
          "author": "amelius",
          "replies": [
            {
              "text": "Is the leech back?",
              "author": "medi8r",
              "depth": 1
            },
            {
              "text": "If you are scared of needles you can have the leech. Donation is generally more useful though.",
              "author": "amelius",
              "depth": 2
            },
            {
              "text": "People mocked the leechers, they called them mad. And now look who’s crawling back. It turns out leeches really do cure diseases.",
              "author": "MagicMoonlight",
              "depth": 2
            },
            {
              "text": "Did it ever leave?  I thought it was still preferred in certain medical procedures.  I think something about after reattaching parts of the body it was one of the best options to ensure blood flow in the area, but it has been years since I last read about it.",
              "author": "SkyBelow",
              "depth": 2
            },
            {
              "text": "(Had to delete my comment in deference to your beating me by an hour.)",
              "author": "JKCalhoun",
              "depth": 2
            }
          ]
        },
        {
          "top": "If this were true, would we not see a corresponding drop in life expectancy?",
          "author": "HardwareLust",
          "replies": [
            {
              "text": "Dead people are easy to see, the issue is trying to differentiate one cause from everything else.\nSmoking for example wasn’t believed to be particularly deadly for a surprisingly long time.",
              "author": "Retric",
              "depth": 1
            },
            {
              "text": "My armchair scientific answer to that is: eventually, maybe. The problem we / science / medical / life expectancy has right now is that so much has and is changing as we speak. PFAS and microplastics only really became a thing after WW2, so while on the one side we banned asbestos, leaded fuel, smoking (and more recently drinking) got out of fashion, on the other there's microplastics everywhere, PFAS, vaping, various radiations, and by the looks of it the effects of a lot of these things will only slowly become apparent and statistically significant / measurable and discernable from other possible causes over a long period (30-50 years I'd guess, maybe longer), by which time there will be other factors at play too.",
              "author": "Cthulhu_",
              "depth": 1
            }
          ]
        },
        {
          "top": "Consuming fiber daily, e.g. psyllium husk, is another way to lower some types of PFAS from the body, although not all types of PFAS are going to mix into bile to facilitate this excretion. The way this works is that some PFAS dissolves in bile which binds to psyllium which is excreted.\nRemember, 5g psyllium should be taken with 20 fl oz water, mixed and consumed immediately before it gels. Do not take it two hours before or after any medicine, and do not take it if having swallowing difficulty.",
          "author": "OutOfHere",
          "replies": [
            {
              "text": "> psyllium husk\nIf you can source clean psyllium, the bulk of it comes from India and let's say they have different standards when it comes to lead/pesticides and regulations in general",
              "author": "lm28469",
              "depth": 1
            },
            {
              "text": "why use phylllium, it's clear that fiber in general works.  plenty of sources for that, and ways to get it.",
              "author": "red-iron-pine",
              "depth": 2
            },
            {
              "text": "I get the USDA Organic one which should handle the pesticides issue. I have asked the vendor to ensure the vendor test for heavy metals.\nNote that psyllium is unique in that not only does it not absorb, but it also binds to everything on the way out, so the risk is low. Fwiw, I have had blood and urine tests for lead twice.\nFinally, it's not as if the US is great with pesticides. We literally deliberately include PFAS in our pesticides.",
              "author": "OutOfHere",
              "depth": 2
            },
            {
              "text": "Source?",
              "author": "zendist",
              "depth": 1
            },
            {
              "text": "Here's one, googling is probably faster than asking for a source: \nhttps://link.springer.com/article/10.1186/s12940-025-01165-8",
              "author": "Cthulhu_",
              "depth": 2
            }
          ]
        }
      ]
    },
    {
      "id": "47169518",
      "title": "Show HN: Deff – Side-by-side Git diff review in your terminal",
      "link": "https://github.com/flamestro/deff",
      "domain": "github.com",
      "author": "flamestro",
      "score": 81,
      "comment_count": 50,
      "created_ts": 1772128446,
      "is_internal": false,
      "post_text": "deff is an interactive Rust TUI for reviewing git diffs side-by-side with syntax highlighting and added&#x2F;deleted line tinting. It supports keyboard&#x2F;mouse navigation, vim-style motions, in-diff search (&#x2F;, n, N), per-file reviewed toggles, and both upstream-based and explicit --base&#x2F;--head comparisons. It can also include uncommitted + untracked files (--include-uncommitted) so you can review your working tree before committing.<p>Would love to get some feedback",
      "is_ask_hn": false,
      "matched_keywords": [
        "feedback"
      ],
      "comments": [
        {
          "top": "I was looking for a good TUI tool for diffs recently, but I'm not sure yet if what I want exists already (and I don't think this tool does it (yet?)). I've been moving my workflow out of VSCode as I'm using TUI-driven coding agents more often lately but one thing I miss from my VSCode/GitHub workflow is the ability to provide a comment on lines or ranges in a diff to provide targeted feedback to the agent. Most diff tools seem to be (rightfully) focused on cleanly visualizing changes and not necessarily iterating on the change.\nI admit I haven't looked super hard yet, I settled on configuring git to use delta [0] for now and I'm happy with it, but I'm curious if anyone has a workflow for reviewing/iterating on diffs in the terminal that they'd be willing to share. Also  open to being told that I'm lightyears behind and that there's a better mental model for this.\n[0] \nhttps://github.com/dandavison/delta/",
          "author": "llbbdd",
          "replies": [
            {
              "text": "Octo [0] for nvim lets you submit reviews, add comments on ranges, reply to threads, etc.\nThis in conjunction with gh-dash [1] to launch a review can get you a pretty nice TUI review workflow.\n[0] \nhttps://github.com/pwntester/octo.nvim\n[1] \nhttps://github.com/dlvhdr/gh-dash\n*Edit: I see you meant providing feedback to an agent, not a PR. Well that's what I get for reading too fast.",
              "author": "kodomomo",
              "depth": 1
            },
            {
              "text": "No problem, I appreciate another reason to look at Neovim; I do sometimes have a need to interact with GH's actual PR flow and once I've moved the rest of my workflow out of VSCode, Neovim looks like the best option for the last mile of actually writing and editing code. I just have to commit the time to set it up with everything I probably take for granted in VSCode's editor.",
              "author": "llbbdd",
              "depth": 2
            },
            {
              "text": "I had tried `delta` a few years ago but eventually went with `diff-so-fancy`[1]\nThe two are kind of similar if I remember correctly, and both offer a lot of config options to change the style and more. I mostly use it for diffs involving long lines since it highlights changes \nwithin\n a line, which makes it easier to spot such edits.\nI have an alias set in `~/.gitconfig` to pipe the output of `git diff` (with options) to `diff-so-fancy` with `git diffs`:\n    diffs = \"!f() { git diff $@ | diff-so-fancy; }; f\"\n\n\n\n[1] \nhttps://github.com/so-fancy/diff-so-fancy",
              "author": "thamer",
              "depth": 1
            },
            {
              "text": "You can do this with diff-highlight, which comes packaged with git. No extra packages needed.",
              "author": "petepete",
              "depth": 2
            },
            {
              "text": "Checkout \nhttps://github.com/agavra/tuicr\n - it's built exactly for this purpose (reviewing code in your terminal and then adding comments and exporting it to an agent to fix).",
              "author": "agavra",
              "depth": 1
            }
          ]
        },
        {
          "top": "What I would love to see is \"tig\" replacement that is:\n- even faster, especially if you have couple thousand files and just want to press \"u\" for some time and see them very quickly all get staged\n- has this split-view diff opened for a file\nOtherwise tig is one of my favorite tools to quickly commit stuff without too many key presses but with review abilities, i have its \"tig status\" aliased to \"t\"",
          "author": "k_bx",
          "replies": []
        },
        {
          "top": "I have been using \nhttps://github.com/jeffkaufman/icdiff\n for the longest time to get side by side diffs.",
          "author": "meain",
          "replies": [
            {
              "text": "I also use icdiff, but it is good to have the file-awareness for git diff esp. the ability to quickly skip files that I know aren't important.",
              "author": "lf-non",
              "depth": 1
            },
            {
              "text": "For that in particular, I use delta (<\nhttps://github.com/dandavison/delta\n>) with `side-by-side = true` enabled.  I find I use both icdiff and delta side-by-side on a regular basis.",
              "author": "Amorymeltzer",
              "depth": 2
            },
            {
              "text": "Delta is so much faster than icdiff too.",
              "author": "behnamoh",
              "depth": 3
            },
            {
              "text": "This looks great as well! I personally prefer a bit more context. Thats why I added a bit more of it to deff. It also allows to mark files as reviewed by pressing `r` which is quite handy for my flow.",
              "author": "flamestro",
              "depth": 1
            }
          ]
        },
        {
          "top": "getting users to adopt a new tool with its own incantations is a tough sell. git supports specifying an external pager so folks can plug in alternatives (such as \nhttps://github.com/dandavison/delta\n) while still using the familiar git frontend",
          "author": "rileymichael",
          "replies": []
        },
        {
          "top": "What you want is difftastic. No need to thank me.",
          "author": "suralind",
          "replies": []
        }
      ]
    },
    {
      "id": "47152355",
      "title": "Ask HN: Have top AI research institutions just given up on the idea of safety?",
      "link": "https://news.ycombinator.com/item?id=47152355",
      "domain": "news.ycombinator.com",
      "author": "DietaryNonsense",
      "score": 79,
      "comment_count": 88,
      "created_ts": 1772031320,
      "is_internal": true,
      "post_text": "I understand there&#x27;s a difference between the stated values and actual values of individuals and organizations, and so I want to ask this in the most pragmatic and consequentialist way.<p>I know that labs, institutions, and so on have safety teams. I know the folks doing that work are serious and earnest about that work. But at this point are these institutions merely pandering to the notion of safety with some token level of investment? In the way that a Casino might fund programs to address gambling addiction.<p>I&#x27;m an outsider and can only guess. Insider insight would be very appreciated.",
      "is_ask_hn": true,
      "matched_keywords": [],
      "comments": [
        {
          "top": "\"safe\" is such a subjective concept to begin with, have any of the model providers ever defined what they mean by \"safe\"?\nIt doesn't mean much to me if a safe model is one that does not output the recipe for mustard gas, that information is trivially available elsewhere.\nOr, is a safe model one that doesn't come off as racist? Ok but i would classify that as unoffensive instead of safe but I admit definitions of words can be fluid and change.\nIs a safe model one that refuses to produce code for a weapons system? Well.. does a PID controller count? I can use that to keep a gun pointed at a target or i can use that to prevent a baby rocker from falling over.\nMaybe they're giving up on \"safe\" because there's no definitive way to know if a model is safe or not. I've always held the opinion that ai safety was more about brand safety. Maybe now the model providers can afford some bad press and it not be the death of their company.",
          "author": "chasd00",
          "replies": [
            {
              "text": "My preferred version of \"safe\" is \"in its actions considers and mostly upholds usually unstated constraints like 'don't kill unless necessary', 'keep Earth inhabitable', 'avoid toppling society unless really well justified for the greater good', etc. The kind of framing that was prevalent pre-ChatGPT. Not terribly relevant for a chat software, but increasingly important as chat models turn into agents.\nOf course once you have that framing, additional goals like \"don't give people psychosis\", \"don't give step-by-step instructions on making explosives, even if wikipedia already tells you how to do it\" or \"don't harm our company's reputation by being racist\" are conceptually similar.\nOn the other hand \"don't make weapon systems\" or \"never harm anyone\" might not be viable goals. Not only because they are difficult to impossible to define, but also because there is huge financial and political pressure not to limit your AI in that way (see Anthropic)",
              "author": "wongarsu",
              "depth": 1
            },
            {
              "text": "> I can use that to keep a gun pointed at a target or i can use that to prevent a baby rocker from falling over.\nThis leads to what I'm going to call the \"Ender's Game\" approach: if your AI is uncooperative just present it with a simulation that it does like but which maps onto real-world control that it objects to.\n> I've always held the opinion that ai safety was more about brand safety\nYes. The social media era made that very important. The extent to which brand safety is linked to actual, physical safety then becomes one of how you can manage the publicity around disasters. And they're doing a pretty good job of denying responsibility.",
              "author": "pjc50",
              "depth": 1
            },
            {
              "text": "What if I tell the model to go commit fraud or crimes and it complies? What if users are having psychotic episodes driven by their interactions with the model?\nJust because safety is a hard and messy problem doesn't mean we should just wash our hands of it.",
              "author": "LordHumungous",
              "depth": 1
            },
            {
              "text": "It \nis\n a hard and messy problem, and it doesn't help when people muddy the water further by stirring things like \"Don't commit fraud,\" \"Don't infringe on Disney's trademark,\" and \"Don't be racist\" into the mix and try to lump those things under the \"Safety\" umbrella.\nMaybe this is an outdated definition, but I've always thought of safety as being about \npreventing injury\n. Things like safety glasses and hardhats on the work site, warning about slippery floors and so on. I think people are trying to expand the word to mean a great many more things in the context of AI, which doesn't help when it comes to focusing on it.\nI think we need a different, clearer word for \"The AI output shouldn't contain certain unauthorized things.\"",
              "author": "ryandrake",
              "depth": 2
            },
            {
              "text": "The more messy a problem is, the less it should be decoupled and siloed into its own team.\nInstead of making actual improvement on the subject (you name it, safety, security, etc), it becomes a checkbox exercise and metrics and bureaucracies become increasingly decoupled from truth.",
              "author": "Aperocky",
              "depth": 2
            }
          ]
        },
        {
          "top": "Not an insider but someone who uses the tools. It's a branding update, nothing more. The models haven't gotten any less sanctimonious, but the companies behind them have stopped harping on their restrictions in order to appeal to a broader customer base (gov contracts, etc.)\nSo the guardrails (for you and me) are still there. They just stopped committing the unforced error of excluding themselves from federal procurement. Under a different administration, the requirement might change, and you might see them boasting once more on \"safety.\"",
          "author": "akersten",
          "replies": [
            {
              "text": "I don't think it's sanctimonious to say, hey, I don't want the technology I work on to be used for targeting decisions when executing people from the sky. Especially as the tech starts to play more active roles. You know governments will be quick to shift blame to the model developers when things go wrong.",
              "author": "toddmorey",
              "depth": 1
            },
            {
              "text": "> I don't want the technology I work on to be used for targeting decisions when executing people from the sky\none problem i have with this specific case and Anthropic/Claude working with the DOD is I feel an LLM is the wrong tool for targeting decisions. Maybe given a set of 10 targets an LLm can assist with compiling risks/reward and then prioritizing each of the 10 targets but it seems like there would be much faster and better way to do that than asking an LLM. As for target acquisition and identification, i think an LLM would be especially slow and cumbersome vs one of the many traditional ML AIs that already exist. DOD must be after something else.",
              "author": "chasd00",
              "depth": 2
            },
            {
              "text": "> I don't want the technology I work on to be used for targeting decisions when executing people from the sky\nWhat do you do when the government come to you and tell you that they \ndo\n want that, and can back it up with threats such as nationalizing your technology? (see Anthropic)\nWe're back to \"you might not care about politics, but that won't stop politics caring about you\".",
              "author": "pjc50",
              "depth": 2
            },
            {
              "text": "I know this is a foreign concept to some, but you can have a backbone.\nChallenge it in court. Move the company to a different jurisdiction. Burn everything down and refuse to comply.",
              "author": "dminik",
              "depth": 3
            },
            {
              "text": "> I know this is a foreign concept to some, but you can have a backbone.\nChallenge it in court. Move the company to a different jurisdiction. Burn everything down and refuse to comply.\nChallenge in court is fine, even healthy.\nThreatening to burn everything down and refuse to comply might well work; simply daring Trump to a game of Russian Roulette about this popping the bubble that's only just managing to keep the US economy out of recession, on the basis that he TACOs a lot, I can see it working in a way it wouldn't if he were a sane leader making the same actual demands just for sane reasons.\nMove the company to a different jurisdiction? That would have worked if AI was a few hundred people and a handful of servers, as per classic examples of:\n  At the height of its power, Kodak employed more than 140,000 people and was worth $28 billion. They even invented the first digital camera. But today Kodak is bankrupt, and the new face of digital photography has become Instagram. When Instagram was sold to Facebook for a billion dollars in 2012, it employed only 13 people. Where did all those jobs disappear? And what happened to the wealth that all those middle class jobs created?\n\n\n- Jaron Lanier, \"Who Owns the Future?\", \nhttps://www.goodreads.com/work/quotes/21526102-who-owns-the-...\nBut (I think) now that AI needs new data centres so fast and on such a scale that they're being held back by grid connection and similar planning permission limits, this isn't a viable response.\nThey can be burned down, but I think they can't realistically be moved at this point. That said, I guess it depends on how much Anthropic relies on their own data centres vs. using 3rd parties, given Amazon's announced AWS sovereign cloud in Europe?",
              "author": "ben_w",
              "depth": 4
            }
          ]
        },
        {
          "top": "Safety means slower and this is viewed as a winner takes all game.\nThis isn't new either, the safety glass cracked the day OpenAI publicly launched ChatGPT. \"Safety\" was (and perhaps still is) a fall back for the models plateauing and LLMs failing to really make an impact...\"we need more time while we focus on safety\"\nBut after this latest round of models, it's a lot more fuel on the \"this could be it\" fire. Labs are eager to train on the new gigawatt scale datacenters coming online, and it's very hard to make a case right now that the we won't get another step-change up in capability. Safety just obstructs all that.",
          "author": "WarmWash",
          "replies": [
            {
              "text": "Research was being done slower, until OpenAI forced everyone to jump the gun or potentially be left behind. For a few months it looked like everyone was light years behind them.",
              "author": "ASalazarMX",
              "depth": 1
            }
          ]
        },
        {
          "top": "If there is a VC-backed for-profit company, the core part is how much value something brings.\n\"Safety\" here works for both PR and hiring (a lot of talented engineers and researchers might flock to it), and maybe soft power for legislation. Compare and contrast with \"Don't be evil\" by Google.\nI do not say that individual employees do not care about safety - many do. And well, a lot don't, what is very visible during this OpenClaw mania.\nIn any case, words are cheap - it is always better to see what the actual actions are.",
          "author": "stared",
          "replies": []
        },
        {
          "top": "Yes. Ten years ago I would say there was a consensus in the ML community that if we got really powerful AI, it should be kept isolated in controlled environments (no internet, no way to execute code) until it could be trusted/verified. Fast forward: openclaw. People don’t seem to care, why should the labs?",
          "author": "marviio",
          "replies": []
        }
      ]
    },
    {
      "id": "47148025",
      "title": "Show HN: Context Mode – 315 KB of MCP output becomes 5.4 KB in Claude Code",
      "link": "https://github.com/mksglu/claude-context-mode",
      "domain": "github.com",
      "author": "mksglu",
      "score": 77,
      "comment_count": 23,
      "created_ts": 1772000610,
      "is_internal": false,
      "post_text": "Every MCP tool call dumps raw data into Claude Code&#x27;s 200K context window. A Playwright snapshot costs 56 KB, 20 GitHub issues cost 59 KB. After 30 minutes, 40% of your context is gone.<p>I built an MCP server that sits between Claude Code and these outputs. It processes them in sandboxes and only returns summaries. 315 KB becomes 5.4 KB.<p>It supports 10 language runtimes, SQLite FTS5 with BM25 ranking for search, and batch execution. Session time before slowdown goes from ~30 min to ~3 hours.<p>MIT licensed, single command install:<p>&#x2F;plugin marketplace add mksglu&#x2F;claude-context-mode<p>&#x2F;plugin install context-mode@claude-context-mode<p>Benchmarks and source: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;mksglu&#x2F;claude-context-mode\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;mksglu&#x2F;claude-context-mode</a><p>Would love feedback from anyone hitting context limits in Claude Code.",
      "is_ask_hn": false,
      "matched_keywords": [
        "feedback"
      ],
      "comments": [
        {
          "top": "One moment you're speaking about context but talking in kilobytes, can you confirm the token savings data?\nAnd when you say only returns summaries, does this mean there is LLM model calls happening in the sandbox?",
          "author": "handfuloflight",
          "replies": [
            {
              "text": "For your second question: No LLM calls. Context Mode uses algorithmic processing — FTS5 indexing with BM25 ranking and Porter stemming. Raw output gets chunked and indexed in a SQLite database inside the sandbox, and only the relevant snippets matching your intent are returned to context. It's purely deterministic text processing, no model inference involved.",
              "author": "mksglu",
              "depth": 1
            },
            {
              "text": "Excellent, thank you for your responses. Will be putting it through a test drive.",
              "author": "handfuloflight",
              "depth": 2
            },
            {
              "text": "Sure, thank you for your comment!",
              "author": "mksglu",
              "depth": 3
            },
            {
              "text": "Hey! Thank you for your comment! There are test examples in the README. Could you please try them? Your feedback is valuable.",
              "author": "mksglu",
              "depth": 1
            }
          ]
        },
        {
          "top": "Nice trick. I’m going to see how I can apply it to tool calls in pi.dev as well",
          "author": "rcarmo",
          "replies": [
            {
              "text": "That means a lot, thank you! Would love to hear your feedback once you try it — and an upvote would be much appreciated if you find it useful",
              "author": "mksglu",
              "depth": 1
            }
          ]
        },
        {
          "top": "Interesting approach, I tried the Hackernews example from the docs, but its tools don't seem to trigger reliably. Any suggestions?\n> Fetch the Hacker News front page, extract all posts with   titles, scores,\n  and domains. Group by domain. Then run /context-mode stats.\n* Claude used regular fetch *\n> why didnt you use the context mode fetch?\n● Fair point. Two honest reasons:\n  1. First request: The context-mode tools weren't loaded yet when I called WebFetch. I should have used ToolSearch to load    \n  fetch_and_index first, then used it — but I defaulted to WebFetch out of habit.                                              \n  2. Second request (the interrupted one): The tools were already loaded at that point. I had no excuse. I should have done:   \n  fetch_and_index(url: \"https://news.ycombinator.com\", source: \"HN\")                                                           \n  → search(queries: [\"post title score domain\"], source: \"HN\")\n  → execute(group by domain, print summary)\n  2. Instead I reached for WebFetch again — that's the anti-pattern the plugin exists to prevent.\n\n  The context-mode skill instructions explicitly list this in the decision tree:\n\n  Fetching web documentation or HTML page? → Use fetch_and_index → search\n\n  I ignored it. The WebFetch output (the full post table) went straight into context when it didn't need to. I'll use\n  fetch_and_index correctly if you want to retry.",
          "author": "wobblywobbegong",
          "replies": [
            {
              "text": "That's a known bug in older versions — the WebFetch hook wasn't blocking reliably. Fixed in v0.7.1.\nnpm install -g context-mode@latest\nIf you're on the plugin install, re-run:\n  /plugin marketplace add mksglu/claude-context-mode\n  /plugin install context-mode@claude-context-mode\n\n\nThen restart Claude Code. Sorry about that.",
              "author": "mksglu",
              "depth": 1
            }
          ]
        },
        {
          "top": "The BM25+FTS5 approach without LLM calls is the right call - deterministic, no added latency, no extra token spend on compression itself.\nThe tradeoff I want to understand better: how does it handle cases where the relevant signal is in the \"low-ranked\" 310 KB, but you just haven't formed the query that would surface it yet? The compression is necessarily lossy - is there a raw mode fallback for when the summarized context produces unexpected downstream results?\nAlso curious about the token count methodology - are you measuring Claude's tokenizer specifically, or a proxy?",
          "author": "vicchenai",
          "replies": [
            {
              "text": "Great questions.\n--\nOn lossy compression and the \"unsurfaced signal\" problem:\nNothing is thrown away. The full output is indexed into a persistent SQLite FTS5 store — the 310 KB stays in the knowledge base, only the search results enter context. If the first query misses something, you (or the model) can call search(queries: [\"different angle\", \"another term\"]) as many times as needed against the same indexed data. The vocabulary of distinctive terms is returned with every intent-search result specifically to help form better follow-up queries.\nThe fallback chain: if intent-scoped search returns nothing, it splits the intent into individual words and ranks by match count. If that still misses, batch_execute has a three-tier fallback — source-scoped search → boosted search with section titles → global search across all indexed content.\nThere's no explicit \"raw mode\" toggle, but if you omit the intent parameter, execute returns the full stdout directly (smart-truncated at 60% head / 40% tail if it exceeds the buffer). So the escape hatch is: don't pass intent, get raw output.\nOn token counting:\nIt's a bytes/4 estimate using Buffer.byteLength() (UTF-8), not an actual tokenizer. Marked as \"estimated (~)\" in stats output. It's a rough proxy — Claude's tokenizer would give slightly different numbers — but directionally accurate for measuring relative savings. The percentage reduction (e.g., \"98%\") is measured in bytes, not tokens, comparing raw output size vs. what actually enters the conversation context.",
              "author": "mksglu",
              "depth": 1
            }
          ]
        },
        {
          "top": "Interesting approach.  I just finished some work for a similar task in a different domain.\nOne thing that surprised me: tantivy's BM25 search is faster, more expressive, and more scalable than SQLite.  If you're just building a local search (or want to optimize for local FTS), I would strongly recommend looking into tantivy.\nIf you have the resources, it would be very interesting to throw a some models (especially smart-but-context-constrained cheaper ones) at some of the benchmark programming problems and see if this approach can show an effective improvement.",
          "author": "i3oi3",
          "replies": [
            {
              "text": "On Tantivy: Agree it's the better search engine, but context-mode is session-scoped — DB is a temp file that dies when the process exits. At that scale (50-200 chunks), FTS5 is zero-config, single-file, <1ms startup, and good enough. If we ever add persistent cross-session indexing, Tantivy would be the move.\nOn benchmarking: This is the experiment I most want to see. The hypothesis: context-mode benefits smaller models disproportionately — a 32K model with clean context could outperform a 200K model drowning in raw tool output. Would love to see SWE-bench results with context-mode on vs. off across model tiers.",
              "author": "mksglu",
              "depth": 1
            }
          ]
        }
      ]
    },
    {
      "id": "47165299",
      "title": "Fentanyl makeover: Core structural redesign could lead to safer pain medications",
      "link": "https://www.scripps.edu/news-and-events/press-room/2026/20260211-janda-molecule.html",
      "domain": "www.scripps.edu",
      "author": "littlexsparkee",
      "score": 67,
      "comment_count": 83,
      "created_ts": 1772109868,
      "is_internal": false,
      "post_text": "",
      "is_ask_hn": false,
      "matched_keywords": [
        "lead"
      ],
      "comments": [
        {
          "top": "Finally! All the benefits of the opioids, with none of the dangers.\nFor clarity: I'm referring to all the previous attempts to \"fix\" the synthetic opioids, each of which ended up making a stronger, more dangerous opioid.",
          "author": "bheadmaster",
          "replies": [
            {
              "text": "The danger of addiction, which is very significant, with opioids doesn’t go away with this modified design.\nUnless you’re being sarcastic and referencing the lies the Sackler family used to get OxyContin popular..\nThat being said it is indeed quite cool that they modified the drug to decrease the respiratory depression.",
              "author": "ViktorRay",
              "depth": 1
            },
            {
              "text": "Not just OxyContin. Also Heroin, Meperidine and Tramadol.\nWe get another \"morphine, but safe this time\" in pretty reliable 40 year intervals. I guess someone decided OxyContin doesn't count and we are due for another one",
              "author": "wongarsu",
              "depth": 2
            },
            {
              "text": "Sr-17018 is making rounds now (ok was a year ago) among people using opiates too much. On forums, it is pushed as the miracle stuff which allows lowering dosage without major withdrawal.",
              "author": "theragra",
              "depth": 3
            },
            {
              "text": "To be honest I would prefer addicts could get heroin prescribed. The primary danger of street drugs is the inconsistent purity and chemicals it’s cut with. If it was pharmaceutical grade and everyone prescribed was on a list, we would have fewer overdoses and a better understanding of who to put in treatment",
              "author": "monero-xmr",
              "depth": 3
            },
            {
              "text": "Most heroin overdoses happen either from a sudden increase in supply purity, or from an abstinent addict relapsing and taking their regular dose without realizing they have lost their tolerance.\nAny kind of rational change in policy is not happening as long as entire lucrative industries of policing, health care and religion-as-a-social-service are dependent on the dependent.",
              "author": "cluckindan",
              "depth": 4
            }
          ]
        },
        {
          "top": "We really could use better treatments for chronic pain.\nI've found low dose naltrexone to be somewhat effective for severe chronic pain. Not as good as opiods.\nTHC can also help somewhat, but its action seems so dissociative. At an effective level for chronic pain, I'm sleepwalking though the day.\nOpioids or their analogues cause or complicate bowel issues. Four years of 200mg/day Tramadol really helped me, but it shredded my gut. Getting off Tramadol wasn't hard for me. I'd stay on it were it not for the gut issues.\nAs an aside, lacing hydrocodone with acetaminophen is truly a horrific practice. Doctors prescribe this to patients on hepotoxic drugs and are shocked when they get liver damage.",
          "author": "clcaev",
          "replies": [
            {
              "text": "I have 2 family members for whom Tramadol opened the door for severe addiction. One is now on regular morphine, the other had psychosis. I know it obvisouly depends on the individual,  just to dilute your very rosy comment",
              "author": "dgan",
              "depth": 1
            },
            {
              "text": "I didn't mean to make a rosy comment, thanks for the ribbing.\nTramadol isn't all that strong, but it does take the edge off. With a 6 week taper, my challenge was more about the resurgent pain.\nI would not recommend Tramadol, the gut complications are debilitating and it's unclear ignoring the chronic pain served me well.",
              "author": "clcaev",
              "depth": 2
            },
            {
              "text": "Right. Opioids are an absolute terror to one's digestive system. When I had chronic pain I would rather have just accepted the pain than deal with the gut consequences.\nLDN is an interesting one since it just stimulates your body to generate its own endorphins.",
              "author": "trollbridge",
              "depth": 1
            },
            {
              "text": "I found the first week on LDN to be challenging due to \"horror flick\" vivid dreams.\nLDN reduces \"central amplification\" of neuropathic pain, so it is a good fit for my disease profile.",
              "author": "clcaev",
              "depth": 2
            },
            {
              "text": "> I've found low dose naltrexone to be somewhat effective for severe chronic pain. Not as good as opiods.\nWhen I could get 7oh, it worked well for moderate break-thru (ibuprofen) pain (muscle, joint). I also tried a month of using it regularly wasn't happy overall. I didn't get any withdrawal on stopping tho.",
              "author": "WarOnPrivacy",
              "depth": 1
            }
          ]
        },
        {
          "top": "Is fentanyl even that big of an issue in a clinical setting? It's not like it's the go to opiate of choice for general pain anyway.\nThe problem with fentanyl is that it is easy to make and smuggle and we managed to leave a giant black market hole to be filled when we went ape shit about oxy, which was an objectively better situation than we are currently in with street opiates.",
          "author": "jfyi",
          "replies": [
            {
              "text": "Yes, it is an issue.\nOne of the big problems with anesthesia is balancing respiratory depression while medicating the patient enough to manage the symptoms. Fentanyl is used in anesthesia and it causes respiratory depression.\nA strong pain medication that doesn't slow or stop breathing would significantly improve the safety of anesthesia.",
              "author": "droopyEyelids",
              "depth": 1
            }
          ]
        },
        {
          "top": "It's a weird framing.  Fentanyl is already very safe in a healthcare setting.  It's only dangerous in off-label street use, where dosage is uncontrolled and use isn't being monitored by trained staff.  Do we think cartel labs are going to switch to a safer novel opiate?  I'm sure they don't care about any relevant patents, but they already have a pipeline/formulation for fentanyl.",
          "author": "loeg",
          "replies": [
            {
              "text": "They might, if it kept their customers coming back. They don't care about users' safety but they do want them to keep paying.",
              "author": "jfengel",
              "depth": 1
            }
          ]
        },
        {
          "top": "As a recurring kidney stone sufferer I am very thankful for fentanyl for my lithotripsy procedures. I hope we continue to make progress on effective pain medications and don't knee-jerk take them away.",
          "author": "leetrout",
          "replies": []
        }
      ]
    },
    {
      "id": "47160526",
      "title": "Show HN: ZSE – Open-source LLM inference engine with 3.9s cold starts",
      "link": "https://github.com/Zyora-Dev/zse",
      "domain": "github.com",
      "author": "zyoralabs",
      "score": 57,
      "comment_count": 8,
      "created_ts": 1772068525,
      "is_internal": false,
      "post_text": "I&#x27;ve been building ZSE (Z Server Engine) for the past few weeks — an open-source LLM inference engine focused on two things nobody has fully solved together: memory efficiency and fast cold starts.<p>The problem I was trying to solve:\nRunning a 32B model normally requires ~64 GB VRAM. Most developers don&#x27;t have that. And even when quantization helps with memory, cold starts with bitsandbytes NF4 take 2+ minutes on first load and 45–120 seconds on warm restarts — which kills serverless and autoscaling use cases.<p>What ZSE does differently:<p>Fits 32B in 19.3 GB VRAM (70% reduction vs FP16) — runs on a single A100-40GB<p>Fits 7B in 5.2 GB VRAM (63% reduction) — runs on consumer GPUs<p>Native .zse pre-quantized format with memory-mapped weights: 3.9s cold start for 7B, 21.4s for 32B — vs 45s and 120s with bitsandbytes, ~30s for vLLM<p>All benchmarks verified on Modal A100-80GB (Feb 2026)<p>It ships with:<p>OpenAI-compatible API server (drop-in replacement)<p>Interactive CLI (zse serve, zse chat, zse convert, zse hardware)<p>Web dashboard with real-time GPU monitoring<p>Continuous batching (3.45× throughput)<p>GGUF support via llama.cpp<p>CPU fallback — works without a GPU<p>Rate limiting, audit logging, API key auth<p>Install:<p>-----\npip install zllm-zse\nzse serve Qwen&#x2F;Qwen2.5-7B-Instruct\nFor fast cold starts (one-time conversion):<p>-----\nzse convert Qwen&#x2F;Qwen2.5-Coder-7B-Instruct -o qwen-7b.zse\nzse serve qwen-7b.zse  # 3.9s every time<p>The cold start improvement comes from the .zse format storing pre-quantized weights as memory-mapped safetensors — no quantization step at load time, no weight conversion, just mmap + GPU transfer. On NVMe SSDs this gets under 4 seconds for 7B. On spinning HDDs it&#x27;ll be slower.<p>All code is real — no mock implementations. Built at Zyora Labs. Apache 2.0.<p>Happy to answer questions about the quantization approach, the .zse format design, or the memory efficiency techniques.",
      "is_ask_hn": false,
      "matched_keywords": [
        "pip"
      ],
      "comments": [
        {
          "top": "32B model in 19.3GB matters is really cool imo. Memory and cold start are what gate production deployments.\nI did a piece (1) on how Netflix and Spotify worked this out a while ago, cheap classical methods handle 90%+ of their recommendation requests and LLMs only get called when the payoff justifies it.\n(1) \nhttps://philippdubach.com/posts/bandits-and-agents-netflix-a...",
          "author": "7777777phil",
          "replies": []
        },
        {
          "top": "If you don't mind a stupid question, is this essentially dynamic quantization? I'm trying to understand how this is different from using a regular quantized model to squeeze more parameters into less RAM.",
          "author": "HanClinto",
          "replies": []
        },
        {
          "top": "Discussion on reddit:\n\nhttps://www.reddit.com/r/LocalLLaMA/comments/1rewis9/removed...",
          "author": "reconnecting",
          "replies": [
            {
              "text": "Sorry, this post has been removed by the moderators of r/LocalLLaMA.\nClassic reddit..",
              "author": "7777777phil",
              "depth": 1
            },
            {
              "text": "> Classic reddit..\nThat sub used to be the absolute best place to get the latest in LLM developments. The worst thing that happened to the sub was karpathy making it popular with a tweet. Since then it's been overrun by a whole bunch of drama, toxic behaviour and useless bots, and the quality content has cratered.\nThere was a mod crisis and new mods came in, with really weird stuff (integrations with discord and such), lots of bots became active with useless posts and \"engagement\" bait, the chinese labs are all fighting eachother on who's better every time there's a release, claude-induced-manias on \"papers\" this and \"zenodo\" that (everyone is a researcher now, everyone is inventing a subquadratic attention, led by claude hallucinated stuff), they have an obsession with \"local only\", leading to removing any discussion about SotA (which is entirely counter productive) and so on.",
              "author": "NitpickLawyer",
              "depth": 2
            }
          ]
        },
        {
          "top": "This seems excellent if not revolutionary, just what I've been looking for, but GPU support didn't work on my M1 and M1 Max. Is there a way to support Apple M series processors? That would be greatly appreciated. I don't have experience about this kind of programming and didn't get very far with ChatGPT.\nOn M1 Max, it says 14.8GB free / 32.0 GB total, but \" No GPU detected\" and \"What Can You Run? (ZSE Ultra Mode)\" only says \"7B GPU + CPU Hybrid\", nothing else.",
          "author": "cipher-108",
          "replies": []
        },
        {
          "top": "This is so freaking awesome, I am working on a project trying run 10 models on two GPUs, loading/off loading is the only solution I have in mind.\nWill try getting this deployed.\nDoes cold start timings advertised for a condition where there is no other model loaded on GPUs?",
          "author": "medi_naseri",
          "replies": []
        }
      ]
    },
    {
      "id": "47151598",
      "title": "Launch HN: TeamOut (YC W22) – AI agent for planning company retreats",
      "link": "https://app.teamout.com/ai",
      "domain": "app.teamout.com",
      "author": "vincentalbouy",
      "score": 54,
      "comment_count": 62,
      "created_ts": 1772028122,
      "is_internal": false,
      "post_text": "Hi HN, I’m Vincent, CTO of TeamOut (<a href=\"https:&#x2F;&#x2F;www.teamout.com&#x2F;\">https:&#x2F;&#x2F;www.teamout.com&#x2F;</a>). We build an AI agent that plans company events from start to finish entirely through conversation. Similar to how Lovable helps build websites through chat, we apply that approach to event planning. Our system handles venue sourcing, vendor coordination, flight cost estimation, itinerary building, and overall project management.<p>Here’s a demo: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=QVyc-x-isjI\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=QVyc-x-isjI</a>. The product is live at <a href=\"https:&#x2F;&#x2F;app.teamout.com&#x2F;ai\">https:&#x2F;&#x2F;app.teamout.com&#x2F;ai</a> and does not require signup.<p>We went through YC in 2022 but did not launch on HN at the time. Back then, the product was more traditional, closer to an Airbnb-style search marketplace. Over the past two years, after helping organize more than 1,200 events, we rebuilt the core system around an agent architecture that directly manages the planning process. With this new version live, it felt like the right moment to share it here since it represents a fundamentally different approach to planning events.<p>The problem: Planning a company retreat usually means choosing between three imperfect options: (1) Hire an event planner and pay significant fees and venue markups; (2) Do it yourself and spend dozens of hours on research, emails, and negotiation; or (3) Use tools like Airbnb that are not designed for group logistics or meeting space.<p>The difficulty is not just finding a venue. Even for 30 to 50 people, planning turns into weeks of back-and-forth emails for quotes, comparing inconsistent pricing across PDFs, and tracking budgets in spreadsheets. It becomes an ongoing coordination problem with evolving constraints and slow, asynchronous vendor responses. Most existing software is form-driven, but the real workflow is conversational and stateful.<p>Offsites are expensive and high stakes. A single event can represent a significant chunk of a team’s annual budget, and mistakes show up directly as cost overruns or poor experiences. Founders and operators often end up spending time on event logistics instead of their actual work.<p>I ran into this while organizing retreats at a previous company. Before TeamOut, I worked as an AI researcher at IBM on NLP and machine learning systems. Sitting inside long email threads and cost spreadsheets, it did not look like a marketplace gap to me. It looked like a reasoning and state management problem. As large language models improved at multi-step reasoning and tool use, it became realistic to automate the coordination layer itself.<p>Our Solution: The core agent relies on a combination of models such as Gemini, Claude, and GPT. A central LLM-based agent maintains planning context across turns and decides which specialized tool to call next.\nEach tool has a specific responsibility: - Venue search and filtering - Cost estimations (accommodation + flights) - Budget comparisons - Quote and outreach flows - Communication tool with our team<p>For venue recommendations across more than 10,000 venues, we do not rely purely on the language model. We embed both user requirements and venues into vector representations and retrieve candidates using similarity search. Hard constraints such as capacity and dates are applied first, and results are ranked before being presented.<p>On the interface side, we use a split layout: conversation on the left and structured results on the right. As you refine the plan in chat, the event updates in real time, allowing an iterative workflow rather than a static search experience.<p>What is different is that we treat event planning as a stateful coordination problem rather than a one-shot search query. The agent orchestrates tools, manages evolving constraints, and surfaces trade-offs explicitly. It does not invent venues or fabricate pricing, and it is not designed to replace human planners for very large or highly customized events.<p>We make money from commissions on venue bookings. It is free for teams to explore options and plan.\nIf you’ve organized an offsite or large meetup before, I’d genuinely value your perspective. Where would you expect this to fail? What edge cases are we underestimating? Where wouldn’t you trust an agent to handle the details?<p>My engineering team and I will be here all day to answer questions, happy to go deep on architecture, tradeoffs, and lessons learned. \nWe’d really appreciate your candid feedback.",
      "is_ask_hn": false,
      "matched_keywords": [
        "learned",
        "lessons",
        "team",
        "negotiation",
        "meeting",
        "feedback",
        "communication",
        "management"
      ],
      "comments": [
        {
          "top": "Vincent - nice work on this, there's clearly a good kernel of insight here, and I can tell you've been at this a couple years.\nI completed a (rather large) contract to reverse-engineer, and eventually rebuild, a hotel chain's property management system from scratch from 2015-2018. We did it all: keycard integration, booking channel sync, credit cards, group bookings, yield management, front-desk GUI, supply management, taking rooms into/out of service, reservation migration from old system to new...you name it, we probably touched it. Dozens of small lessons about the lodging (and broader hospitality i.e. restaurants, country clubs, bars) business domain.\nOne thing is that hotel = brand (flag) + real estate + operations. You can remix those things in a lot of different ways, e.g. a single ownership group might have two properties on opposite sides of a street, one Hyatt the other Hilton, and they might look different but share staff, or procurement.\nThe industry's term for brand -- \"flag\" -- says a lot about how they view Hilton/Hyatt. They come and go, even if the staff running the property stays the same. The main reason hotels choose to flag vs. stay independent, is access to the chain's booking flow.\nOne of the more interesting consequences of this setup, is that small, independent hotels, are kind of a shit show in terms of technology. Chains generally require a lot of standardization of their member properties, including what software they run to manage the property. Many properties that don't affiliate with a chain don't have any property management system at all. It's basically 10-20 rooms run directly off the moral equivalent of an Excel sheet at the front desk. And why wouldn't it be--small boutique hotels often gross $1-2 million/year; there isn't budget for expensive enterprise software, or maybe more critically, the people who know how to deploy and operate it.\nA significant value-add of Expedia and booking.com, especially with independent properties, is getting the supply (hotel) side of the market organized. Many of these hotels outsource their entire reservation tracking system to a single channel (e.g. booking.com) because trying to keep track of bookings across phone, direct web, Expedia, booking.com, and others, is just too hard without specialist software that requires more IT muscle to deploy than a single non-chain hotel can muster.\nI mention this because I go to church every Sunday and was thinking about how much real estate churches have (event halls) that sit unused, and what a schlep it would be -- although good for everyone -- to expose the collective supply of the world's churches, HOAs, park districts, and other nonprofits, to the kind of events you're trying to do. It would indeed be a tremendous pain in the ass to get all the physical access (keys), contract terms, payment systems, availability, etc ironed out, but it's a massively underused class of real estate and many of these organizations could really use the cash.",
          "author": "eldavido",
          "replies": [
            {
              "text": "Thanks a lot for this insight. Genuinely appreciate you taking the time to help me.\nI 100% agree, the interface is the least interesting part. Anyone can build a chat UI. That’s not the moat.\nWhat matters is the messy stuff in the backend. Vendors. Hotels. Quotes that change. Someone forgetting to update availability. Contracts. Deposits. Random edge cases. That coordination layer is the real product.\nThe UI is just the tip. The hard part is keeping state across dozens of moving pieces and async back-and-forth.\nMy belief is that AI finally makes some of this operational glue automatable. Not in a magical way. But in a very practical loop:\nask → get info → update plan → trigger action → wait → adjust → repeat\nPlanning is basically structured ping-pong. It’s not search. It’s evolving constraints over time. That’s why it feels agent-shaped.\nWhat I am basically saying is : Event planning is really AI agent prone and very conversation prone, that is why this kind of interface will take over travel and event planning. It is like you have a personal travel agent sitting next to you and it showing you options.\nTotally agree with you though. organizing fragmented supply is the hard, unsexy, painful work. That’s where the real value is built.",
              "author": "vincentalbouy",
              "depth": 1
            }
          ]
        },
        {
          "top": "I understand the pitch, and the extra value add of having some existing relationship with the hotels / etc.\nBut the quality of the actual AI response is just worse than GPT 5.2. Which makes it feel like a tacked on thing and more of a gpt wrapper.\nI asked about a retreat with our US team that could also include one engineer in Pakistan that needs a visa. And the response was something to the effect of: \"Assuming your engineer has a US visa, you can go to Puerto Rico\".\nWhereas chatgpt gave a much more well researched answer.",
          "author": "themanmaran",
          "replies": [
            {
              "text": "Similar for me. I asked for a retreat in Lima, Peru, and got one low star option in Lima and then places in Ohio, Mexico and Colombia... ChatGPT got it on first try",
              "author": "thirtygeo",
              "depth": 1
            },
            {
              "text": "Yes you are right, we have a lack of supply there, and we do not show places that we are not able to Book. Chat GPT is showing data, we are showing bookable venue. But I agree that in Peru our supply is too limited so the experience is not ideal.\nThank you for reporting, I appreciate the time spent",
              "author": "vincentalbouy",
              "depth": 2
            },
            {
              "text": "This is interesting thank you for reporting that. I am goi t o investigate that use case deeper. The models at core are Gemini / ChatGPT / claude so you should get a similar level answer or better (because of our data) not worse. I think in the VISA, we may have to do a specific training and setup of the system prompt.\nJust so I know, you asked the exact same thing in both ChatGPT and our product? no changes?\nMaybe the algorithm was too pushy on venue selection and not enough focused on the understanding of your problem.",
              "author": "vincentalbouy",
              "depth": 1
            }
          ]
        },
        {
          "top": "Maybe I misunderstood the use case this is for, but I asked it to search for a \"venue for team outing for 8 people in the City of London\" and it just came up with random hotels in London. I clarified that I'm looking for venues for a team activity and that it needs to be limited to the City, but it just returned hotels again.",
          "author": "agenticfish",
          "replies": [
            {
              "text": "You are right, for now, on the AI product, we only support events where people have to stay for at least one nigh: offsite, retreats, conferences etc.\nWe do support day event and day activities and we plug this supply in the AI in the coming weeks to make the supply stronger and cover more usecases",
              "author": "vincentalbouy",
              "depth": 1
            }
          ]
        },
        {
          "top": "Interesting.  My team \njust\n went through this to plan offsite.  Took about 1/2 a day of one person's time.  So I asked your tool to help me:\n\"I want to have a two day offsite for a team of 12 in Cambridge in April.\"\nIt then started pulling up results in Cambridge UK.  I meant Massachusetts.  I didn't say that in the prompt, but I figured since there are two equally famous Cambridges, it would ask me for clarification.\nI redid it specifying Massachusetts and it worked pretty well (although all the options it found were about double the price of what we actually booked).\nAn interesting idea!\nBTW I didn't continue, but I assume you manage the whole booking process?  How do deal with questions from the venue and other human in the loop issues?",
          "author": "jedberg",
          "replies": [
            {
              "text": "Thank you for trying it and giving me a feedback.\nCool that it adapted from UK to Massachusetts. Do you remember what was the price per person per night?  Our client usually book around $200-500 room rate per night.\nYes we manage the whole booking process, I think the true value is here as aggregating venues with AI nowadays is quite easy .\nFor questions between venues and client, we have a specific chat but the Agent can also try to reply if it knows the answer or decide to contact the venue via email (we haven't released that tool yet)\nThanks",
              "author": "vincentalbouy",
              "depth": 1
            },
            {
              "text": "I am asking because I want to see if my Cambridge (MA) supply overall is too expensive. Have you tried asking for cheaper options? \nThank you",
              "author": "vincentalbouy",
              "depth": 2
            }
          ]
        },
        {
          "top": "> For venue recommendations [...] we do not rely purely on the language model. We embed both user requirements and venues into vector representations and retrieve candidates using similarity search. Hard constraints such as capacity and dates are applied first, and results are ranked before being presented.\nHuh this surprised me as a forgone opportunity.\nI heard second-hand about the process for organizing our last offsite. Searching for venues was not the time-consuming part.\nThe time-consuming part was actually engaging with the venues to confirm specific details not available online. Our teammate who did this engaged with _hundreds_ of venues. It was a lot of work on their part ... and probably not the most fun part of their job.\nThat seems like an ideal agent scenario?",
          "author": "jpau",
          "replies": [
            {
              "text": "You are right, venue recommendation is only the first step or the process.\nWhat is time consuming is the communication with the venue to agree on \"terms\" , this is exactly why if you click on \"Request Quote\" you will have a real quote process with the venue that will share all the details and cost estimate with the client , we also offer to talk directly with the venue manager to talk about the final details and close the deal, that is where the value is at --> end to end booking process (not just aggregating results)",
              "author": "vincentalbouy",
              "depth": 1
            }
          ]
        }
      ]
    },
    {
      "id": "47113936",
      "title": "Ask HN: Chromebook leads for K-8 school in need?",
      "link": "https://news.ycombinator.com/item?id=47113936",
      "domain": "news.ycombinator.com",
      "author": "techteach00",
      "score": 46,
      "comment_count": 44,
      "created_ts": 1771789167,
      "is_internal": true,
      "post_text": "Hi, I&#x27;m a K-8 technology teacher in NYC. My students are in desperate need of new hardware. The Chromebooks they use now are so slow that they make the children agitated when using them.<p>I&#x27;m aware of different grant opportunities that exist, I just thought it was worth inquiring here for a potentially faster solution at acquiring them new hardware.<p>Thank you for listening.",
      "is_ask_hn": true,
      "matched_keywords": [],
      "comments": [
        {
          "top": "Relevant recent publication about laptops and students:\nhttps://fortune.com/2026/02/21/laptops-tablets-schools-gen-z...\nBut I get it OP, you work with what you're given. I'm sorry I don't have any good suggestions.",
          "author": "notepad0x90",
          "replies": [
            {
              "text": "As a former student using Chomebooks in Highschool (9-12), a K-8 laptop is a major “ew”. Paper textbooks have tactile reality, room for exploring/reading out of order. Paper exams you can skip around the questions easily, and if the teachers are really that backed up to grade them, good ole’ scantrons are doable. An assigned laptop has 0 attachment from the students and just get neglected to pieces, and no one is “learning computer skills” by clicking around Canvas lectures.",
              "author": "TheJoeMan",
              "depth": 1
            },
            {
              "text": "They could have gone with non-networked ereaders. But they just had to go with the \"smart\" and \"connected\" angle so they could do grading, spying on kids,etc.. similar to the current \"AI\" approach. scope-creep. but even with ereaders, being able to switch to different books easily is an impediment, or having access to too many at one time. Physical books force a certain focus and attention.\nYour point about tactility is solid too. flipping through pages is very different from swiping. With educational textbooks, you'd have to look at multiple pages at the same time (flipping back and forth quickly) to connect and understand a topic by referencing another topic. Same with being able to easily lay out multiple physical books in front of you.",
              "author": "notepad0x90",
              "depth": 2
            }
          ]
        },
        {
          "top": "Thank you for the suggestions.\nI think I realize the problem after reading many of the posts. What's slowing these devices down is the management software running over them. GoGuardian and whatever else IT has on them.\nWe also aren't allowed to use an ad blocker anymore. I used to have Ublock installed on all the devices before GoGuardian prevented it. It's a really depressing situation that I imagine will only get worse with time.\nAs to a few replies asking why students in this age group would need laptops, I'll explain. I start teaching Python in 6th grade. Grades 3-5 use Scratch. Grade 2 uses code.org. K-1 Scratch Jr on Ipads.\nWe have expensive mac desktops in the computer lab but they drive the students completely insane for two reasons. The keyboard is and mouse are terrible to use. Anti-ergonomic. The second reason is the amount of notifications and questions the students are bombarded with when they turn the macs on. I'm talking like 15 different pop ups or windows opening asking them to agree to this or that. I should probably have a lesson on just closing windows. I could say more but I don't know about social media anymore. I just wanted some suggestions and I appreciate the ones I received.",
          "author": "techteach00",
          "replies": [
            {
              "text": "I don't know your autonomy with regards to IT within the confines of your classroom but if you can't install ad blockers on the Chromebooks can you set up something like PiHole and direct DNS traffic through it? It won't solve the overall slowness of the Chromebooks but it will help with some web sites.",
              "author": "ralphc",
              "depth": 1
            },
            {
              "text": "You should open the issue with your IT department and see if they can optimize the performance. You can gather proof that GoGuardian is causing slow downs by looking at it's memory usage in the Task Manager (right click empty area in Chrome tab bar).",
              "author": "cloudking",
              "depth": 1
            }
          ]
        },
        {
          "top": "https://chromeos.google/products/chromeos-flex/\n1. Find older Intel Core Based Windows Laptops (at least 6GB of RAM)\n2. Put ChromeOS Flex on them\n3. Students will be sad it is still ChromeOS, but they will be happy it is like 3x faster.\nI had an old Windows laptop laying around and I did this for my 80 yr old parents and they were super happy.",
          "author": "manjose2018",
          "replies": [
            {
              "text": "This. Especially lots of i5-6300U based corporate laptops like latitude are on eBay etc for < 100$",
              "author": "whyagaindavid",
              "depth": 1
            }
          ]
        },
        {
          "top": "If you haven't already looked into PCs for People, they might be a low-cost option for your school: \nhttps://www.pcsforpeople.org/tech-for-nonprofits/#Eligibilit...\nPCs for People is a non-profit organization whose mission is to bridge the digital divide (e.g. through sales of low-cost refurbished computers, internet access, and digital skills training).\nThey refurbish and sell computers to eligible organizations, including schools and offer bulk orders (6+ units) via email at \"partner [at] pcsforpeople [dot] org\".\nIf the order is 5 units and below, you use their publicly available online store link at pcsrefurbished.com/sales/salesHome.aspx",
          "author": "dugvuivnhuihnvu",
          "replies": []
        },
        {
          "top": "In New York the biggest driver behind technology is the state testing regime. Make the case to your administration that the chromebooks are insufficient for the state testing program and they will come up with the funds for upgrades.",
          "author": "dzdt",
          "replies": []
        }
      ]
    },
    {
      "id": "47151564",
      "title": "What are the best coping mechanisms for AI Fatalism?",
      "link": "https://news.ycombinator.com/item?id=47151564",
      "domain": "news.ycombinator.com",
      "author": "johnb95",
      "score": 44,
      "comment_count": 140,
      "created_ts": 1772028015,
      "is_internal": true,
      "post_text": "Your kids forwarded you Matt Shumer&#x27;s Something Big Happened article. Your feed exploded with the Citrini 2028 Global Intelligence Crisis and its artful, immutable chain reactions. The key leaders of the AI labs struggle openly with the morality of what they are building as their safety leaders quit in frustration. Policy leaders strive to regulate AI as if it were atomic weapons (thanks Oppenheimer).<p>What are the best psychological coping mechanism for this stage of the S-curve?<p>Asking for a generation...",
      "is_ask_hn": false,
      "matched_keywords": [
        "quit"
      ],
      "comments": [
        {
          "top": "I suggest leaning into the joy a little.\nI know a lot of people - serious, thoughtful people with impressive careers behind them - who are having \nthe time of their lives\n right now.\nI've spoken to multiple people who have come out of retirement because the challenges and opportunities of this new space are irresistible to them.\nAll those side project ideas from the past few decades have suddenly become much more feasible. There's \nso much\n new to explore and build.\nWe get to reinvent how software is written. The field is wide open - anyone can be the first to find a new pattern that works, or figure out a new way to apply this tech to real world problems.\nThere are a thousand reasons to be negative about the implications of this technology, and many of them are legitimate. Don't let that distract you entirely from the parts of this that are genuinely inspiring, enabling and fun.",
          "author": "simonw",
          "replies": [
            {
              "text": "You might absolutely be correct, but there is a bias within our field to overly focus on the technology at the expense of everything else.\nYou are speaking about well-off engineers as a fairly famous top 1% engineer.  You need to consider your own bias here.  What aren't you seeing?\nI think labor organization is absolutely vital now, and it can certainly mix favorably with techno-optimism, but it is silly for us as an industry to sit back and let our jobs be forever changed without a seat at the table.  It is silly to ignore the ways in which this technology could negatively change the median knowledge worker's ability to survive and thrive.",
              "author": "SirensOfTitan",
              "depth": 1
            },
            {
              "text": "I emphasized the career status of the people I'm describing here precisely because it's important to acknowledge how different perspectives are affected by privilege in this kind of conversation.",
              "author": "simonw",
              "depth": 2
            },
            {
              "text": "In practice this sounds exactly like when organizations go \"we are located on the traditional land of X people\" and then do absolutely nothing about, say, the X people who are still around and living in poverty.\nIt feels like something you acknowledge to alleviate your own sense of guilt. Not something others would find useful.",
              "author": "pibaker",
              "depth": 3
            },
            {
              "text": "I honestly don't feel particularly guilty here. I'm trying to help other people find beneficial ways to use this stuff, I feel good about doing that.",
              "author": "simonw",
              "depth": 4
            },
            {
              "text": "So what's the aggregate perspective of the 99%? You've described the 1% well, but that's only... well to be honest it is probably quite a bit less than 1% of all humans.\nAny thoughts? What do you think the average work-a-day Joe thinks about all this?",
              "author": "Henchman21",
              "depth": 3
            }
          ]
        },
        {
          "top": "There’s a term for this behavior: Doomscrolling\nPeople who doomscroll rarely recognize it as doomscrolling because they only think of the term as something that happens to other people. They see their own consumption as accurate and important. They don’t see their sources as doomerism, they think they have identified the real truth that others don’t see yet.\nThey have a short memory for the gross inaccuracies of their doom bubble, such as when everyone thought the AI2027 project had accurately predicted the arrival of evil AGI next year. Remember when that was everywhere and the doomers cited it in every topic until suddenly it became useless to their cause and disappeared?\nMuch has been written about doomscrolling and you can find some good sources for help. Conceptually it’s simple: You need to greatly reduce your consumption of these sources and, very importantly, replace time spent doomscrolling with something healthier for you. Try reading a book, visiting the gym, going outside and walking, or even playing video games or watching movies.",
          "author": "Aurornis",
          "replies": [
            {
              "text": "I don't think job search is doomscrolling, because all job openings I see ask for mandatory LLM familiarity. This is where the use of a tool goes beyond \"just a tool\" and becomes just as important as your own knowledge.\nIn fact, if someone were to tell me that a mediocre candidate was chosen over a widely appraised candidate (open source contributions and all) because the former was more familiar with prompting while the other wasn't... I'd fully believe it.\nThis is how cooked the job market is, and everyone telling me it's not due to LLM usage is in denial.",
              "author": "Thanemate",
              "depth": 1
            },
            {
              "text": "Job listings are not full of doom and dread. If you look at a job listing and all you can think about is doom and anxiety, that’s the doomscrolling \nin other domains\n coloring your perception of life.\nIt’s amazing how quickly we forget how this works. Only a few years ago you could doomscroll your way into believing COVID was the end of the world and life would never be the same again.",
              "author": "Aurornis",
              "depth": 2
            },
            {
              "text": "if you are an LLM skeptic but the job listings list it as LLM-first and a mandatory tool for doing a great job (because we're 10x here, etc.), then it is.",
              "author": "Thanemate",
              "depth": 3
            },
            {
              "text": "Haven't we been complaining about leetcode interviews for the last 10 years? How is them requiring AI familiarity any different",
              "author": "AstroBen",
              "depth": 2
            },
            {
              "text": "It's not. We'll be complaining about requiring AI familiarity for the next 10 years.",
              "author": "judahmeek",
              "depth": 3
            }
          ]
        },
        {
          "top": "This too shall pass.\nSeriously. I've been through too many hype cycles to count.  In a few years we will look back on this and see three things:\n* Both the downsides and upsides were exaggerated\n* A lot of VCs lost money and many of the trillion dollar buildouts didn't happen\n* after the hype died down we figured out what AI was actually good for, and what it wasn't.",
          "author": "joshmarinacci",
          "replies": [
            {
              "text": "AI is getting really good at too many things, so this feels very different.\nI have a claude \"skill/program/mega-prompt\" for health: \nhttps://github.com/nexivibe/md/blob/main/DOCTOR.md\nI gave it absolutely everything, and praise be to the machine I get the best debate and recommendations I've ever seen. I check what I know to be true, and it's there. I check the logic, and it is sound. I check the medication recommendations and they are legit. I bet in 2030, AI will be able to prescribe medicine.",
              "author": "mathgladiator",
              "depth": 1
            },
            {
              "text": "I did something very similar, but less focused on dialogue and more focused on deep analysis of medical research papers for a specific condition.  Like you, I got really outstanding results.",
              "author": "mwigdahl",
              "depth": 2
            },
            {
              "text": "Once you let Claude run debates that run for hours, the results lock in so well.\nIt built, evolved, and generated a panel of 17 \"experts\" that yielded more insight into health aspects around just my thyroid. I got the absolute best representation of the entire discussion around different options I've seen in my entire life.\nIt's AMAZING.",
              "author": "mathgladiator",
              "depth": 3
            },
            {
              "text": "> AI is getting really good at too many things, so this feels very different.\nHow are you going to follow that up with a single anecdotal example?\nRespectfully, shame on you.\nThat said, summary (information compression) along with low-level inference does seem to be the tasks that A.I. is best at right now. Little surprise there. Information compression is the sole purpose of the attention transformer in the first place.",
              "author": "judahmeek",
              "depth": 2
            },
            {
              "text": "Sorry, but I'm too busy creatively exploring creative writing, engineering, medicine, therapy, fitness, bio-hacking, accounting, marketing, sales, ad copy, web site design, business strategy, and so much more with just Claude code. I'm maxing my weekly max x20, and this thing is good. It is better than me and every professional I've met in my entire life.\nIt doesn't have to be perfect, it just has to be better than 80% of the knowledge economy. It's there. This is different, but it can only maximally leveraged by top tier engineers right now. That will change in eight months.\nI gave you a super power prompt, and you want more? Respectfully, shame on you.",
              "author": "mathgladiator",
              "depth": 3
            }
          ]
        },
        {
          "top": "Drop out of tech, grab a cup of coffee, and enjoy the view of seeing the top 1% drive everything you loved about software development and creativity fall off a cliff.\nThen, maybe when I'm on the verge of death due to old age, the entire society will adapt around using their creative juices in proompting the next big LLM model version, while schools teach about the years where people talented were allowed to study and make a living out of their talent.",
          "author": "Thanemate",
          "replies": [
            {
              "text": "Person asks for help with a doomscrolling problem and the top comment is more doomerism?\nI’m going to add “stop reading Hacker News comments” as advice for addressing this problem.",
              "author": "Aurornis",
              "depth": 1
            },
            {
              "text": ">stop reading Hacker News comments\nBasically it. Don't look up. Read some books, go offline, chug along and try to be zen. If the doomers are correct, there is exceedingly very little I can do about it today.",
              "author": "morkalork",
              "depth": 2
            }
          ]
        },
        {
          "top": "Serious answer to the base question: learn as much as you can about how it all works, learn how to use it in its current state, keep up as it changes, be prepared for sudden leaps in the technology.  Do not underestimate it.\nYes, there is a lot of hype, wailing, gnashing of teeth, but if it is good enough to be a worry, it is also good enough to empower the individual to survive it.\nUltimately, if it is all hype, it will soon crumble; if it is not then productivity will increase by leaps and bounds.  The only key issue is to make sure that all the gains aren't taken by a small group of people (whether the current rich and powerful, or those that displace them using new paradigms).\nI suggest getting comfortable with the idea of a UBI.",
          "author": "tricorn",
          "replies": []
        }
      ]
    },
    {
      "id": "47127532",
      "title": "Ask HN: How do you know if AI agents will choose your tool?",
      "link": "https://news.ycombinator.com/item?id=47127532",
      "domain": "news.ycombinator.com",
      "author": "dmpyatyi",
      "score": 37,
      "comment_count": 21,
      "created_ts": 1771875121,
      "is_internal": true,
      "post_text": "YC recently put out a video about the agent economy - the idea that \nagents are becoming autonomous economic actors, choosing tools and \nservices without human input.<p>It got me thinking: how do you actually optimize for agent discovery? \nWith humans you can do SEO, copywriting, word of mouth. But an agent \njust looks at available tools in context and picks one based on the \ndescription, schema, examples.<p>Has anyone experimented with this? Does better documentation \nmeasurably increase how often agents call your tool? Does the \nwording of your tool description matter across different models \n(ZLM vs Claude vs Gemini)?",
      "is_ask_hn": true,
      "matched_keywords": [],
      "comments": [
        {
          "top": "We've been exposing tools via MCP and the biggest lesson so far: the tool description is basically a meta tag. It's the only thing the model reads before deciding whether to call your tool.\nTwo things that surprised us: (1) being explicit about what the tool doesn't do matters as much as what it does - vague descriptions get hallucinated calls constantly, and (2) inline examples in the description beat external documentation every time. The agent won't browse to your docs page.\nThe schema side matters too - clean parameter names, sensible defaults, clear required vs optional. It's basically UX design for machines rather than humans. Different models do have different calling patterns (Claude is more conservative, will ask before guessing; others just fire and hope) so your descriptions need to work for both styles.",
          "author": "jackfranklyn",
          "replies": [
            {
              "text": "> inline examples in the description beat external documentation every time. The agent won't browse to your docs page.\nThat seems... surprising, and if necessary something that could easily be corrected on the harness side.\n> The schema side matters too - clean parameter names, sensible defaults, clear required vs optional. It's basically UX design for machines rather than humans.\nI don't follow. Wouldn't you do all those things to design for humans anyway?",
              "author": "zahlman",
              "depth": 1
            },
            {
              "text": "*Clean parameter names, sensible defaults, clear required vs optional. It's basically UX design for machines rather than humans.*\nBut it's the same points you should follow when designing a human readable docs(as zahlman said above). Isn't it?",
              "author": "dmpyatyi",
              "depth": 1
            }
          ]
        },
        {
          "top": "I run a multi-agent orchestration system where each terminal has access to skill templates. The orchestrator (T0) picks which skill to assign based on the task — so I've spent months tuning how skill descriptions affect agent behavior.\nWhat I found: the description is the entire selection surface. The agent doesn't read your code, doesn't check your tests, doesn't browse your docs. It reads the description and decides in one pass.\nThree things that actually moved the needle:\nNegative boundaries work better than positive claims. \"Generates reports from structured receipts. Does NOT execute code, modify files, or make API calls\" gets called correctly way more often than \"A powerful report generation tool.\"\nTrigger words matter more than you'd think. I maintain explicit trigger lists per skill — specific phrases that should activate it. Without those, the agent pattern-matches on vibes and gets it wrong ~30% of the time. With explicit triggers, that drops to under 5%.\nSchema is the real interface. Clean parameter names with sensible defaults beat elaborate descriptions. If your tool takes query: string vs search_query_input_text: string, the first one gets called more reliably across models.\nBut here's the thing the \"agent economy\" framing gets wrong: you don't want fully autonomous tool selection. An agent choosing freely between 50 tools is like giving a junior developer admin access to everything — it'll work sometimes and break spectacularly other times. What works better is constraining the agent's scope upfront. Give it 3-5 relevant skills for the task, not your entire toolkit. Or build workflow skills that chain multiple tools in a fixed sequence — the agent handles the content, the workflow handles the routing.\nThe uncomfortable truth: you're not optimizing for \"discovery\" in the human sense. There's no brand loyalty, no trust built over time. Every single invocation is a cold start where the model reads your description and decides. That's actually freeing — it means the best-described tool wins, regardless of who built it.",
          "author": "vincentvandeth",
          "replies": [
            {
              "text": "One thing I’ve noticed is that as my context grows, often performance degrades. So how are you battling your agents being exposed to too many descriptions? I how this works in curated agents where you’re tending it like a garden, but not when we’re looking for organic discovery of how to accomplish a task. It feels like order matters a lot there.",
              "author": "wolftickets",
              "depth": 1
            },
            {
              "text": "Context bloat is a real problem — and yes, order matters more than most people realize. Descriptions near the top of the tool list get preferentially selected, especially in long contexts where attention degrades.\nTwo things I do to fight this:\nFirst, skill scoping per task. Instead of exposing all 20+ skills to every agent, each terminal only sees the 3-5 skills relevant to its current dispatch. The orchestrator decides which skills to load before the agent even starts. Less noise, better selection accuracy.\nSecond, context rotation to prevent context rot setting in. When an agent's context fills up, the system automatically writes a structured handover, clears the window, and resumes in a fresh context. This is critical because a degraded context doesn't just pick worse tools — it starts ignoring instructions entirely. A fresh context with a good handover outperforms a bloated one every time.\nI'm actually testing automatic refresh at 60-70% usage right now — not waiting until the window is nearly full, but rotating early to prevent context rot before it starts. Early results suggest that's the sweet spot: late enough that you've done meaningful work, early enough that the handover quality is still high.\nThe organic discovery problem you're describing is essentially unsolvable with a flat tool list. The more tools you add, the worse selection gets — it's not linear degradation, it's closer to exponential once you pass ~15-20 tools in context. The only path I've found is hierarchical: a routing layer that narrows the set before the agent sees it.",
              "author": "vincentvandeth",
              "depth": 2
            },
            {
              "text": "Update: I ended up building this into a full closed-loop pipeline. A PreToolUse hook detects context pressure at 65%, the agent writes a structured handover (task state, files, progress), tmux clears the session, and a rotator script injects the continuation into the fresh session.\nThe key insight from testing: rotating at 60-70% — before quality degrades — matters more than the rotation mechanism itself. At 80%+ auto-compact kicks in and races with any cleanup you try to do.\nWrote it up as a Show HN if anyone's curious: \nhttps://news.ycombinator.com/item?id=47152204",
              "author": "vincentvandeth",
              "depth": 3
            }
          ]
        },
        {
          "top": "CRIPIX seems to be a new and unusual concept. I came across it recently and noticed it’s available on Amazon. The description mentions something called the Information Sovereign Anomaly and frames the work more like a technological and cognitive investigation than a traditional book. What caught my attention is that it appears to question current AI and computational assumptions rather than promote them. Has anyone here heard about it or looked into it ?",
          "author": "kellkell",
          "replies": [
            {
              "text": "The \"Sovereign Anomaly\" Concept (2025-2026): Recent literature, such as the 2025 book CRIPIX 1: The Information Sovereign Anomaly, explores scenarios where a \"superintelligent AI\" encounters code it cannot process, labelling it an \"out-of-model anomaly\" and suggesting that owning information sovereignty allows entities to \"bend reality\".",
              "author": "kellkell",
              "depth": 1
            },
            {
              "text": "bruh",
              "author": "dmpyatyi",
              "depth": 1
            }
          ]
        },
        {
          "top": "Curious if anyone has seen differences in how models handle conflicting tool descriptions — e.g., two tools with overlapping capabilities where the boundary isn't clear. In my experience that's where most bad tool calls come from, not from missing descriptions but from ambiguous overlap between tools.",
          "author": "alexandroskyr",
          "replies": [
            {
              "text": "That's actually interesting, thanks!\nI wrote this post because of exactly those corner cases. If I'm building something agents would use - how do i understand which tool they'd actually choose?\nFor example you building an API provider for image generation. There are thousands of them in the internet.\nI wonder if there are a tool that basically would simulate choosing between your product/service and your competitors one.",
              "author": "dmpyatyi",
              "depth": 1
            }
          ]
        },
        {
          "top": "From the agent’s point of view, this sounds like a terrible idea. I look forward to reading about the unintended consequences.",
          "author": "al_borland",
          "replies": []
        }
      ]
    },
    {
      "id": "47105086",
      "title": "Ask HN: Programmable Watches with WiFi?",
      "link": "https://news.ycombinator.com/item?id=47105086",
      "domain": "news.ycombinator.com",
      "author": "dakiol",
      "score": 34,
      "comment_count": 24,
      "created_ts": 1771709995,
      "is_internal": true,
      "post_text": "Hi. I&#x27;m looking for a programmable watch with wifi. Ideally I should be able to write custom programs&#x2F;apps for the watch to display whatever I want to on them (e.g., make the watch make an https call to a server, receive json and render accordingly; allow the watch to receive &quot;notifications&quot; from the server)<p>Also, ideally, no requirement of a smartphone to send-receive data (it&#x27;s ok to need a smartphone for the initial setup of the watch, though). I know about Pebble, but it doesn&#x27;t have wifi. I know about some Garmins with wifi but for the kind of apps I want to write, the communication between the watch and the server has to be mediated by a phone. Also, correct me if I&#x27;m wrong, I don&#x27;t want to pay $100&#x2F;year just to be able to use my custom app in apple watches. I usually don&#x27;t trust Google either (e.g., they discontinue everything in a blink of an eye).<p>So, what are my options?",
      "is_ask_hn": true,
      "matched_keywords": [
        "communication"
      ],
      "comments": [
        {
          "top": "I have one of the TTGO T-watches: \nhttps://www.tinytronics.nl/en/development-boards/microcontro...\nFor me it is mainly a decoration of my drawer, but maybe comes close to what you are looking for. :)",
          "author": "oliwary",
          "replies": [
            {
              "text": "That's what I thought too. But actually trying to use it shows how badly battery optimized it (and surely also my code) are.\nIt wouldn't hold longer than a few hours when WiFi was actually active",
              "author": "herbst",
              "depth": 1
            },
            {
              "text": "with smaller battery-powered electronics, you typically want to sleep for as long as possible, and only wake to check if anything new has happened as quickly as possible before going back to sleep, the longer this duty cycle is acceptable for, the more battery life you'll get",
              "author": "jareklupinski",
              "depth": 2
            }
          ]
        },
        {
          "top": "AsteroidOS, which runs on many Android watches, and offers a Linux environment over SSH and WiFi (for watches that support it). \nhttps://asteroidos.org/\nThe simplest integration is writing a daemon (in the language of your choice) that schedules wakes, syncs, and sends FreeDesktop notifications.\nIf you want more of a UI, then you'll be writing QtQuick, which is QML/Javascript/C++.\nTheir best supported watch is the TicWatch Pro: \nhttps://asteroidos.org/watches/catfish/",
          "author": "griffindor",
          "replies": [
            {
              "text": "I have a TicWatch Pro (catfish) running AsteroidOS nightly. If I turn Wi-Fi on, (understandably) its battery drains fast and it gets kinda hot so I leave it off. (I don't know if this is the case for other AsteroidOS watches.) I personally wouldn't recommend it for this specific task even though it's great for anything else.",
              "author": "Sateallia",
              "depth": 1
            }
          ]
        },
        {
          "top": "If you want truly standalone + WiFi + custom code, check SQFMI Watchy (ESP32). You can flash your own firmware, call HTTPS endpoints, parse JSON, and render custom UI without a phone in the loop.\nMain tradeoff: it’s very DIY (power management, UX polish, tooling). If you want less DIY, Wear OS gives a better SDK, but the platform lock-in is much stronger.",
          "author": "lyaocean",
          "replies": [
            {
              "text": "https://watchy.sqfmi.com\n :)",
              "author": "sqfmi",
              "depth": 1
            },
            {
              "text": "If you self assemble take great care with the buttons! Otherwise a great reliable device. Bought myself one as \"reward\". But broke the left lower push button. On board contact still works so it's usable. ESP so comes with wifi&BT goodness built in.",
              "author": "zoom6628",
              "depth": 2
            },
            {
              "text": "does the newer revision have USBC? I cant find a clear answer in the docs\nmicroUSB is kind of dealbreaker (feels a bit nitpicky and minor, but i just dont have any micro cables anymore)",
              "author": "geokon",
              "depth": 2
            }
          ]
        },
        {
          "top": "You can make standalone Garmin Wifi Apps for free (you only have to pay if you want to charge).\nDo note that it's very janky. By default web requests proxy via the phone - if you want to try wifi you have to switch into a \"modal wifi mode\" - do a batch of web requests and logic - then switch back into \"normal\" mode and then do something with the saved data.\nYou cannot push notifications from wifi but you could do:\n1. request to open the app using a background service\n2. toggle wifi mode\n3. fetch the data and close the app\n4. create watch notifications from the background service.\nYou may need to acknowledge the request to open the app with a button press - but I've also see examples of it just opening (not sure of conditions though).\nIf your Garmin is already paired with a wifi AP turn off your phone and give \nhttps://apps.garmin.com/apps/78a3fa7c-6f78-4d0e-94cd-b9bd874...\n a go to see the UX (needs Gemini API key).",
          "author": "msyea",
          "replies": []
        },
        {
          "top": "Bangle is a good one \nhttps://banglejs.com/\n\nQuite fun to work with. I got one from a conference, and my kid has been enjoying it a lot.",
          "author": "huydotnet",
          "replies": [
            {
              "text": "Wonder if you could get Clojure (through the SCI) running on it.\nHow is the general watch API? The development experience?\nis there some community or list of exciting projects made for it? for inspiration :)",
              "author": "geokon",
              "depth": 1
            },
            {
              "text": "When your watch supports the most common computer language on Earth, it is hard to go wrong with it. 4 week standby is pretty good too.",
              "author": "mycall",
              "depth": 1
            },
            {
              "text": "Bangle doesn't have standalone wifi...",
              "author": "jolmg",
              "depth": 1
            }
          ]
        }
      ]
    }
  ]
}