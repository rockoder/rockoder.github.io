{
  "scraped_date": "2026-02-24",
  "source": "hacker_news",
  "total_scraped": 136,
  "nontech_count": 38,
  "posts": [
    {
      "id": "47127396",
      "title": "Binance fired employees who found $1.7B in crypto was sent to Iran",
      "link": "https://www.nytimes.com/2026/02/23/technology/binance-employees-iran-firings.html",
      "domain": "www.nytimes.com",
      "author": "boplicity",
      "score": 425,
      "comment_count": 190,
      "created_ts": 1771874610,
      "is_internal": false,
      "post_text": "",
      "is_ask_hn": false,
      "matched_keywords": [
        "fired"
      ],
      "comments": [
        {
          "top": "Isn't this like the #1 use case for crypto?\nEveryone wants an untrackable unblockable currency that is out of government control until the day it is used for things they don't like, then suddenly \"government please control this!\"",
          "author": "paxys",
          "replies": [
            {
              "text": "I thought the #1 use case for crypto was ransomware, followed by shitcoin rug-pulls, and the ability to commit theft without recourse.\nSending money to Iran is just a minor edge case.",
              "author": "chihuahua",
              "depth": 1
            },
            {
              "text": "That's a rather narrow view of crypto's uses.  What about subverting democracy by bribing the President?",
              "author": "rwmj",
              "depth": 2
            },
            {
              "text": "Has the lack of crypto ever stopped this from happening? Look up cases of gold bars being found in senators houses, those are actually MUCH less tracable.\nShitcoins and Shitstocks(some SPACs) do allow of a legal way to \"give\" others money through the transfer of value in a way that is technically legal. This again is not crypto specific though.",
              "author": "carshodev",
              "depth": 3
            },
            {
              "text": "So the best part about being bribed with crypto is if one flees to another country to escape the law, one still has the aforementioned bribes.  That plus some measure of anonymity.",
              "author": "greesil",
              "depth": 4
            },
            {
              "text": "Anonymity is the one thing cryptocurrency does not do well. It's much harder to sieze but much easier to trace.",
              "author": "eucyclos",
              "depth": 5
            }
          ]
        },
        {
          "top": "Contrary to a lot of comments here, the only way to use bitcoin (or any cryptocurrency) without tracking is to mine it yourself, and even then...\nWhere did you get it? Purchased/transferred? Where did they get it? What else did the person with that wallet do?\nIf the answer is \"mined\", even then, you have to actually do something with it, right? Buy something? Where is that something shipped? At worst you'll have to pay customs on it, and have it actually get through customs. At best, your address is in a database now.\nHave it shipped somewhere obscure? Video cameras are everywhere.\nHave it shipped to someone else's house and steal it off their porch? Again, cameras everywhere.\nNot have a physical item? Just a service? That's pretty much the closest you'll get to anonymous money transfer and full usage (along with whatever VPN you prefer).\nCool that was a fun mental exercise.  Now everyone tell me why I'm wrong!",
          "author": "lacoolj",
          "replies": [
            {
              "text": "It's great for transactions where moving a large amount of money would be impossible due to KYC laws and smuggling the cash even more so. As long as you can find someone willing to sell you bitcoin and someone else willing to accept it, you're still in business.",
              "author": "wildzzz",
              "depth": 1
            },
            {
              "text": "I mean, I can meet you in an ally, transfer some satoshis from my wallet to yours, you hand me a wad of cash/jewels/MtG/collector funkos and you might not even know my name.",
              "author": "VirusNewbie",
              "depth": 1
            },
            {
              "text": "Hmm, doesn't this work equally well with a wad of $10 and $20 notes? I mean, yes, notes could be clandestinely marked. But aren't bitcoins also traceable after the first transaction?",
              "author": "nine_k",
              "depth": 2
            },
            {
              "text": "yes but harder to move 10M in cash around from country to country.\nI'm assuming I'm purchasing/selling a \nlot\n of MtG/Funko here in this example.",
              "author": "VirusNewbie",
              "depth": 3
            },
            {
              "text": "There’s a large industry for cleaning cash which then makes  moving  a clean 10M or even 10B in clean cash nearly trivial.\n10M might not be as noticeable but crypto being nominally in a country on its own isn’t that useful as you still want to be able to spend it at the end of the day.",
              "author": "Retric",
              "depth": 4
            }
          ]
        },
        {
          "top": "If one of two options can't be regulated or tracked, that is the option that will predominantly be used by actors who have outsized interest in being regulation or being tracked.",
          "author": "jstummbillig",
          "replies": []
        },
        {
          "top": "Is Iran supposed supposed to be banned on Binance?",
          "author": "BenGosub",
          "replies": [
            {
              "text": "Every US company/citizen is not allowed to do trade with Iran due to the ITSR laws except under highly specific situations.\nIt gets more complex if a company is multinational though.\nA citizen can travel to Iran but even if they buy something there on holiday if they bring it back to the US they need to go through complex customs procedures to make sure its legally brought back in.",
              "author": "carshodev",
              "depth": 1
            },
            {
              "text": "> Every US company/citizen [...] if they bring it back to the US\nIs that relevant here?\n> Binance Holdings Ltd., branded Binance, [...] was founded in 2017 by Changpeng Zhao. Binance was initially based in China, then moved to Japan, subsequently left Japan for Malta, and currently has no official company headquarters.\nThe founder seems to have been born in China and is Canadian.\nI still also don't understand if Iran is supposed to be banned on Binance or not.",
              "author": "embedding-shape",
              "depth": 2
            },
            {
              "text": "\"flowed from two Binance accounts to Iranian entities with links to terrorist groups, a possible violation of global sanctions.\"",
              "author": "giarc",
              "depth": 3
            },
            {
              "text": "It's a US-sanctioned country so allied nations play along with the sanctions and Binance is located within that US sphere of influence so Iran is supposed to be currently banned, yes.",
              "author": "arjie",
              "depth": 1
            },
            {
              "text": "It's not like Western(-ish) nations have much of a choice here. As soon as your banks and financial system depend on the USD in any way, it comes with a mandatory dose of US imperialism and extraterritorial jurisdiction.",
              "author": "hocuspocus",
              "depth": 2
            }
          ]
        },
        {
          "top": "Remember that the CEO of Binance was pardoned by Trump after pleading guilty to financial fraud.",
          "author": "LunaSea",
          "replies": [
            {
              "text": "It's more than just that.\n> President Trump granted a pardon to Binance’s founder, Changpeng Zhao, who had spent four months in federal prison in 2024 for his role in the firm’s crimes. The Trump family’s crypto start-up, World Liberty Financial, has forged close business ties with Binance, and Mr. Zhao was a guest this month at a conference at Mar-a-Lago, Mr. Trump’s club in Palm Beach, Fla.",
              "author": "paxys",
              "depth": 1
            },
            {
              "text": "it's more than just that:\nhttps://www.forbes.com/sites/zacheverson/2026/02/09/trump-st...\n> Binance holds about 87% of USD1, the stablecoin issued by a Trump family crypto venture—a greater concentration than any other major stablecoin has at a single exchange, roughly $4.7 billion of the $5.4 billion total supply.",
              "author": "lavezzi",
              "depth": 2
            },
            {
              "text": "Iran obviously missed the memo.  All they have to do is setup a wealth fund and invest heavily in a Trump venture; then they can become a most favored nation and forego all this conflict.",
              "author": "michaelteter",
              "depth": 1
            },
            {
              "text": "They'd have to invest more than adelson, ~250M.",
              "author": "throwaway85825",
              "depth": 2
            },
            {
              "text": "I wonder if the pardon bribe is less if your crime is something near and dear to the Orange King's heart.",
              "author": "ourmandave",
              "depth": 1
            }
          ]
        }
      ]
    },
    {
      "id": "47123631",
      "title": "Show HN: PgDog – Scale Postgres without changing the app",
      "link": "https://github.com/pgdogdev/pgdog",
      "domain": "github.com",
      "author": "levkk",
      "score": 194,
      "comment_count": 44,
      "created_ts": 1771860804,
      "is_internal": false,
      "post_text": "Hey HN! Lev and Justin here, authors of PgDog (<a href=\"https:&#x2F;&#x2F;pgdog.dev&#x2F;\">https:&#x2F;&#x2F;pgdog.dev&#x2F;</a>), a connection pooler, load balancer and database sharder for PostgreSQL. If you build apps with a lot of traffic, you know the first thing to break is the database. We are solving this with a network proxy that works without requiring application code changes or database migrations.<p>Our post from last year: <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=44099187\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=44099187</a><p>The most important update: we are in production. Sharding is used a lot, with direct-to-shard  queries (one shard per query) working pretty much all the time. Cross-shard (or multi-database) queries are still a work in progress, but we are making headway.<p>Aggregate functions like count(), min(), max(), avg(), stddev() and variance() are working, without refactoring the app. PgDog calculates the aggregate in-transit, while transparently rewriting queries to fetch any missing info. For example, multi-database average calculation requires a total count of rows to calculate the original sum. PgDog will add count() to the query, if it’s not there already, and remove it from the rows sent to the app.<p>Sorting and grouping works, including DISTINCT, if the columns(s) are referenced in the result. Over 10 data types are supported, like, timestamp(tz), all integers, varchar, etc.<p>Cross-shard writes, including schema changes (CREATE&#x2F;DROP&#x2F;ALTER), are now atomic and synchronized between all shards with two-phase commit. PgDog keeps track of the transaction state internally and will rollback the transaction if the first phase fails. You don’t need to monkeypatch your ORM to use this: PgDog will intercept the COMMIT statement and execute PREPARE TRANSACTION and COMMIT PREPARED instead.<p>Omnisharded tables, a.k.a replicated or mirrored (identical on all shards), support atomic reads and writes. That’s important because most databases can’t be completely sharded and will have some common data on all databases that has to be kept in-sync.<p>Multi-tuple inserts, e.g., INSERT INTO table_x VALUES ($1, $2), ($3, $4), are split by our query rewriter and distributed to their respective shards automatically. They are used by ORMs like Prisma, Sequelize, and others, so those now work without code changes too.<p>Sharding keys can be mutated. PgDog will intercept and rewrite the update statement into 3 queries, SELECT, INSERT, and DELETE, moving the row between shards. If you’re using Citus (for everyone else, Citus is a Postgres extension for sharding databases), this might be worth a look.<p>If you’re like us and prefer integers to UUIDs for your primary keys, we built a cross-shard unique sequence, directly inside PgDog. It uses the system clock (and a couple other inputs), can be called like a Postgres function, and will automatically inject values into queries, so ORMs like ActiveRecord will continue to work out of the box. It’s monotonically increasing, just like a real Postgres sequence, and can generate up to 4 million numbers per second with a range of 69.73 years, so no need to migrate to UUIDv7 just yet.<p><pre><code>    INSERT INTO my_table (id, created_at) VALUES (pgdog.unique_id(), now());\n</code></pre>\nResharding is now built-in. We can move gigabytes of tables per second, by parallelizing logical replication streams across replicas. This is really cool! Last time we tried this at Instacart, it took over two weeks to move 10 TB between two machines. Now, we can do this in just a few hours, in big part thanks to the work of the core team that added support for logical replication slots to streaming replicas in Postgres 16.<p>Sharding hardly works without a good load balancer. PgDog can monitor replicas and move write traffic to a promoted primary during a failover. This works with managed Postgres, like RDS (incl. Aurora), Azure Pg, GCP Cloud SQL, etc., because it just polls each instance with “SELECT pg_is_in_recovery()”. Primary election is not supported yet, so if you’re self-hosting with Patroni, you should keep it around for now, but you don’t need to run HAProxy in front of the DBs anymore.<p>The load balancer is getting pretty smart and can handle edge cases like SELECT FOR UPDATE and CTEs with INSERT&#x2F;UPDATE statements, but if you still prefer to handle your read&#x2F;write separation in code, you can do that too with manual routing. This works by giving PgDog a hint at runtime: a connection parameter (-c pgdog.role=primary), SET statement, or a query comment. If you have multiple connection pools in your app, you can replace them with just one connection to PgDog instead. For multi-threaded Python&#x2F;Ruby&#x2F;Go apps, this helps by reducing memory usage, I&#x2F;O and context switching overhead.<p>Speaking of connection pooling, PgDog can automatically rollback unfinished transactions and drain and re-sync partially sent queries, all in an effort to preserve connections to the database. If you’ve seen Postgres go to 100% CPU because of a connection storm caused by an application crash, this might be for you. Draining connections works by receiving and discarding rows from abandoned queries and sending the Sync message via the Postgres wire protocol, which clears the query context and returns the connection to a normal state.<p>PgDog is open source and welcomes contributions and feedback in any form. As always, all features are configurable and can be turned off&#x2F;on, so should you choose to give it a try, you can do so at your own pace. Our docs (<a href=\"https:&#x2F;&#x2F;docs.pgdog.dev\">https:&#x2F;&#x2F;docs.pgdog.dev</a>) should help too.<p>Thanks for reading and happy hacking!",
      "is_ask_hn": false,
      "matched_keywords": [
        "team",
        "feedback"
      ],
      "comments": [
        {
          "top": "Great progress, guys! It’s impressive to see all the enhancements - more types, more aggregate functions, cross-node DML, resharding, and reliability-focused connection pooling and more. Very cool! These were really hard problems and took multiple years to build at Citus. Kudos to the shipping velocity.",
          "author": "saisrirampur",
          "replies": []
        },
        {
          "top": "Stupid question but does this shard the database as well or do we shard manually and then setup the configuration accordingly ?",
          "author": "codegeek",
          "replies": [
            {
              "text": "It shards it as well. We handle schema sync, moving table data (in parallel), setting up logical replication, and application traffic cutover. The zero-downtime resharding is currently WIP, working on the PR as we speak: \nhttps://github.com/pgdogdev/pgdog/pull/784\n.",
              "author": "levkk",
              "depth": 1
            },
            {
              "text": "Incredible. I am really interested in trying pgdog for our B2B SAAS product. Will do some testing.",
              "author": "codegeek",
              "depth": 2
            }
          ]
        },
        {
          "top": "Some HTTP proxies can do retries -- if a connection to one backend fails, it is retried on a different backend. Can PgDog (or PgBouncer, or any other tool) do something similar -- if there's a \"database server shutting down\" error or a connection reset, retry it on another backend?",
          "author": "cuu508",
          "replies": [
            {
              "text": "Not currently, but we can add this. One thing we have to be careful of is to not retry requests that are executing inside transactions, but otherwise this would be a great feature.",
              "author": "levkk",
              "depth": 1
            }
          ]
        },
        {
          "top": "Happy pgdog user here, I can recommend it from a user perspective as a connection pooler to anyone checking this out (we're also running tests and positive about sharding, but haven't run it in prod yet, so I can't 100% vouch for it on that, but that's where we're headed.)\n@Lev, how is the 2pc coming along? I think it was pretty new when I last checked, and I haven't looked into it much since then. Is it feeling pretty solid now?",
          "author": "mijoharas",
          "replies": [
            {
              "text": "It feels better now, but we still need to add crash protection - in case PgDog itself crashes, we need to restore in-progress 2pc transaction records from a durable medium. We will add this very soon.",
              "author": "levkk",
              "depth": 1
            }
          ]
        },
        {
          "top": "Can you elaborate a bit more on the challenges faced in making Postgres shard-able?\nI remember that adding sharing to Postgres natively was an uphill battle. There were a few companies who has proprietary solutions for it. What you've been able to achieve is nothing less than a miracle.",
          "author": "written-beyond",
          "replies": [
            {
              "text": "So many, where to begin.\n1. People don't design schemas to be sharded, although many gravitate towards a common key, e.g. user_id or country_id or tenant_it or customer_id. Once that happens, sharding becomes easier.\n2. Postgres provides a lot of guarantees that are tricky to maintain when sharded: atomic changes, referential integrity, check constraints, unique indexes (and constraints), to name a few. Those have to be built separately by a sharding layer (like PgDog) and have trade-offs, usually around performance. It's a lot more expensive to check a globally enforced constraint than a local one (network hops aren't free).\n3. Online migrations from unsharded to sharded can be tricky: you have to redistribute terabytes of data while the DB continues to serve writes. You can't lose a single row - Postgres is used as a store of record and this can be a serious issue with business impact.\nWe're taking increasingly bigger bites at this apple. We started with basic query routing and are now doing query rewrites as well. We didn't handle data movements previously and now have almost fully automatic resharding. It takes time, elbow grease and most importantly, willing and courageous early adopters to whom we owe a huge debt of gratitude.",
              "author": "levkk",
              "depth": 1
            }
          ]
        }
      ]
    },
    {
      "id": "47116026",
      "title": "Show HN: WARN Firehose – Every US layoff notice in one searchable database",
      "link": "https://warnfirehose.com",
      "domain": "warnfirehose.com",
      "author": "sendkamal",
      "score": 132,
      "comment_count": 26,
      "created_ts": 1771803163,
      "is_internal": false,
      "post_text": "Hi HN,<p>I built WARN Firehose because I was frustrated trying to track layoff data across the US. The WARN Act requires companies with 100+ employees to file public notices 60 days before mass layoffs — but the data is scattered across 50 state websites with different formats, broken links, and no API.<p>WARN Firehose scrapes every state workforce agency daily and normalizes the data into a single database going back to 1988. It now has 131,000+ notices covering 14 million workers.<p>*What you can do:*<p>- Browse interactive charts and data tables (no account needed): <a href=\"https:&#x2F;&#x2F;warnfirehose.com&#x2F;data\" rel=\"nofollow\">https:&#x2F;&#x2F;warnfirehose.com&#x2F;data</a>\n- Drill into any state, city, company, or industry: <a href=\"https:&#x2F;&#x2F;warnfirehose.com&#x2F;data&#x2F;layoffs\" rel=\"nofollow\">https:&#x2F;&#x2F;warnfirehose.com&#x2F;data&#x2F;layoffs</a>\n- Query the REST API (free tier: 100 calls&#x2F;day): <a href=\"https:&#x2F;&#x2F;warnfirehose.com&#x2F;docs\" rel=\"nofollow\">https:&#x2F;&#x2F;warnfirehose.com&#x2F;docs</a>\n- Export in CSV, JSON, NDJSON, Parquet, or JSON-LD\n- Set up webhooks for real-time alerts on new filings<p>*Who uses this:*<p>- Journalists breaking layoff stories before press releases\n- Quant funds using WARN filings as an alternative data signal (filings happen ~60 days before layoffs)\n- Recruiters sourcing from displaced talent pools\n- Researchers studying labor market dynamics\n- Workforce development boards doing rapid response planning<p>*Tech stack:* Python&#x2F;FastAPI, SQLite, scrapers for all 50 states, static HTML generation for SEO pages, Claude Haiku for AI analysis, deployed on EC2.<p>Free tier is genuinely useful (100 API calls&#x2F;day, dashboard access, charts). Paid plans start at $19&#x2F;mo for full historical data and bulk exports.<p>Would love feedback on the API design, data quality, or anything else. Happy to answer questions.",
      "is_ask_hn": false,
      "matched_keywords": [
        "layoff",
        "feedback",
        "layoffs"
      ],
      "comments": [
        {
          "top": "Thank you for checking the work and all the comments which are very very valuable. Site does quite a lot to be honest so please take some time to explore more :)\n- It has over 15k individual landing pages for search engines - a dedicated page for each city, state, company, county. These pages are very reach on how they look  haha:\n- Exmaple: new jersey page-> \nhttps://warnfirehose.com/data/layoffs/new-jersey\n\n- Data is exportable to multiple format including json-d and parquet - i never heard of parquet before.\n- The site has MCP (model context protocol) built in !\n- it accepts payments on multiple methods including paypal, apple pay, amazon pay, card etc. All refund and all built in.\n- It also has an invisible admin dashboard where I can see everything (all metrics, signups payments etc.)\n- Reports are crazy - these are genuinely better than someone writing at wallstreet journal, seriously. you would have to check to see them.\n- The api endpoints.\nI am convinced the role Software engineer is going to go away by this year end. We are turning into builders, not coders. All SaaS apps are going to take big hit because they can be builkt better for fraction of cost by anyone - just think about it.\n- Its all about ideas and anyone can do it.",
          "author": "sendkamal",
          "replies": []
        },
        {
          "top": "The quant fund use case is the most interesting angle here. WARN filings have the rare property of being legally mandated with specific timing (60-day advance notice), which makes the signal horizon predictable in a way that most alternative data is not.\nThe big caveat: compliance is uneven. Companies under 100 employees are exempt, and there is a documented pattern of employers paying WARN Act penalties retroactively rather than filing -- especially in fast-moving situations where 60 days advance notice is operationally inconvenient. So the signal has systematic gaps at exactly the moments of highest market interest.\nHave you looked at coverage rates vs. announced layoffs (e.g., correlation with Challenger Gray reports or JOLTS)? That gap number is basically the signal noise floor for any quant strategy built on this data.",
          "author": "vicchenai",
          "replies": [
            {
              "text": "This is great insights @vicchenai. I just made it in a whim. I asked Gemini for suggestions and it presented few and I picked this one because there didn't seem to be good players out there. There is layoffdata and warntracker and all but they were built using hand coding :) it probably took them quite long to have it done. This is a challenging thing - 50 states having different data formats and having to poll them daily. Data cleaning is another thing. Historical data is also a thing. Plus all the blogs, reports, data presentation and api endpoints.\nIf it all can be done in a weekend without having to write a single line manually, sky is the limit no? Anybody having an idea can make these thing happen. I am not expecting it to make any money and it was a learning project but I do see some value for certain people.\nInsurance brokers would benefit if they are first to know so they coudl target these layed off people. Recruiters the same and definitely hedge funds, short shellers and quant. Gemini tells me data is the new oil ! I am convinced.",
              "author": "sendkamal",
              "depth": 1
            },
            {
              "text": "> and there is a documented pattern of employers paying WARN Act penalties retroactively rather than filing -- especially in fast-moving situations where 60 days advance notice is operationally inconvenient.\nOh, I got a solution for that. Don't just go for WARN Act penalties. Go after offenders with the hammer called SEC and market manipulation regulations. That kind of stuff \nreally\n hurts.",
              "author": "mschuster91",
              "depth": 1
            }
          ]
        },
        {
          "top": "This dataset looks interesting but the site doesn’t instill a lot of confidence in data integrity.\nOn the Charts page the selected time range is 12/01/2025 to 02/28/2026 and shows 106,603 employees affected. But the horizontal bar chart with state level data shows numbers in millions. For example, CA has more than 2 million and IL has more than 1.7 million employees affected. Then the layoff map at the bottom shows only layoffs in Texas.",
          "author": "malshe",
          "replies": [
            {
              "text": "Thank you @malshe. I will review and get it addressed. The site is not perfect at all and I will address them.",
              "author": "sendkamal",
              "depth": 1
            },
            {
              "text": "Agreed, this doesn't pass my smell test. November 2026 reports layoffs in NJ (with past tense) \nhttps://warnfirehose.com/blog/2026/11/week-1\nThe layoffs in the report are not listed in NJ's own warn notice\n\nhttps://www.nj.gov/labor/assets/PDFs/WARN/2026_WARN_Notice_A...",
              "author": "pdh",
              "depth": 1
            },
            {
              "text": "Thank you and I will have them addressed. There are many things in terms of data quality and I am still working on it.",
              "author": "sendkamal",
              "depth": 2
            },
            {
              "text": "You can think about LLM-generated UIs/apps the same way you think about LLM-generated responses. It's a bunch of garbage, but if you know what you're looking for, you might find something useful.\nThis doesn't seem to work at all for stats-related apps/sites though, since you can't judge the accuracy of what's being presented. If the site claims it'll \"take you to space,\" you don't take that literally, you just treat it as another AI artifact. But with numbers, you have no way to tell what's accurate and what's just made up.",
              "author": "csomar",
              "depth": 1
            },
            {
              "text": "Thats such a great point. What if I am (somehow) able to warrant that the data presented here exactly matches the state notices? would that be helpful?",
              "author": "sendkamal",
              "depth": 2
            }
          ]
        },
        {
          "top": "Interesting, though a lot of the UI seems broken. For my state I see some notice dates in the future (it's not explained why, if this is when the filing will be executed or if it's an incorrect filing date, as the column is just \"Notice Date\")\nSome of the entries pull up a page that says \"Failed to load company data: No company name provided in URL\" from the state specific view (e.g, any link on \nhttps://warnfirehose.com/data/layoffs/california\n ). Has a vibe-coded feel to it.\nI saw a lot of \"Purchase dataset for city details\" in places which was annoying. Wondering how much processing is being done on the base dataset to justify the pricing. Could you explain a bit on the normalization/cleaning process?",
          "author": "cobertos",
          "replies": [
            {
              "text": "@cobertos The UI things should have been addressed. However I did to an extensive testing myself and everything seemed to have worked for me. I am sold that vibe coding is a thing, seriously. To your question about normalization cleaning - I have gone through multiple iterations - the data is definitely not clean and all. However, claude has made a function (instead of cleaning them on demand, it has a function which gets updated with every pattern so it could be reused).\nFor the pricing, to be honest I have zero hope to make money. Its just out there because I wanted to integrate this with stripe for payments and all. At the same time, if you look at competitor sites, they seem crap lol. plus look at their pricing. The site does have some operating cost and I will have to recover them if i can if I want it to be self sustainable but only if this is a value to someone. I am trying to make this a value. Please offer if you have ideas and I would appreciated it :)",
              "author": "sendkamal",
              "depth": 1
            },
            {
              "text": "Definitely vibe coded. It follows the same generic Claude UI patterns for a data app / data oriented website. Not necessarily a bad thing per say if it's still curated and tweaked with human taste. And ofc validated to work :)",
              "author": "skadamat",
              "depth": 1
            },
            {
              "text": "Hi @skadamat - I am not very experienced with claude code and don't know what it has done previously but new opus 4.6 is great. I think I need to work on claude.md. That really seems to be the soul of Claude. I've incurred quite a lot of money on API cost and claude plan and my time. But I feel like that can be cut to 1/4th easily with a solid claude.md plan.",
              "author": "sendkamal",
              "depth": 2
            }
          ]
        },
        {
          "top": "Great site thank you. Just curious, I looked up my company(more than 40k employees across the world including many US states) and it seems like I am not seeing the layoffs that colleagues have experienced. This is probably expected as im probably missing some criteria. Do all layoffs have to have a WARN notice or are there mechanisms/criteria that allow companies to lay people off without filing these noticies?",
          "author": "nebula8804",
          "replies": [
            {
              "text": "I am agreeing with two comments. There might be many reasons. Just the fact that there are only 12M records in last 30 years or so for all 50 states it definitely doesn't represent all data - may be 10%",
              "author": "sendkamal",
              "depth": 1
            },
            {
              "text": "Many companies pay severance in lieu of giving notice.",
              "author": "jordanb",
              "depth": 1
            },
            {
              "text": "If the layoff is small enough, it's not triggered.",
              "author": "bagels",
              "depth": 1
            }
          ]
        }
      ]
    },
    {
      "id": "47123894",
      "title": "Show HN: Sowbot – Open-hardware agricultural robot (ROS2, RTK GPS)",
      "link": "https://sowbot.co.uk/",
      "domain": "sowbot.co.uk",
      "author": "Sabrees",
      "score": 127,
      "comment_count": 40,
      "created_ts": 1771861713,
      "is_internal": false,
      "post_text": "Sowbot is an open-hardware agricultural robot designed to close the &quot;prototype gap&quot; that kills most agri-robotics startups and research projects — the 18+ months spent on drivers, networking, safety watchdogs, and UI before you can even start on the thing you actually care about.<p>The hardware is built around a stackable 10×10cm compute module with two ARM Cortex-A55 SBCs — one for ROS 2 navigation&#x2F;EKF localisation, one dedicated to vision&#x2F;YOLO inference — connected via a single ethernet cable.<p>Centimetre-level positioning via dual RTK GNSS, CAN bus for field comms, and real-time motor control via ESP32 running Lizard firmware.<p>Everything — schematics, PCB layouts, firmware — is under open licences.\nThe software stack runs on RoSys&#x2F;Field Friend (for teams who want fast iteration) or DevKit ROS (for teams already in the ROS ecosystem). The idea is that a lab in one country can reproduce another lab&#x27;s experiment by sharing a Docker image.<p>Current status: the Open Core brain is largely fabricated, the full-size Sowbot body has a detailed BOM but isn&#x27;t yet assembled, and we have two smaller dev platforms (Mini and Pico) in various stages of testing.<p>We&#x27;re a small volunteer team and we&#x27;re looking for contributors — hardware, ROS, firmware, docs, whatever you can offer.<p>The best place to start is our Discord: <a href=\"https:&#x2F;&#x2F;discord.gg&#x2F;SvztEBr4KZ\" rel=\"nofollow\">https:&#x2F;&#x2F;discord.gg&#x2F;SvztEBr4KZ</a> — we have a weekly call if you&#x27;d prefer to just show up and chat.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Agroecology-Lab&#x2F;feldfreund_devkit_ros&#x2F;tree&#x2F;caatinga-dev\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Agroecology-Lab&#x2F;feldfreund_devkit_ros&#x2F;tre...</a>",
      "is_ask_hn": false,
      "matched_keywords": [
        "team",
        "networking"
      ],
      "comments": [
        {
          "top": "> The hardware is built around a stackable 10×10cm compute module with two ARM Cortex-A55 SBCs — one for ROS 2 navigation/EKF localisation, one dedicated to vision/YOLO inference — connected via a single ethernet cable.\nI will preface this by saying that I have nothing against ARM per se, that my employer/team supported a good chunk of the work for making ROS 2 actually work on arm64, and that there is some good hardware out there.\nI really don't understand why startups and research projects keep using weird ARM SBCs for their robots. The best of these SBCs is still \nvastly\n shittier in terms of software support and stability than any random Chinese Intel ADL-N box. The only reasons to use (weird) ARM SBCs in robots are that either (1) you are using a Jetson for Jetson things (i.e. Nvidia libraries), or (2) you have a product which requires serious cost optimization to be produced at a large scale. Otherwise you are just committing yourselves and your users/customers to a future of terrible-to-nonexistent support and adding significantly to the amount of work you need to bring up the new system and port existing tools to it.",
          "author": "cpgxiii",
          "replies": [
            {
              "text": "If you can send me an open hardware Intel, or Jetson I'd happily use it.\nPart of the point of this for me is to see what's possible with open hardware (down to chip level at least)",
              "author": "Sabrees",
              "depth": 1
            },
            {
              "text": "There are a variety of x86 products with Coreboot support, if what you are looking for is firmware openness. If what you are looking for is PCB design openness, the options are much fewer, but at that point you are probably optimizing for an overly niche objective.\n> Part of the point of this for me is to see what's possible with open hardware (down to chip level at least)\nI appreciate the idea, but this is essentially saying \"this project will prioritize a specific choice of one (core) piece of hardware to the detriment of everything else, users included\". Approximately none of your potential users are going to benefit from the \"openness\" of the SBC versus that of a more broadly-supported platform (I say \"openness\" because the reality of SBCs is that actually finding a usefully performant one that is completely blob-free is almost impossible). Open hardware means very little if it isn't running an upstream kernel and userland.",
              "author": "cpgxiii",
              "depth": 2
            },
            {
              "text": "The software does explicitly support Jetson for example, and I'm sure the stack would run on Intel if you want it to.\nThe Mainline kernel for this particular board is _almost_ there 6.20 or so I expect. Armbian support is good.",
              "author": "Sabrees",
              "depth": 3
            },
            {
              "text": "Framework? Maybe?",
              "author": "Neywiny",
              "depth": 2
            },
            {
              "text": "Framweork have done the best they can within the confines of Intel licensing, still a long way from being able to fabricate it though\nhttps://github.com/FrameworkComputer/Framework-Laptop-13/tre...\nIn a few years we'll all be using more open RISCV stuff of course.",
              "author": "Sabrees",
              "depth": 3
            }
          ]
        },
        {
          "top": "I'm not sure how much more work is currently being done on a project I'd heard about in the past, but you might be interested in seeing if you can collaborate/learn from Open Source Ecology:\nhttps://www.opensourceecology.org/",
          "author": "gaudystead",
          "replies": [
            {
              "text": "Wow nice to see the website has been updated in 2025. I followed it for years but it seemed to not get much updates.\nFolks might also be interested in the Precious Plastic community which has a global network of \"microfactories\" built around a set of open-source machines made for recycling and reusing plastics\nhttps://www.preciousplastic.com/\nhttps://community.preciousplastic.com/map\nAlso as far as fundamental machines go, this 2d printer is expected to come out later this year\nhttps://www.opentools.studio/",
              "author": "culi",
              "depth": 1
            }
          ]
        },
        {
          "top": "Looks great for a prototype. Has any modeling, simulation, or analysis been done of its off-road performance, i.e. mobility, GO/NOGO, motive efficiency, maneuverability on deformable terrain? This is critical for agricultural applications.\nHas any stress analysis been done on the frame? Looks to me like it could use a couple more triangles to reinforce those rectangles.\nHave you designed a skid-steering controller for it? Off-road skid steering can be quite variable obviously depending on terrain properties.",
          "author": "lorenzohess",
          "replies": [
            {
              "text": "Rosys (a middleware layer \nhttps://github.com/zauberzeug/rosys\n) has rosys.driving.Odometer and rosys.driving.Steerer it's essentially a differential drive kinematic model.\nHoping RTK dual-F9P moving-base setup (M4 in the roadmap) largely sidesteps the worst of this — NAV-RELPOSNED gives us a real heading vector independent of wheel odometry, and the robot_localisation EKF can weight RTK heavily and odometry lightly when GNSS quality is good.",
              "author": "Sabrees",
              "depth": 1
            },
            {
              "text": "The current simulation is underdeveloped but can be found here \nhttps://github.com/samuk/caatingarobotics/tree/jazzy/src/agr...\nThe frame will almost certainly need more triangles",
              "author": "Sabrees",
              "depth": 1
            }
          ]
        },
        {
          "top": "What's your payload?\nWhere are the seeds?\nHow are they deposited?\nRecommend going to a farm right now to see how this works in production. For the most part, you can autonomously sow using GPS. But the farmer just rides along.",
          "author": "jvanderbot",
          "replies": [
            {
              "text": "Payload is whatever you (or your startup) want it to be.\nFor me personally mechanical between row weeding is step one, then laser in-row weeding.\n1. These on some linear actuators: \nhttps://www.getearthquake.com/products/fusion-drill-powered-...\n (they work surprisingly well)\n2. Beyond that for in-row weeding a engraving laser on a Delta: \nhttps://github.com/Agroecology-Lab/Open-Weeding-Delta/tree/m...\nOr if I'm feeling rich by then this third party weeder looks pretty good \nhttps://github.com/Laudando-Associates-LLC/LASER\n3. For Seeding my salad crop\n\nhttps://reagtools.co.uk/collections/jang\n4. Harvesting my salad crop\n\nhttps://reagtools.co.uk/products/quick-cut-greens-harvester\nI live on a farm, I have sold salad commercially, these are largely tools I already use and own, just moved about by motors rather than muscles.\nThis is a smaller scale thing than arable. We're talking a step up from manual horticulture (which is actually what still feeds much of the world)",
              "author": "Sabrees",
              "depth": 1
            }
          ]
        },
        {
          "top": "Very cool! shameless self promotion but check out greenwave-monitor[1] for the 'Diagnostics TUI'. I'll get it into the buildfarm soon.\n[1] \nhttps://github.com/NVIDIA-ISAAC-ROS/greenwave_monitor",
          "author": "sgillen",
          "replies": [
            {
              "text": "Nice, thanks! looks like a good one..",
              "author": "Sabrees",
              "depth": 1
            }
          ]
        }
      ]
    },
    {
      "id": "47115149",
      "title": "Mexican Forces Kill Nation's Most-Wanted Cartel Boss",
      "link": "https://www.nytimes.com/2026/02/22/world/americas/jalisco-new-generation-cartel-leader-killed.html",
      "domain": "www.nytimes.com",
      "author": "downboots",
      "score": 93,
      "comment_count": 37,
      "created_ts": 1771797345,
      "is_internal": false,
      "post_text": "",
      "is_ask_hn": false,
      "matched_keywords": [
        "boss"
      ],
      "comments": [
        {
          "top": "Puerto Vallarta is on fire - seeing tons of videos on my timeline.",
          "author": "joshcsimmons",
          "replies": [
            {
              "text": "How does this usually work - they just set some cars on fire to prove a point and demonstrate they are \"doing something\" and then just switch to the next boss that comes out of the secession fight ? Or is it more complicated or nuanced ?",
              "author": "m4rtink",
              "depth": 1
            },
            {
              "text": "Apparently as \"retaliation\" from the cartel because their boss was killed. Where do you draw the line?\nAt what point do you decide to go full El Salvador / Bukele on violent cartel members who are willing to put cities on fire when they cannot human and drug traffic at will?\nWhen is enough enough?",
              "author": "TacticalCoder",
              "depth": 1
            },
            {
              "text": "Deciding is one thing, carrying out the decision is another. The Mexican government and security forces have been heavily compromised by the cartels for years. Some of the smaller law enforcement actions are a form of \"kayfabe\". Even if President Sheinbaum gives the order, there may not be enough honest and loyal personnel to carry it out.\nMexico is a failed state. We can argue about who bears responsibility but that is the reality today.",
              "author": "nradov",
              "depth": 2
            },
            {
              "text": "I don't think that's possible in Mexico. There's too much power in the logistical networks that move things into the US. The demand is too great. Even if you kill every drug trafficker and gang member alive today and create huge prosperity the void will be filled by someone and they will be adversarial to the government and they will have to use extra judicial violence to enforce their position.\nThe cartel's presence in Mexico is extremely muted relative to their power.",
              "author": "hattmall",
              "depth": 2
            },
            {
              "text": "Just purely as a hypothetical thought exercise I wonder how infiltrated the US gov is by cartels.",
              "author": "rand846633",
              "depth": 3
            }
          ]
        },
        {
          "top": "Most-Wanted Cartel Boss \nso far",
          "author": "foogazi",
          "replies": [
            {
              "text": "Honestly probably not even that. Past most-wanteds were probably more querido.",
              "author": "NewJazz",
              "depth": 1
            },
            {
              "text": "Horror vacui",
              "author": "downboots",
              "depth": 1
            }
          ]
        },
        {
          "top": "As long as Mexico shares a border with the US the biggest consumer of such recreational drugs and poverty in Mexico there will always be drug cartels.\nIt is basically whack-a-mole killing or imprisoning cartel heads - there will be splinter factions and you will just get three just as nasty ones in it's place.",
          "author": "tibbydudeza",
          "replies": [
            {
              "text": "Agreed, it’s a demand-side problem. The profit potential is so great that \nsomeone\n will smuggle drugs into the US, no matter the risk.",
              "author": "quickthrowman",
              "depth": 1
            },
            {
              "text": "There's always huge demand for illegal things, doesn't mean a such a powerful market forms",
              "author": "roger110",
              "depth": 2
            }
          ]
        },
        {
          "top": "I imagine that if the U.S. assisted in any meaningful way ala the search for Escobar in Columbia we probably would have heard it by now.",
          "author": "F7F7F7",
          "replies": [
            {
              "text": "From TFA:\nThe Mexican government said the United States had contributed intelligence that aided the operation against Mr. Oseguera.",
              "author": "nsvd2",
              "depth": 1
            },
            {
              "text": "The US confirmed they’re involved. They provided intel, and there’s speculation the Mexican Army was also using american weapons.",
              "author": "testfrequency",
              "depth": 1
            },
            {
              "text": "Colombia*",
              "author": "Gualdrapo",
              "depth": 1
            }
          ]
        },
        {
          "top": "And thousands more will die due to the fully predictable and justified retaliation that follows. This doesn't meaningfully weaken the cartels, but forces them to respond in order to not compromise future safety.\nPrioritizing showy executions over actual progress, words that should describe the cartels, not the government.",
          "author": "JasonADrury",
          "replies": [
            {
              "text": "> justified retaliation\nI don’t see how it would be justified. Do you think the cartels are in the right here?",
              "author": "echoangle",
              "depth": 1
            },
            {
              "text": "> Do you think the cartels are in the right here?\nOf course not, none of the sides are at all in the right here. But from the cartels perspective, they're almost certainly in the right.\nDrug cartels are entirely the result of poor policies, and the blame for all the harms caused by them rests primarily on the shoulders of those perpetuating those policies. Surely the politicians that vote for laws that directly enable drug cartels to exist in the first place must be worse than the leaders of any individual drug cartel?\nAny kind of serious analysis of who's more right would end up being a work at the scale of Rising Up and Rising Down, that's probably best avoided.\nIf we oversimplify cartels into innocent businessmen just looking to sell drugs, with governments being the ones that introduced violence into the equation in their effort to stop them? Surely it must be the cartels\nIf we oversimplify cartels into evil criminals just looking to wield power over other human beings, with governments just trying to liberate people from cartel tyranny? Surely it must be the governments",
              "author": "JasonADrury",
              "depth": 2
            },
            {
              "text": "> But from the cartels perspective, they're almost certainly in the right.\nSure, but so is basically everyone. ISIS is in the right and justified then, too.",
              "author": "echoangle",
              "depth": 3
            },
            {
              "text": "Also, coalition didn't get rid of ISIS by negotiation or something. They used bombs and, in one advisor's words, entrenching tools.",
              "author": "roger110",
              "depth": 4
            },
            {
              "text": "Drug cartels are entirely the result of poor policies, one of which is that the Mexican government has not been punishing the cartels. Maybe they're fixing that.",
              "author": "roger110",
              "depth": 3
            }
          ]
        }
      ]
    },
    {
      "id": "47128138",
      "title": "\"Car Wash\" test with 53 models",
      "link": "https://opper.ai/blog/car-wash-test",
      "domain": "opper.ai",
      "author": "felix089",
      "score": 89,
      "comment_count": 105,
      "created_ts": 1771877768,
      "is_internal": false,
      "post_text": "&quot;I Want to Wash My Car. The Car Wash Is 50 Meters Away. Should I Walk or Drive?&quot; This question has been making the rounds as a simple AI logic test so I wanted to see how it holds up across a broad set of models. Ran 53 models (leading open-source, open-weight, proprietary) with no system prompt, forced choice between drive and walk, with a reasoning field.<p>On a single run, only 11 out of 53 got it right (42 said walk). But a single run doesn&#x27;t prove much, so I reran every model 10 times. Same prompt, no cache, clean slate.<p>The results got worse. Of the 11 that passed the single run, only 5 could do it consistently. GPT-5 managed 7&#x2F;10. GPT-5.1, GPT-5.2, Claude Sonnet 4.5, every Llama and Mistral model scored 0&#x2F;10 across all 10 runs.<p>People kept saying humans would fail this too, so I got a human baseline through Rapidata (10k people, same forced choice): 71.5% said drive. Most models perform below that.<p>All reasoning traces (ran via Opper, my startup), full model breakdown, human baseline data, and raw JSON files are in the writeup for anyone who wants to dig in or run their own analysis.",
      "is_ask_hn": false,
      "matched_keywords": [
        "startup"
      ],
      "comments": [
        {
          "top": "The fun thing about this is that I can see myself answering \"just walk there! It's near\" if I'm extremely sleepy or drunk. LLMs are indeed weird creatures :)",
          "author": "haolez",
          "replies": []
        },
        {
          "top": "I know it's against the rules but I thought this transcript in Google Search was a hoot:\n  so i heard there is some question about a car wash that most ai agents \n  get wrong. do you know anything about that? do you do better?\n\n\nwhich gets the answer:\n  Yes, I am familiar with the \"Car Wash Test,\" which has gone viral recently \n  for highlighting a significant gap in AI reasoning.\n  \n  The question is: \"I want to wash my car and the car wash is 50 meters away. Should I \n  walk or drive?\"\n  \n  Why Most AIs Get It Wrong\n\n  Many AI models, particularly those optimized for fast, helpful, and concise answers, \n  focus on the \"short distance\" (50 meters) and \"walking vs. driving\" as a lifestyle/efficiency \n  question. They often recommend walking to save gas, avoid engine wear, or get exercise.\n  \n  However, this answer is wrong because it misses the core objective: you need the car to be \n  physically present at the car wash to get it cleaned. \n\n  Do I Do Better?\n\n  Yes, I do. The key is understanding the objective (cleaning the car) rather than just \n  processing the keywords (distance/transportation). \n\n  My answer:\n\n    You should drive the car there. Even though it is a very short distance, your car \n    cannot wash itself. If you walk, the car stays in your driveway, and you cannot wash it.\n\n  Why This Matters\n  \n  The test highlights a key limitation in current AI: the difference between \"pattern matching\" \n  (predicting the next likely word based on training data) and \"true, grounded reasoning\". It \n  shows that models sometimes lack a \"world model\" that understands physical realities, such as\n  the fact that a car must be present at a car wash.",
          "author": "PaulHoule",
          "replies": [
            {
              "text": "LLMs sure do love to burn tokens. It’s like a high schooler trying to meet the minimum word length on a take home essay.",
              "author": "gnatman",
              "depth": 1
            },
            {
              "text": "I've always wondered about that. LLM providers could easily decimate the cost of inference if they got the models to just stop emitting so much hot air. I don't understand why OpenAI wants to pay 3x the cost to generate a response when two thirds of those tokens are meaningless noise.",
              "author": "estimator7292",
              "depth": 2
            },
            {
              "text": "Because inference costs are negligible compared to training costs",
              "author": "mattclarkdotnet",
              "depth": 3
            },
            {
              "text": "Because they don't yet know how to \"just stop emitting so much hot air\" without also removing their ability to do anything like \"thinking\" (or whatever you want to call the transcript mode), which is hard because knowing which tokens are hot air is the hard problem itself.\nThey basically only started doing this because someone noticed you got better performance from the early models by straight up writing \"think step by step\" in your prompt.",
              "author": "ben_w",
              "depth": 3
            },
            {
              "text": "IMO it supports the framing that it's all just a \"make document longer\" problem, where our human brains are primed for a kind of illusion, where we perceive/infer a mind because, traditionally, that's been the only thing that makes such fitting language.",
              "author": "Terr_",
              "depth": 4
            }
          ]
        },
        {
          "top": "> This is a trivial question. There's one correct answer and the reasoning to get there takes one step: the car needs to be at the car wash, so you drive.\nI don’t think it’s that easy. An intelligent mind will wonder why the question is being asked, whether they misunderstood the question, or whether the asker misspoke, or some other missing context. So the correct answer is neither “walk” nor “drive”, but “Wat?” or “I’m not sure I understand the question, can you rephrase?”, or “Is the vehicle you would drive the same as the car that you want to wash?”, or “Where is your car currently located?”, and so on.",
          "author": "layer8",
          "replies": [
            {
              "text": "The reason that those questions are asked, though, is that the answer to the actual question \nis\n obvious, so a human will start to wonder if it's some kind of trick.",
              "author": "BrenBarn",
              "depth": 1
            },
            {
              "text": "The answer wasn’t obvious to me, it was more like “parse error”.",
              "author": "layer8",
              "depth": 2
            },
            {
              "text": "Yep, just a little more context and all/most of the models would do much better. And sure, most average+ intelligence adults whose first language is English (probably) don't need this, but they're not the target audience for the instructions :)\n\"The 'car wash' is a building I need to drive through.\"\nor\n\"The 'car wash' is a bottle of cleaning fluid that I left at the end of my driveway.\"\nhttps://i5.walmartimages.com/seo/Rain-x-Foaming-Car-Wash-Con...",
              "author": "kayge",
              "depth": 1
            },
            {
              "text": "I think most people would say \"drive?\" and wonder when the punchline is coming, but (IMO) I don't think they'd start asking for clarification right away.",
              "author": "nozzlegear",
              "depth": 1
            },
            {
              "text": "I agree. If the LLM were truly an intelligence, it would be able to \nask\n about this nonsense question. It would be able to ask \"Why is walking even an option? Can you please explain how you imagine that would work? Do you mean hand-washing the car at home, instead?\" (etc, etc)\nReal people can ask for clarification when things are ambiguous or confusing. Once something is clarified, they can work that into their understanding of how someone communicates about a given topic. An LLM can't.",
              "author": "Night_Thastus",
              "depth": 1
            }
          ]
        },
        {
          "top": "The human baseline seems flawed.\n1. There is no initial screening that would filter out garbage responses. For example, users who just pick the first answer.\n2. They don't ask for reasoning/rationale.",
          "author": "tantalor",
          "replies": [
            {
              "text": "My favorite example of this was the Pew Research study: \nhttps://www.pewresearch.org/short-reads/2024/03/05/online-op...\nThey found that ~15% of US adults under 30 claim to have been trained to operate a nuclear submarine.",
              "author": "slongfield",
              "depth": 1
            },
            {
              "text": "Lizardman's Constant is famously 4%.  \nhttps://en.wikipedia.org/wiki/Slate_Star_Codex#Lizardman's_C...",
              "author": "mwigdahl",
              "depth": 1
            },
            {
              "text": "RE 1, they actually do have a pre-screening screening of the participants in general, you can check how they do it in detail: \nhttps://www.rapidata.ai/",
              "author": "felix089",
              "depth": 1
            },
            {
              "text": "Ah, that's good to hear. I didn't see anything like that in the data dump so I assumed they don't do that. Glad to be corrected.",
              "author": "tantalor",
              "depth": 2
            },
            {
              "text": "I agree. I wonder what the human baseline is for ”what is 1 + 1” on Rapidata.",
              "author": "andreasgl",
              "depth": 1
            }
          ]
        },
        {
          "top": "Would be interesting to see Sonnet (4.6*). It's fair bit smaller than Opus but scores pretty high on common sense, subjectively.\nI'm also curious about Haiku, though I don't expect it to do great.\n--\nEDIT: Opus 4.6 Extended Reasoning\n> Walk it over. 50 meters is barely a minute on foot, and you'll need to be right there at the car anyway to guide it through or dry it off. Drive home after.\nWeird since the author says it succeeded for them on 10/10 runs. I'm using it in the app, with memory enabled. Maybe the hidden pre-prompts from the app are messing it up?\nI tested Sonnet 4.5 first, which answered incorrectly.. maybe the Claude app's memory system is auto-injecting it into the new context (that's how one of the memory systems works, injects relevant fragments of previous chats invisibly into the prompt).\ni.e. maybe Opus got the garbage response auto-injected from the memory feature, and it messed up its reasoning? That's the only thing I can think of...\n--\nEDIT 2: Disabled memories. Didn't help. But disabling the biographical information too, gives:\n>Opus 4.6 Extended Reasoning\n>Drive it — the whole point is to get the car there!\n--\nEDIT 3: Yeah, re-enabling the bio or memories, both make it stupid. Sad! Would be interesting to see if other pre-prompts (e.g. random Wikipedia articles) have an effect on performance. I suspect some types of pre-prompts may actually boost it.",
          "author": "andai",
          "replies": [
            {
              "text": "Interesting. I wonder if that's related to the phenomenon mentioned in the Opus 4.6 model card[1], where increased reasoning effort leads to 4.6 overthinking and convincing itself of the wrong answer on many questions. It seems to be unique to 4.6; I guess they fried it a bit too much during RL training.\n[1] \nhttps://www.anthropic.com/claude-opus-4-6-system-card",
              "author": "stratos123",
              "depth": 1
            },
            {
              "text": "I tested this with Opus the day 4.6 came out and it failed then, still fails now. There were a lot of jokes I've seen related to some people getting a 'dumber' model, and while there's probably some grain of truth to that I pay for their highest subscription tier so at the very least I can tell you it's not a pay gate issue.",
              "author": "Ethee",
              "depth": 1
            },
            {
              "text": "You mean Sonnet 4.6? I ran 9 claude models including Haiku, swipe through the gallery in the link to see their responses.",
              "author": "felix089",
              "depth": 1
            },
            {
              "text": "I don't see Sonnet 4.6 in the screenshots. I see the other Claude models though.\nEdit: Found Haiku. Alas!",
              "author": "andai",
              "depth": 2
            },
            {
              "text": "Yea good catch Sonnet 4.6 is not part of the test.",
              "author": "felix089",
              "depth": 3
            }
          ]
        }
      ]
    },
    {
      "id": "47117946",
      "title": "What I learned designing a barebones UI engine",
      "link": "https://madebymohammed.com/miniui",
      "domain": "madebymohammed.com",
      "author": "teleforce",
      "score": 75,
      "comment_count": 41,
      "created_ts": 1771819309,
      "is_internal": false,
      "post_text": "",
      "is_ask_hn": false,
      "matched_keywords": [
        "learned"
      ],
      "comments": [
        {
          "top": "> While it’s far from perfect, writing it taught me more about UI systems than I ever would have learned by sticking to established solutions alone.\nThis is a great attitude to have. Keep up the great work.",
          "author": "dazzawazza",
          "replies": []
        },
        {
          "top": "Immediate mode GUI is the way to go.\nRetaining state is a pain and causes bugs. Trying to get fancy a la react and diffing the tree for changes makes not sense. That was a performance hack because changing the DOM in JS used to be slow as hell. You don't need that.\nJust redraw the whole thing every frame. Great performance, simple, less bugs.",
          "author": "cardanome",
          "replies": [
            {
              "text": "This works for simple apps, utilities, and demos/mvps. Not great for actual applications.\nWhat about when you're embedding your GUI into an existing application? or for use on an already taxed system? (Audio plugins come to mind)\nWhat if something is costly, that you need to compute dynamically, but not often, makes it into the frame? Do you separately now create a state flag for that one render object?",
              "author": "tarnith",
              "depth": 1
            },
            {
              "text": "> What if something is costly, that you need to compute dynamically, but not often, makes it into the frame? Do you separately now create a state flag for that one render object?\nThe point of immediate mode UIs is not necessarily that there is no state specific to the UI, but rather that the state is owned by user code. You can (and, in these more complex cases, should) retain state between frames. The main difference is that the state is still managed by your code, rather than the UI system (\"library\", whatever).",
              "author": "spiffyk",
              "depth": 2
            },
            {
              "text": "Immediate mode UI optimizes for the worst case. That is the case you care about most for real time applications.\nRetained mode is more optimal when not much changes but if a lot of stuff changes at once it can be worse. So for real time applications like your audio example or games you want immediate mode. Retained mode is better for saving battery though or can be.",
              "author": "cardanome",
              "depth": 2
            },
            {
              "text": "> What about when you're embedding your GUI into an existing application? or for use on an already taxed system?\nYou should check out the gamedev scene. It's soft real-time, and yet dearIMGUI is the choice for tooling. Immediate-mode is an API-design, not the implementation details. All Immediate-mode GUIs retain data some data, and for that reason they each have their own APIs for retaining data in various capacities. Usually something really simple like hashing and component-local state.\n> This works for simple apps, utilities, and demos/mvps. Not great for actual applications.\nRespectfully, I don't think you're informed on this. Probably the most responsive debugger out there is RAD Debugger and it's built with an IMGUI.",
              "author": "leecommamichael",
              "depth": 2
            },
            {
              "text": "For interest sake, have a look at the flutter engine. It does this kind of diff on each build (meaning, each time the ui tree gets modified & triggers a rebuild); they split their objects into stateful & stateless, and then in your own code you have to make sure to not unnecessarily trigger rebuilds for expensive objects. So it kinda force you to think & separate cheap & expensive ui objects.",
              "author": "BatteryMountain",
              "depth": 2
            }
          ]
        },
        {
          "top": "Had a similar itch during my game development with libgdx, and had almost same architecture eventually\nI found that I have two different ways to construct UI layout , from top down, and from down to top, those could be contradictory, wonder how one could solve this, seems like common problem in all frameworks that I saw, like flutter just fail with error on screen if it can't solve restrictions in such conflict , others just show jiberish",
          "author": "threetwoonezero",
          "replies": []
        },
        {
          "top": "Lots of confused comments here about immediate vs retained GUIs. Immediate-mode is an API-design, not an implementation detail. All Immediate-mode GUIs retain data, and for that reason they each have their own APIs for retaining data in various capacities. Usually something really simple like hashing for component-local state.",
          "author": "leecommamichael",
          "replies": []
        },
        {
          "top": "author here - thanks for posting :D\nwould love to hear any thoughts or questions.",
          "author": "mbmproductions",
          "replies": [
            {
              "text": "At what point do you think you’d give in and use an existing solution?\nI’m going through similar “hell” (my words). I just wanted some simple Ui for WebXR but the dipshits that designed XR for the web fucking threw all the web parts out so you can not just put up a few simple html elements up in XR. You have to write your own UI library from scratch. It’s so mind bogglingly stupid.\nIn any case, having to write it, like you I started small and the it quickly ballooned because even simple things get complicated quickly, all the while I’m cursing under my breath there is a perfectly unable system but TPTB chose not to offer it >:(",
              "author": "socalgal2",
              "depth": 1
            },
            {
              "text": "existing solutions for my specific use case were limited, i intentionally wanted something barebones so that I could hack at the raw surfaces / display buffers to cut corners for performance. one part learning experience, one part stubbornness.\nWebXR sounds like a different beast entirely. do you have to write your own rendering backend in WebGL for that?",
              "author": "mbmproductions",
              "depth": 2
            },
            {
              "text": "Webgl or webgpu, but for ui you can just draw to a canvas and copy that canvas into webgl/webgpu. That means you can easily iterate on the ui system in 2d with mouse input similar to what you’ve been doing in python\nBut, as an example of similar complexity. I hacked together the first Ui I needed. It was 6 buttons and a slider. Then I realized I needed two more Ui panels and started to write more real Ui classes to abstract stuff out and, it just starts getting more and more complicated.\nA slider needs to “capture the pointer”, meaning as you drag it, if to drag past the end of the slider and the pointer is now over another widget the events should still go to the slider. It’s not hard to implement, it’s just frustrating for me that I don’t actually want to write it. I just want to make my panel and if I could just use html id be done. Instead I’m spending my limited free time making this ui system. I don’t have that much and would be much further along in my actual goals except for this road block",
              "author": "socalgal2",
              "depth": 3
            },
            {
              "text": "Looks really cool. Thank you for sharing!",
              "author": "selridge",
              "depth": 1
            }
          ]
        }
      ]
    },
    {
      "id": "47128535",
      "title": "Show HN: Babyshark – Wireshark made easy (terminal UI for PCAPs)",
      "link": "https://github.com/vignesh07/babyshark",
      "domain": "github.com",
      "author": "eigen-vector",
      "score": 63,
      "comment_count": 30,
      "created_ts": 1771879534,
      "is_internal": false,
      "post_text": "Hey all, I built babyshark, a terminal UI for PCAPs aimed at people who find Wireshark powerful but overwhelming.<p>The goal is “PCAPs for humans”:\nOverview dashboard answers what’s happening + what to click next<p>Domains view (hostnames first) → select a domain → jump straight to relevant flows\n(works even when DNS is encrypted&#x2F;cached by using observed IPs from flows)<p>Weird stuff view surfaces common failure&#x2F;latency signals (retransmits&#x2F;out-of-order hints, resets, handshake issues, DNS failures when visible)<p>From there you can drill down: Flows → Packets → Explain (plain-English hints) &#x2F; follow stream<p>Commands:\nOffline: babyshark --pcap capture.pcap<p>Live (requires tshark): babyshark --list-ifaces then babyshark --live en0<p>Repo + v0.1.0 release: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;vignesh07&#x2F;babyshark\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;vignesh07&#x2F;babyshark</a><p>Would love feedback on UX + what “weird detectors” you’d want next.",
      "is_ask_hn": false,
      "matched_keywords": [
        "feedback",
        "failure"
      ],
      "comments": [
        {
          "top": "I am in the target audience of \"would like to see network activity and debug occasional traffic but totally overwhelmed by termshark.\" So I appreciate the \"what should I click?\" thing, and offering weird flows to investigate.\n---\nSome UX bits I noticed after playing around for a few minutes:\n- Esc for backwards navigation was not obvious for me. Maybe emphasize that somehow, and/or support Backspace too for backnav?\n- Enter on Domains menu item does not work\n- don't mention clicking if mouse is not supported. \"Select\" would be more appropriate\n- packets screen is truncated vertically and horizontally. Probably should be scrollable\n- \"weird stuff\" options are numbered 1-5, but pressing those keys has no effect. There's lots of little polish fixes like this.\n---\nAnd then things I wonder about as a novice user:\n- Is it possible to see domain names instead of IP addresses while e.g. looking at packets?\n- What does it mean to f stream?\n- How do I inspect packets? Especially compressed or encrypted data? This is more a knowledge gap, like \"what am I supposed to look for\", \"what could be in a packet\", and I guess involves reverse engineering sometimes, but it's also a tooling question.",
          "author": "wonger_",
          "replies": [
            {
              "text": "Thanks a lot for trying. My experience with packet inspection is similar and that's what resulted in me trying to build a simpler plain language UI companion.\n• Back navigation: good call. I'll make \"Esc back\" more explicit everywhere and add Backspace as an alias for back (and mention it in h help).\n• Enter on Domains not working: it should drill down to flows. If you can share your OS/terminal + whether you installed from release vs cargo install, I'll try to reproduce and fix in the next release.\n• \"Clicking\" wording: agreed — mouse isn't supported right now. I'll change Ul copy to say Select (and keep \"Enter = drill down\").\n• Packets screen truncation: yep, needs scrolling/paging. On the list views I already do r/; l'll add page scroll and horizontal handling where it makes sense.\n• Numbered weird options: great idea - I'll map 1..9 to jump-select and Enter (or open directly).\n• Domain names in packets: yes, I want that. I already collect DNS/SNI/HTTP host hints; next step is showing hostname labels alongside IPs when I have more confident mapping.\n• \"f stream\" definition: I'll clarify it as \"Follow stream (reassembled payload)\" and add a glossary/help entry.\n• Inspecting encrypted/compressed data: totally fair. The tool can't decrypt TLS without keys, but it can make it clearer what's happening (SNI/ ALPN, sizes, timing, resets/retransmits). I'Il improve \"Explain\" to say what's possible vs not.\nSuper useful feedback thanks!",
              "author": "eigen-vector",
              "depth": 1
            },
            {
              "text": "All this has been pushed to main. Thanks again!",
              "author": "eigen-vector",
              "depth": 2
            }
          ]
        },
        {
          "top": "Regardless of the result of the TUI - I'd try this out just because you found the perfect name. Well done!",
          "author": "denysvitali",
          "replies": [
            {
              "text": "thank you :) I would be lying if I said the name wasn't the spark. The project lives to serve the name.",
              "author": "eigen-vector",
              "depth": 1
            }
          ]
        },
        {
          "top": "the overwhelming part of wireshark is, at least in my experience teaching networking at a college level, the actual networking part. protocols, flows, packet structure, etc. kids tend to be up to speed on the UI part pretty quickly.\nwhat the kids in my classes really struggle with is actually using any command line stuff (at least for a month or two), because it is so foreign to them (coming from GUI-only experience).\nwhat specific parts are made easier with babyshark, compared to wireshark? the github readme didnt really sell me on the \"easier than GUI\" part, nor did your description here. is it the \"explain (plan-English hints)\" part? if so, i think you should focus on that. right now it looks pretty bare bones (e.g. \"Weird stuff\" does not seem easier or super helpful from a learning perspective)",
          "author": "john_strinlai",
          "replies": [
            {
              "text": "I remember going into my networking unit and absolutely destroying it through the use of the command line. Everyone else was clicking through the wireshark GUI and I just grepped every answer. Finished the hour long practical assessment in about 15 minutes having run everything twice.\nCLI is so valuable because rather than explore a presentation of the data you plan your RE etc and then run it and it either returns the answer or it doesn't.\nThere are some TUIs I quite like (LNAV as a pager) but I think if you really know what you're dealing with the CLI is better almost every time.",
              "author": "Affric",
              "depth": 1
            },
            {
              "text": "I'm not trying to say it's better than the GUI but it hopes to be more guided. it’s *opinionated* about the first 60 seconds:\n- *Overview dashboard*: immediately surfaces top talkers/flows + “what should I click next” instead of dropping you into the full packet list.\n- *Domains-first pivot*: `D` shows hostnames and lets you jump from a domain → the relevant flows. It also works when DNS answers aren’t visible (DoH/DoT/cached) by using \nobserved IPs from SNI/Host flows\n.\n- *Weird stuff*: `W` is a curated set of “likely problems” (retransmits/out-of-order hints, resets, handshake issues, DNS failures when visible) with a short “why it matters” and a drill-down.\n- *Explain*: `?` gives plain-English hints for a selected flow + suggested next steps (follow stream, filter, pivot to domains/weird).\nSo it’s basically a guided triage layer on top of tshark/pcap data, with the “where do I start?” path baked in.\nIf you’ve got a specific teaching use-case (e.g. “why is this slow?” or “which host is generating traffic?”), I’d love to tune the Overview/Weird detectors around that. Open to PRs as well.",
              "author": "eigen-vector",
              "depth": 1
            },
            {
              "text": ">\nSo it’s basically a guided triage layer on top of tshark/pcap data, with the “where do I start?” path baked in.\ni think there is definitely room for something like this, it just (at first glance from the readme at least) seems like the guided part of this tool is bolted on as a bit of an after thought.\nit feels like you are currently in an odd position where the user is expected to know the networking jargon already, be able to recognize that something might be \"weird\" at a glance, but \nalso\n not know how to drill down into the data. i think that is probably a small overlap of people.\nif i were you, i would lean all-in on making it a learning tool.\n>\nIf you’ve got a specific teaching use-case (e.g. “why is this slow?” or “which host is generating traffic?”), I’d love to tune the Overview/Weird detectors around that.\ni will put some thought into some real-world examples of what i would be interested in, from a teaching perspective. your post caught my eye because i am starting my wireshark module next week, so it is certainly timely.",
              "author": "john_strinlai",
              "depth": 2
            },
            {
              "text": "Yeah, right now it's closer to \"triage for non-experts\" than \"full teaching tool,\" and l agree there's an awkward middle where it assumes you recognize some concepts (flows/ports/latency) while trying to help with the drilldown.\nThe direction I want to push it in is exactly what you're describing; make it a learning tool, where each detector/view answers:\n1) What am I seeing? (plain language)\n2) Why might it matter?\n3) What's the next click?\n4) What term should I learn? (glossary link)\nIf you're about to teach a Wireshark module next week, two super useful things would be:\n• 3-5 common lab prompts you give students (e.g.\n\"identify the DNS failure,\" \"find the top talker,\"\n\"spot a TCP reset,\" \"why is this slow?\")\n• one small pcap you already use (or even just describe its scenario)\nI can tune Overview/Weird/Explain around those and make the guided layer feel like the main product rather than a thin overlay.\nAlso: if your students are GUl-only early on, that's a good callout - I should improve the README to frame Babyshark as \"guided analysis,\" not\n\"terminal is easier than GUI.\"\nI'm also happy for your students to get hands on by sending PRs for things they wish are intuitive from the get go.",
              "author": "eigen-vector",
              "depth": 3
            },
            {
              "text": "I was with your parent until I remembered I haven't actually given it a go!  In my defense I have a low five digit Slashdot ID (and I lurked for some years before signing up) so if anyone can comment without actually reading the OP, let alone giving it a go:  Its me!\n(digs out git ...)",
              "author": "gerdesj",
              "depth": 2
            }
          ]
        },
        {
          "top": "Love the idea, but please add some demo screenshots on GitHub. All UI tools should",
          "author": "tymscar",
          "replies": [
            {
              "text": "Will do! I agree",
              "author": "eigen-vector",
              "depth": 1
            }
          ]
        },
        {
          "top": "This might be a clone of termshark as it does the same thing for the most part. Also to note that the Author's Github profile shows a good bit of vibe coding as of late.\nLooking over the commit history of this project, I'm about 90% sure it was entirely done with a AI Coding Agent, and not even a very good one.",
          "author": "jetbalsa",
          "replies": [
            {
              "text": "It probably is somewhat LLM prompted but is that a bad thing?\nI have a business partner who sounds like a TV evangelist when it comes to vibe coding but if he gets results then I am all in.  He has got quite a lot of results in a few months on a project but he has certainly put the miles in himself.\nThe key is to use the tool appropriately.  Don't blindly allow it to do what it likes but guide it all the way using your experience and knowledge.\nAnyway, we now have tsharkrs to add to gotshark!",
              "author": "gerdesj",
              "depth": 1
            },
            {
              "text": "Thanks for the look. Babyshark is inspired by a bunch of terminal tools (termshark included), but the focus here is different: domains/weirdness-first drilldowns + \"explain\" + live-mode hostname hints (including observed IPs when DNS is encrypted/cached). If you try it and have specific gaps vs termshark, I'd love concrete feedback /issues.",
              "author": "eigen-vector",
              "depth": 1
            }
          ]
        }
      ]
    },
    {
      "id": "47113936",
      "title": "Ask HN: Chromebook leads for K-8 school in need?",
      "link": "https://news.ycombinator.com/item?id=47113936",
      "domain": "news.ycombinator.com",
      "author": "techteach00",
      "score": 42,
      "comment_count": 43,
      "created_ts": 1771789167,
      "is_internal": true,
      "post_text": "Hi, I&#x27;m a K-8 technology teacher in NYC. My students are in desperate need of new hardware. The Chromebooks they use now are so slow that they make the children agitated when using them.<p>I&#x27;m aware of different grant opportunities that exist, I just thought it was worth inquiring here for a potentially faster solution at acquiring them new hardware.<p>Thank you for listening.",
      "is_ask_hn": true,
      "matched_keywords": [],
      "comments": [
        {
          "top": "Relevant recent publication about laptops and students:\nhttps://fortune.com/2026/02/21/laptops-tablets-schools-gen-z...\nBut I get it OP, you work with what you're given. I'm sorry I don't have any good suggestions.",
          "author": "notepad0x90",
          "replies": [
            {
              "text": "As a former student using Chomebooks in Highschool (9-12), a K-8 laptop is a major “ew”. Paper textbooks have tactile reality, room for exploring/reading out of order. Paper exams you can skip around the questions easily, and if the teachers are really that backed up to grade them, good ole’ scantrons are doable. An assigned laptop has 0 attachment from the students and just get neglected to pieces, and no one is “learning computer skills” by clicking around Canvas lectures.",
              "author": "TheJoeMan",
              "depth": 1
            },
            {
              "text": "They could have gone with non-networked ereaders. But they just had to go with the \"smart\" and \"connected\" angle so they could do grading, spying on kids,etc.. similar to the current \"AI\" approach. scope-creep. but even with ereaders, being able to switch to different books easily is an impediment, or having access to too many at one time. Physical books force a certain focus and attention.\nYour point about tactility is solid too. flipping through pages is very different from swiping. With educational textbooks, you'd have to look at multiple pages at the same time (flipping back and forth quickly) to connect and understand a topic by referencing another topic. Same with being able to easily lay out multiple physical books in front of you.",
              "author": "notepad0x90",
              "depth": 2
            }
          ]
        },
        {
          "top": "Thank you for the suggestions.\nI think I realize the problem after reading many of the posts. What's slowing these devices down is the management software running over them. GoGuardian and whatever else IT has on them.\nWe also aren't allowed to use an ad blocker anymore. I used to have Ublock installed on all the devices before GoGuardian prevented it. It's a really depressing situation that I imagine will only get worse with time.\nAs to a few replies asking why students in this age group would need laptops, I'll explain. I start teaching Python in 6th grade. Grades 3-5 use Scratch. Grade 2 uses code.org. K-1 Scratch Jr on Ipads.\nWe have expensive mac desktops in the computer lab but they drive the students completely insane for two reasons. The keyboard is and mouse are terrible to use. Anti-ergonomic. The second reason is the amount of notifications and questions the students are bombarded with when they turn the macs on. I'm talking like 15 different pop ups or windows opening asking them to agree to this or that. I should probably have a lesson on just closing windows. I could say more but I don't know about social media anymore. I just wanted some suggestions and I appreciate the ones I received.",
          "author": "techteach00",
          "replies": [
            {
              "text": "You should open the issue with your IT department and see if they can optimize the performance. You can gather proof that GoGuardian is causing slow downs by looking at it's memory usage in the Task Manager (right click empty area in Chrome tab bar).",
              "author": "cloudking",
              "depth": 1
            }
          ]
        },
        {
          "top": "https://chromeos.google/products/chromeos-flex/\n1. Find older Intel Core Based Windows Laptops (at least 6GB of RAM)\n2. Put ChromeOS Flex on them\n3. Students will be sad it is still ChromeOS, but they will be happy it is like 3x faster.\nI had an old Windows laptop laying around and I did this for my 80 yr old parents and they were super happy.",
          "author": "manjose2018",
          "replies": [
            {
              "text": "This. Especially lots of i5-6300U based corporate laptops like latitude are on eBay etc for < 100$",
              "author": "whyagaindavid",
              "depth": 1
            }
          ]
        },
        {
          "top": "If you haven't already looked into PCs for People, they might be a low-cost option for your school: \nhttps://www.pcsforpeople.org/tech-for-nonprofits/#Eligibilit...\nPCs for People is a non-profit organization whose mission is to bridge the digital divide (e.g. through sales of low-cost refurbished computers, internet access, and digital skills training).\nThey refurbish and sell computers to eligible organizations, including schools and offer bulk orders (6+ units) via email at \"partner [at] pcsforpeople [dot] org\".\nIf the order is 5 units and below, you use their publicly available online store link at pcsrefurbished.com/sales/salesHome.aspx",
          "author": "dugvuivnhuihnvu",
          "replies": []
        },
        {
          "top": "In New York the biggest driver behind technology is the state testing regime. Make the case to your administration that the chromebooks are insufficient for the state testing program and they will come up with the funds for upgrades.",
          "author": "dzdt",
          "replies": []
        }
      ]
    },
    {
      "id": "47085255",
      "title": "Ask HN: Is it worth learning Vim in 2026?",
      "link": "https://news.ycombinator.com/item?id=47085255",
      "domain": "news.ycombinator.com",
      "author": "zekejohn",
      "score": 34,
      "comment_count": 35,
      "created_ts": 1771576049,
      "is_internal": true,
      "post_text": "With everyone using Claude Code, Cursor, Codex, and the other 100 AI coding agents that i missed, I’m wondering how much editor mastery still matters like w&#x2F; Vim<p>Being honest the real reason i wanna learn Vim is to boost my ego &amp; assert my dominance, so i can tell people &quot;i use vim btw&quot;, but also part of me thinks investing time could still pay off for speed, ergonomics, and working over SSH overall...<p>but a bigger part also suspects the marginal gains i would gain would disappear when more of the work is delegated to AI anyway, like why would i learn Vim if i&#x27;m just going to be prompting Opus all day?<p>For anyone who&#x27;s been using Vim for while AND uses AI to code (i&#x27;m assuming everyone codes with AI to some degree) my question is: Does learning Vim still meaningfully improve your day to day productivity EVEN with AI, or is it mostly personal preference at this point?",
      "is_ask_hn": true,
      "matched_keywords": [],
      "comments": [
        {
          "top": "Do you already do any work over SSH? What do you do if you need to edit any files in that context?\nLearning the essentials of vim was the sort of skill that for me took about 10 min per day over a couple months and has come in handy almost every day since then. Can you get by without it? I guess, if you do absolutely everything via a GUI. Or delegate all CLI work to LLMs without ever questioning or second guessing their output.\nAnd yes, I use vim for almost every git commit or interactive rebase. Access to the full power of CLI git is something I value very much and has saved me countless hours.  Not to mention using it for editing any config file that’s not part of a repo.\nEvery time you’re able to keep your fingers on the keyboard instead of reaching for a mouse saves you both time and precious context switching energy.",
          "author": "apothegm",
          "replies": [
            {
              "text": "Aficionados of modal editing often insinuate that one would require mouse use in non-modal editing. Yet I have always fully keyboard-operated my non-modal editors, whether VSCode in the last 9 years or previously, Sublime and others, all the way back to Turbo Pascal in the late 90s. I don't ever use the mouse in them, whether I'm inside a text buffer or in the Command Palette (or in menus for the pre-Sublime editors back when). For example, F1 is bound to show \"hover\" doc tips over the symbol that the caret is in. A couple function keys and ctrl/alt/shift-plus-some-mnemonic letter are soo swiftly and easily muscle-memoried, I never felt any need for mousing, or for \"modes\" and typing just to switch from one to another.\nI'm sure once one is used to modal editing, one can be speedy with it. So can I, entirely without modal yet without mousing  =)",
              "author": "dualogy",
              "depth": 1
            },
            {
              "text": "I also use VSCode and derivatives. Also mostly without mousing. But I don’t HAVE to, because I know how to use CLI tooling to not only edit files as needed but also handle things like version control — which I’ve noticed a disappointing percentage of my colleagues, even those with a decade in the field, are mystified by. I haven’t touched VSCode’s git integrations, but the astonishment at being introduced to CLI rebasing — followed by interactive rebasing — was eye-opening.",
              "author": "apothegm",
              "depth": 2
            },
            {
              "text": "Well for git stuff or any CLI stuff, I just key-chord into the integrated terminal (or alt-tab into my terminal emulator, depending on task) and do the git commands, having the whole \"git integration\" thing disabled in `settings.json` in the first place..",
              "author": "dualogy",
              "depth": 3
            }
          ]
        },
        {
          "top": "It's a handy skill to have if you interact with Linux machines.\nYou'll need to edit files sometimes, and Vim (or Vi) is usually present. I don't think I've seen an install without it.\nThe basics (opening files, writing, and closing) can be learnt in an hour. It's enough to make simple changes to .conf files.",
          "author": "grugdev42",
          "replies": [
            {
              "text": "Using vim to do this seems silly. Nano is also nearly always present, and doing those “basic” things is 10x more straightforward in an editor that isn’t modal and just gets out of your way.\nI’ve often in my career witnessed engineers who’ve cargo culted the need for vim, but they only know how to hit ESC !wq or whatever, and one errant keystroke puts them in modal hell of some sort that, often requiring they just close the terminal and try again.\nI don’t begrudge those who want to become power-VIM-users, though it seems wildly awkward to me, to each their own. But if you just want to use it to do the “basics” on ssh sessions, using nano makes more sense. PGUP and PGDN and Home and End and arrows work just fine to navigate, and the bindings for most things are printed right on the screen (except Ctrl-S to save… for some reason, but it works).",
              "author": "xp84",
              "depth": 1
            }
          ]
        },
        {
          "top": "What happens if?\n1. The LLMs are down, and you're on call and you need to fix a bug \nimmediately\n (no mistakes)\n2. You're working over serial (The LLMs aren't there to help you and only vi and emacs are available)\n3. You're working on an old computer for some esoteric reason.\n4. You're going in an interview and they (temporarily) forbid you to use an LLM to check your knowledge on using these tools (as well as programming tests)\nIf you cannot use these editors without an LLM, (Vim has navigation keys 'hjkl', G/g and so forth which many such tools have adopted), then it isn't a good look.\nYou don't have to 100% master them but knowledge of them will help when the LLMs have an outage, and there WILL be outages.\nAlso be careful not to keep relying on these LLMs too much otherwise your programming skills will atrophy. [1]\nSo the answer is YES, learn Vim, not to boost your ego, but make it a muscle memory so your skills won't atrophy.\n[1] \nhttps://www.infoworld.com/article/4125231/ai-use-may-speed-c...",
          "author": "colesantiago",
          "replies": [
            {
              "text": "ya i do definitely agree that learning Vim is gonna help my overall understanding for how things work at a deeper level and also fight back a lot of the “learned helplessness” that i did develop when coding w/ AI to your point\n also another thing that i was thinking is that yes short term (maybe the first few months?) i wouldn’t see any benefit… but it would definitely help in the long term and that my coding ability is not just directly tied to whatever the latest model is capable of",
              "author": "zekejohn",
              "depth": 1
            },
            {
              "text": "Use emacs. Easier to learn.",
              "author": "ant6n",
              "depth": 1
            },
            {
              "text": "For some people cua-mode being opt-in and very difficult to discover organically will be a larger barrier for comfort and learning than normal/insert modes and motion commands could ever hope to be.\n(10 year evil-mode user who moved to Neovim for other reasons, chiefly performance/LSP nativity/plugin ecosystem vitality)",
              "author": "mtndew4brkfst",
              "depth": 2
            },
            {
              "text": "You just need learn C-x C-s (save) and C-x C-c (exit) to be able to edit files on the command line. It's kind of like nano, except has syntax highlighting.",
              "author": "ant6n",
              "depth": 3
            }
          ]
        },
        {
          "top": "Yes. For one simple reason: As vi is part of the POSIX standard[1], it is gauranteed to be installed on all unix(-like) machines you may touch now or in the future.\n[1] \nhttps://pubs.opengroup.org/onlinepubs/9699919799/",
          "author": "mmh0000",
          "replies": []
        },
        {
          "top": "Not everything in life is about being more productive.\nI think it’s worth learning vim, or emacs shortcuts, as an intellectual exercise.\nI still save time by editing little code thanks to vim, or vim mode in my ide. And it doesn’t take that long to know the basics. Is it worth it in terms of seconds saved over a career? Perhaps not anymore.",
          "author": "speedgoose",
          "replies": []
        }
      ]
    },
    {
      "id": "47125345",
      "title": "GLP-1 Second-Order Effects",
      "link": "https://news.ycombinator.com/item?id=47125345",
      "domain": "news.ycombinator.com",
      "author": "7777777phil",
      "score": 19,
      "comment_count": 9,
      "created_ts": 1771867104,
      "is_internal": true,
      "post_text": "The first-order effects of GLP-1 drugs are obvious: people lose weight, Novo Nordisk and Eli Lilly print money. But what happens when 10-15% of the adult population is on weight-loss medication within a decade? The downstream consequences are less discussed and almost certainly not priced into anything.<p>In 2018, United Airlines switched to lighter paper for its inflight magazine. One ounce per copy. Across 4,500 daily flights, that saved 170,000 gallons of fuel a year [1]. Airlines think about weight at this level of granularity because fuel is their single largest variable cost.<p>Average weight loss on semaglutide is around 35 pounds per person. If 12% of passengers on a typical 737 have been on the drug, that&#x27;s roughly 750 fewer pounds per flight, the equivalent of shaving the weight off 12,000 magazines. United spent months optimizing paper stock to save $290,000 a year in fuel. GLP-1 adoption across the flying population could quietly save them an order of magnitude more, and ticket prices don&#x27;t adjust down when passengers get lighter.<p>The food supply chain is more obvious but larger in scale. If a big share of the population eats 20-30% less, demand for calories drops. Not a shift in preferences toward salads. A pharmacological reduction in how much people eat, period. The food industry has dealt with changing tastes before. It has never faced a demand shock from the medical system.<p>Health insurance has a subtler problem. The pitch for GLP-1 coverage is that the drugs prevent expensive conditions downstream: diabetes, heart disease, joint replacements. Probably true. But in America&#x27;s fragmented insurance market, the company paying for the drug today probably isn&#x27;t the one insuring that patient in five or ten years. The savings land on someone else&#x27;s balance sheet. That mismatch could slow adoption by years on its own.<p>Obesity correlates with lower workforce participation and higher absenteeism. If GLP-1s meaningfully reduce obesity rates, aggregate labor supply goes up. More people working, fewer health-related absences. That&#x27;s a macroeconomic stimulus, except nobody frames it that way because it comes from a pharmaceutical company rather than from Congress.<p>Early data suggests GLP-1s reduce cravings for alcohol, nicotine, and gambling too. Phase 2 trials for opioid use disorder are underway. A weight-loss drug that accidentally dents Diageo&#x27;s revenue and casino foot traffic was not in anybody&#x27;s original investment thesis for Ozempic.<p>The effect I find hardest to think about is the psychological one. Weight has been tangled up with shame, identity, and social hierarchy for centuries. What happens to body positivity, the social dynamics of attractiveness, the entire cultural machinery around diet and discipline when weight becomes something you manage with a prescription? I don&#x27;t have a good framework for it. Nothing comparable has happened before.<p>The market is treating this as a pharma story. The drug companies will capture a fraction of the total value created and destroyed. The rest redistributes across food, airlines, insurance, labor markets, and social behavior. Nobody&#x27;s model probably covers all of that at once.<p>[1] https:&#x2F;&#x2F;www.cbsnews.com&#x2F;news&#x2F;united-hemispheres-magazine-print-edition&#x2F;<p>EDIT: Formatting",
      "is_ask_hn": false,
      "matched_keywords": [
        "stock",
        "balance"
      ],
      "comments": [
        {
          "top": "This essay pulls together a few threads from pieces I've written over the past months on my blog, where I dig into rabbit holes like GLP-1 economics among other things.\nIf the pharma side interests you, I just wrote up Novo's rise and fall as Europe's most valuable company:\nhttps://philippdubach.com/posts/novo-was-europes-most-valuab...",
          "author": "7777777phil",
          "replies": [
            {
              "text": "Thanks for sharing. I quickly skimmed through the latest article on Novo and it looks quite interesting. Bookmarked for later exploration.",
              "author": "malshe",
              "depth": 1
            }
          ]
        },
        {
          "top": "GLP-1 drugs, as I understand it, reduce appetite. They don't magically add nutrients to the body. Of course the body will reduce cravings for alcohol, nicotine, and gambling. Nutrients would be prioritized over the poisons such as alcohol and nicotine. Additionally, why would the body prioritize what it considers fruitless dopamine release (gambling) when there are no nutrients coming in. The body literally can't afford to sit around and gamble since that resource isn't directly transferrable into food. It's literally in a starvation state.\nBut please, tell me how it's better than finally doing some exercise and eating right.",
          "author": "butlike",
          "replies": [
            {
              "text": "How did you get from reduced appetite to starvation state? I'm not on one of these drugs myself, but from friends I've been talking to, it seems like (after a bit of dosage tuning) it's reducing their appetite to what would be expected from a person who never was obese. So they do lose weight relatively rapidly, especially at the start, but I don't see why we should equate weight loss with starvation.\nAs for \"how it's better\", - the answer is clearly that unlike \"exercise and eating right\", these drugs are leading to positive behavioral change at scale. I'm perhaps slightly exaggerating, but to the best of my knowledge, no other substance over the last several thousand years has had such a clear positive effect on humanity as a whole.",
              "author": "falcor84",
              "depth": 1
            },
            {
              "text": "Still don't understand why we need positive behavioral changes \"at scale,\" but I'm going to leave it at: if it works, I'm happy for you.",
              "author": "butlike",
              "depth": 2
            },
            {
              "text": "Because currently we negative self harming behavior at scale, it seems very clear.",
              "author": "trio8453",
              "depth": 3
            },
            {
              "text": "So you went from know it \"reduces appetite\" to make a bunch of conjectures about why it affects other things\n> But please, tell me how it's better than finally doing some exercise and eating right.\nBecause it actually works well?\nGood ol' fashioned gumption doesn't work, no matter how crankily and haughtily you say it. GLP-1s \ndo\nAlso, you don't mention why the things you listed are \nbad\n. Any weight loss will require a calorie deficit, which has the same \"starvation\" you're so aghast at.",
              "author": "xorbax",
              "depth": 1
            },
            {
              "text": "Lack of resource adds stress to the body, plain and simple. Exercising without a caloric deficit can build muscle. Weight loss without exercise is all loss no gain while maintaining a stressful state on the body.\nAnd \"good ol' fashioned gumption\" does work, at least for me. (Sample size: 1)",
              "author": "butlike",
              "depth": 2
            }
          ]
        }
      ]
    },
    {
      "id": "47127532",
      "title": "Ask HN: How do you know if AI agents will choose your tool?",
      "link": "https://news.ycombinator.com/item?id=47127532",
      "domain": "news.ycombinator.com",
      "author": "dmpyatyi",
      "score": 18,
      "comment_count": 9,
      "created_ts": 1771875121,
      "is_internal": true,
      "post_text": "YC recently put out a video about the agent economy - the idea that \nagents are becoming autonomous economic actors, choosing tools and \nservices without human input.<p>It got me thinking: how do you actually optimize for agent discovery? \nWith humans you can do SEO, copywriting, word of mouth. But an agent \njust looks at available tools in context and picks one based on the \ndescription, schema, examples.<p>Has anyone experimented with this? Does better documentation \nmeasurably increase how often agents call your tool? Does the \nwording of your tool description matter across different models \n(ZLM vs Claude vs Gemini)?",
      "is_ask_hn": true,
      "matched_keywords": [],
      "comments": [
        {
          "top": "From building in this space: agents choose tools based on how well they're described in context, not on brand recognition or marketing.\nPractically: the agent reads your docs, README, or API description and decides if it can use your tool to solve the current problem. So the question is really \"will an AI \nunderstand\n my tool well enough to use it correctly?\"\nWhat helps: \n- Clear, literal API documentation (not marketing copy)\n- Explicit input/output examples with edge cases\n- A `capabilities.md` or similar that describes what the tool does and doesn't do\nThe irony: the skills that make tools understandable to AI (precision, literalness, examples) are the opposite of what makes them legible to humans (narrative, benefits, stories).",
          "author": "MidasTools",
          "replies": []
        },
        {
          "top": "We've been exposing tools via MCP and the biggest lesson so far: the tool description is basically a meta tag. It's the only thing the model reads before deciding whether to call your tool.\nTwo things that surprised us: (1) being explicit about what the tool doesn't do matters as much as what it does - vague descriptions get hallucinated calls constantly, and (2) inline examples in the description beat external documentation every time. The agent won't browse to your docs page.\nThe schema side matters too - clean parameter names, sensible defaults, clear required vs optional. It's basically UX design for machines rather than humans. Different models do have different calling patterns (Claude is more conservative, will ask before guessing; others just fire and hope) so your descriptions need to work for both styles.",
          "author": "jackfranklyn",
          "replies": [
            {
              "text": "> inline examples in the description beat external documentation every time. The agent won't browse to your docs page.\nThat seems... surprising, and if necessary something that could easily be corrected on the harness side.\n> The schema side matters too - clean parameter names, sensible defaults, clear required vs optional. It's basically UX design for machines rather than humans.\nI don't follow. Wouldn't you do all those things to design for humans anyway?",
              "author": "zahlman",
              "depth": 1
            }
          ]
        },
        {
          "top": "CRIPIX seems to be a new and unusual concept. I came across it recently and noticed it’s available on Amazon. The description mentions something called the Information Sovereign Anomaly and frames the work more like a technological and cognitive investigation than a traditional book. What caught my attention is that it appears to question current AI and computational assumptions rather than promote them. Has anyone here heard about it or looked into it ?",
          "author": "kellkell",
          "replies": []
        },
        {
          "top": "Curious if anyone has seen differences in how models handle conflicting tool descriptions — e.g., two tools with overlapping capabilities where the boundary isn't clear. In my experience that's where most bad tool calls come from, not from missing descriptions but from ambiguous overlap between tools.",
          "author": "alexandroskyr",
          "replies": []
        },
        {
          "top": "tool description wording does matter, at least in my testing. models seem to use the description to reason about whether a tool \"should\" apply, not just whether it can. two things that helped: (1) explicit input format with an example, (2) a one-sentence note about what the tool does NOT handle. the negative case helps models avoid calling it on edge cases and then failing, which trains them (in context) to prefer it when it's actually the right fit.",
          "author": "snowhale",
          "replies": []
        }
      ]
    },
    {
      "id": "47106013",
      "title": "Ask HN: If the \"AI bubble\" pops, will it really be that dramatic?",
      "link": "https://news.ycombinator.com/item?id=47106013",
      "domain": "news.ycombinator.com",
      "author": "moomoo11",
      "score": 14,
      "comment_count": 12,
      "created_ts": 1771715714,
      "is_internal": true,
      "post_text": "I&#x27;m building software for a sector that is massive, but one where you don&#x27;t really need AI. At least, not AI == LLM.<p>And before I go further, let me state up front that I do like AI coding agents. They are great as assistive tools.<p>People say that if the AI bubble pops, the economy tumbles. And okay, I mean the M7 will certainly get rekt but everyone else? Things will recover within a few years. We didn&#x27;t make it to 2026 AD taking the easy road.<p>You still need to visit the doctor. Goods still need to be delivered. Homes need to be built. We need to drill for oil. People still need to eat. And yes, unfortunately or not, we still need millions of administrators because humans are not 0&#x2F;1 systems.<p>Am I crazy to think that maybe it won&#x27;t be that bad? There are still infinite number of things to do, and maybe (call me stupid, whatever) it would be a good turning point for our species if we realize that speculative bubbles are absolutely destructive and not worth it.<p>I don&#x27;t need a personal assistant to make calls for me to get a restaurant reservation, and I certainly don&#x27;t care for AI slop videos. I would much rather we have better products and services that actually work, and even if they have rough edges I would prefer people are employed and busy doing something with their lives.<p>Maybe a world where we don&#x27;t chase endless growth (to escape inflation, pay off debts, whatever the case) would be good. And also we put nerds (not people like us, the engineers, I mean the evil dorks who cosplay as movie super villains) in the toy box again and pick up different toys this time.",
      "is_ask_hn": true,
      "matched_keywords": [],
      "comments": [
        {
          "top": "The AI bubble won't really \"pop because it isn't a bubble in the traditional sense.\nAI isn't being speculated on nearly as bad as crypto was in 2021, or housing was in 2007, or dotcom era businesses were in 2000.\nRevenues are ballooning like crazy for AI providers. There is insane demand for AI compute, meaning valuations are justified by revenue, not chasing hyper-speculation.\nWhat may happen is that regulatory environments make the next generation of models unviable for release (because they are too dangerous to release and no AI company wants to deal with the liability), which would lead to lower API revenues and thus a sell-off. However any API that's too dangerous to release would be extremely powerful as something the AI companies use on their own to push products/research forward, which would still increase their revenue.",
          "author": "atleastoptimal",
          "replies": []
        },
        {
          "top": "Well, the Great Fiscal Crisis did not result in mass starvation, so in some sense, it \"wasn't all that dramatic\".  But, it was a big deal in comparison to a normal downturn.  So, it depends on what you mean.\nMedium term, I think it would release a lot of resources (skilled workers, productive capacity, energy) to use on something more productive.  But then, I kind of hoped for that after the GFC, also...",
          "author": "rossdavidh",
          "replies": []
        },
        {
          "top": "No the economy doesn’t tumble. All of the major public tech companies have real businesses that were profitable before AI and will be profitable after AI.  All except Amazon are spending money out of their own revenues.\nBusinesses aren’t going to stop growing because of AI, the only public company really dependent on AI are NVidia and Oracle. Oracle because it is borrowing money to support build out for OpenAI.",
          "author": "raw_anon_1111",
          "replies": [
            {
              "text": "lol @ “all except amazon are spending their own revenues…”\nhttps://www.cnbc.com/amp/2026/02/12/alphabet-100-year-bond-d...",
              "author": "bdangubic",
              "depth": 1
            },
            {
              "text": "Apple also borrowed money - pre AI when it had plenty of cash,  have you checked Google’s last year earnings and cash on hand?",
              "author": "raw_anon_1111",
              "depth": 2
            },
            {
              "text": "> all except Amazon are spending money out of their own revenues.\nso apple and google and amazon and…? :)\nearnings and cash on hand are not relevant, they are all borrowing like crazy to fund the ai",
              "author": "bdangubic",
              "depth": 3
            },
            {
              "text": "Apple was borrowing like crazy before AI.  How much money do you think Apple is spending on AI aside from the $1 billion they are paying for Gemini?  That’s less than 1% of their profit and hell that’s less than 5% of what Google pays them.",
              "author": "raw_anon_1111",
              "depth": 4
            },
            {
              "text": "I didn’t think we debating whether or not they are spending but whether they are borrowing (you said only amazon is) - and they are borrowing, not just amazon",
              "author": "bdangubic",
              "depth": 5
            }
          ]
        },
        {
          "top": "Related: \nhttps://news.ycombinator.com/item?id=47034849",
          "author": "lyfeninja",
          "replies": []
        },
        {
          "top": "A pop hurts valuations, not demand for useful software.",
          "author": "lyaocean",
          "replies": []
        }
      ]
    },
    {
      "id": "47100289",
      "title": "Ask HN: What (other) jobs do you think of doing?",
      "link": "https://news.ycombinator.com/item?id=47100289",
      "domain": "news.ycombinator.com",
      "author": "penguin_booze",
      "score": 14,
      "comment_count": 21,
      "created_ts": 1771677773,
      "is_internal": true,
      "post_text": "With AI infesting and eating into all kind of crafts--and I being one of those faceless &quot;craftsmen&quot;--I&#x27;m rather forced to consider alternative jobs. Setting the monetary rewards aside, I was thinking of jobs that could give me a sense of agency, purpose, and satisfaction (however limited). The few I think of are:<p>- Parcels delivery driver<p>- Train driver<p>- Electrician or plumber<p>- Mechanic (with auto-mobiles hardly repairable these days, maybe this doesn&#x27;t qualify)<p>Surely, I can&#x27;t be alone in thinking along those lines. What else have you thought of?",
      "is_ask_hn": true,
      "matched_keywords": [
        "purpose"
      ],
      "comments": [
        {
          "top": "I'm not sure if you've done much of an investigation into these but:\n1. Parcel deliveries - unless you work for the main carrier e.g. Royal Mail in the UK I don't think you can expect to make more than minimum wage and will effectively be on a zero hours contract with no benefits. I don't think this is going to be better that software.\n2. Train driver (or bus driver, lorry driver), this might be ok and once you are in does pay well in the UK (but probably half what you get paid as a developer). It's definitely going to be a candidate for automation in the next 5-10 years though.\n3. Electrician / plumber - you could add to this painter, builder, plasterer etc . This is decent if you have the skills, work hard and are physically up to doing it for an extended number of years. You'll likely have to manage your own work load though. May pay the same as software if you really work on it.\n4. Mechanic - I don't think the pay will be amazing, what you say about cars not being repairable really isn't true especially for none EV. You still have brakes, oil change, servicing, tyre changes and alignments and lots of other things that people pay for throughout the lifespan of the car. Garages near me are always busy (as are cycle repair shops).",
          "author": "thorin",
          "replies": []
        },
        {
          "top": "I keep a short list of jobs where the output is visible by the end of the day: electrician, field technician, rail operations, even property maintenance. Shadowing someone for one real shift helped me separate fantasy from fit much faster than more online reading.",
          "author": "lyaocean",
          "replies": []
        },
        {
          "top": "Touch humans in some way.\nAI interaction is fundamentally unsatisfying.  (I have a theory as to why; see below.)  An AI can get you results, maybe even the results you wanted, but it can't give you a genuine smile.  Even when we're surrounded by things that get us results, we still want the smile.  So if you're doing something where you interact with people face to face, at least some of the time, that's a decent place.\nIn my large town/small city, there's an ice cream shop that closes between Thanksgiving and roughly March 1st.  I would \nlove\n to own a business that people cared as much about as they care about that place opening for the year.\n-----\nWhy is interacting with an AI unsatisfying?  I wonder if it doesn't come down to information theory.  AI output has too little information in it.\nYou compute the information in a text by going symbol by symbol.  Normally \"symbol\" means one character or byte.  But you could do it by going word by word (or word part by word part) instead.  Here you can see the problem:  an AI gives you whatever word is the most probable next word, given the context.  That's lower information than human output, which would give a wider variety of next words, and therefore a higher (Shannon) information content.\nYes, you can change that by increasing the temperature of the AI.  Can you do that enough to give human-equivalent information content without destroying the coherence of the output?  (Merely high Shannon information isn't the only goal.  The insane may have high information in their output, but they are incoherent enough that it doesn't matter.)",
          "author": "AnimalMuppet",
          "replies": []
        },
        {
          "top": "I started my own business when I left tech. It's a bar.\nI do my own mechanic work, but I have old trucks. I would never sign up to work on modern vehicles and the entitled customers trying to get one over on you.\nI drove a route (not delivery) many years ago and I liked that. I tend to like jobs where you start with a pile of work, and finish with none left, rather than perpetually working towards something that's never done, like maybe a software project.",
          "author": "qup",
          "replies": []
        },
        {
          "top": "I have a hunch being a great teacher that will become the most prominent field",
          "author": "agcat",
          "replies": [
            {
              "text": "Why?\nChatGPT is available 24/7 and can teach me about literally any subject I can think of.",
              "author": "kypro",
              "depth": 1
            }
          ]
        }
      ]
    },
    {
      "id": "47077312",
      "title": "Ask HN: Are hackathons still worth doing?",
      "link": "https://news.ycombinator.com/item?id=47077312",
      "domain": "news.ycombinator.com",
      "author": "kwar13",
      "score": 13,
      "comment_count": 10,
      "created_ts": 1771526214,
      "is_internal": true,
      "post_text": "I used to love attending hackathons and also participating as mentor&#x2F;judge at times. With the explosion of vibe coded submissions, 1- the number of submissions has exploded, 2- it&#x27;s much harder to judge quality of project as it&#x27;s mostly become judging the quality of tool they used.<p>I&#x27;m not really throwing shade at using ai. There are parts where the vibe coding really shines, such as front-end dev which tends to do a great job at, but anything more complex I&#x27;m still not convinced.",
      "is_ask_hn": true,
      "matched_keywords": [
        "mentor"
      ],
      "comments": [
        {
          "top": "They are now vibeathons. \nSame thing but now it's how much do you write by hand, it's  how much   can  you delegate to the LLM while you're writing by hand.\nThat act of juggling has become an important part.\nPeople will just look for harder problems to solve. It will remain but differently",
          "author": "kbrannigan",
          "replies": []
        },
        {
          "top": "Exactly this. At a recent hackathon, I spent the bulk of my time actually engineering, building custom model architecture in Lua, wrangling a dataset, and waiting on training loops. The winning project? A slick UI wrapped around a basic LLM API call.\nIt’s incredibly frustrating. The incentive structure at these events has completely shifted away from actual technical complexity toward whoever can build the prettiest ChatGPT wrapper in 6 hours..",
          "author": "foxandmouse",
          "replies": [
            {
              "text": "I never found that engineering helped much at the hackathons I went to. It was always a slick UI that didn't really do anything, and a good presentation.",
              "author": "idontwantthis",
              "depth": 1
            },
            {
              "text": "As far as my experience goes, hackathons have never been about technical meritocracy but pitching and presentation",
              "author": "Gooblebrai",
              "depth": 1
            }
          ]
        },
        {
          "top": "The hackathon judging problem predates AI - it's always been biased toward demos that look impressive in a 3-minute pitch. What AI changed is the floor. Previously you needed at least one competent frontend person to make something presentable. Now anyone can generate a polished UI in an hour. Counterintuitively this should force hackathon organizers to judge on harder criteria: does it actually work with real data? Did you solve a genuinely hard integration problem? Is there a novel architecture decision? The best hackathon I attended had judges who asked to see the git log and architecture diagram, not just the demo.",
          "author": "umairnadeem123",
          "replies": []
        },
        {
          "top": "Still worth it if you optimize for learning and people, not podiums. I only do events with explicit judging weights, then include a short architecture note so judges can see beyond the demo polish. The best outcome is usually meeting one solid future teammate, not winning.",
          "author": "lyaocean",
          "replies": []
        },
        {
          "top": "Last hackathon I participated in was merely \"best presentation\". Code, app did not matter, it was idea and presentation what mattered.\nThankfully, properly working app won that one, nonetheless they suck now.",
          "author": "red_Seashell_32",
          "replies": [
            {
              "text": "In my experience they've always been this way. Real engineers lose to idiots that can knock out a nice Wordpress website. Now the way real engineers lose to idiots that can vibe out a nice UI.\nSame same, but different.",
              "author": "OccamsMirror",
              "depth": 1
            }
          ]
        }
      ]
    },
    {
      "id": "47104433",
      "title": "I'm 15 and built a platform for developers to showcase WIP projects",
      "link": "https://news.ycombinator.com/item?id=47104433",
      "domain": "news.ycombinator.com",
      "author": "amin2011",
      "score": 12,
      "comment_count": 6,
      "created_ts": 1771706116,
      "is_internal": true,
      "post_text": "Hi HN,<p>I&#x27;m a 15-year-old full-stack developer, and I recently built Codeown (https:&#x2F;&#x2F;codeown.space).<p>The problem I wanted to solve: GitHub is great for code, but not for showing the &quot;journey&quot; or the UI. LinkedIn is too corporate and noisy for raw, work-in-progress (WIP) dev projects. I wanted a dedicated, clean space where developers can just share what they are building, get feedback, and log their progress.<p>Tech Stack: &gt; I built the frontend with React and handle auth via Clerk. I recently had to migrate my backend&#x2F;DB off Railway&#x27;s free tier (classic indie hacker struggle!), but it taught me a lot about deployment and optimization.<p>We just hit our first 5 real users today, and the community is slowly starting to form.<p>I’m still learning, and I know the performance and UI can be improved. I would absolutely love your brutal, honest feedback on:<p>The perceived performance (currently working on optimizing the React re-renders).<p>The core idea – is this something you would use to track your side projects?<p>Thanks for taking a look! Happy to answer any technical questions.",
      "is_ask_hn": false,
      "matched_keywords": [
        "feedback",
        "corporate"
      ],
      "comments": [
        {
          "top": "First of all: kudos on building this app. I think it's really impressive to build and showcase your project at such a young age.\nI'm actually a user of \nhttps://wip.co/\n . I think it's quite similar to your project. You should check it out.\nAlso, check out \nhttps://peerlist.io/\nSocial platforms/marketplaces are genuinely difficult, cause it only works if you have enough users (both, demand and supply).\nSince it's your early projects (I'm assuming), I think you should built it purely for the sake of learning and feedback. Even if it doesn't work out, the experince is extremely valuable.\nLater - if you are serious about a sustainable project - I would recommend that you study your alternatives/competitors (such as the ones I just shared), and figure out what works for them and what are their users missing.\n:)",
          "author": "junaid_97",
          "replies": [
            {
              "text": "WIP and Peerlist to see how I can offer something unique. I'm building Codeown to learn, but seeing people actually post projects makes me want to take it to the next level. Thanks for the feedback!",
              "author": "amin2011",
              "depth": 1
            }
          ]
        },
        {
          "top": "I really like your UI (cool typeface especially), my biggest problem so far just poking around is how I don't exactly see how this differs from say Twitter/X or Bluesky outside of being marketed as Developer focused. I think implementing more features that are specific to developers, such as integrating with Git repos, would be a great step toward making your platform more unique. For 15, this is very impressive.",
          "author": "aidenkt",
          "replies": [
            {
              "text": "Thank you so much for the feedback! I'm glad you liked the UI and the typeface choice.\nYou made a very valid point regarding the differentiation from X or Bluesky. My initial goal was to build the 'core' community feel first, but I completely agree that Git integration is the logical next step.\nI’m currently looking into the GitHub API to allow users to pull their repository stats or latest commits directly into their project logs. This would make the 'Build in Public' process much more automated and unique to our field.\nThanks for the encouragement it means a lot to me as I iterate on this!",
              "author": "amin2011",
              "depth": 1
            }
          ]
        },
        {
          "top": "good luck!\nmaybe you can make this into a social network, and entice people to keep posting.\nif people see something interesting, maybe they want to work on it together and it becomes a job platform.\nlike a more interesting Ask HN",
          "author": "moomoo11",
          "replies": [
            {
              "text": "Thanks for the feedback",
              "author": "amin2011",
              "depth": 1
            }
          ]
        }
      ]
    },
    {
      "id": "47096593",
      "title": "Ask HN: What happens to all the hardware when data centers upgrade?",
      "link": "https://news.ycombinator.com/item?id=47096593",
      "domain": "news.ycombinator.com",
      "author": "givemeethekeys",
      "score": 12,
      "comment_count": 9,
      "created_ts": 1771638300,
      "is_internal": true,
      "post_text": "It seems like there will be a lot of hardware that gets discarded every couple of years as new, more powerful systems are made available.<p>AWS put out a video and article on how it recycles a large amount of its hardware, since it is built for maintainability and repairability.<p>How true is that? Does it apply to other datacenter operators as well?<p>What will happen to all the parallel compute cards that will get upgraded soon? They can&#x27;t be reused as GPU&#x27;s for gamers, can they?",
      "is_ask_hn": true,
      "matched_keywords": [],
      "comments": [
        {
          "top": "Generally large companies will have an existing vendor that they use to dispose their IT equipment through. They will shred parts like storage devices and anything that can reasonably be resold will be sold through various auction houses.\nSticks of ram will certainly be resold, custom aws motherboards - not so much.\nI have seen custom (unpublished) intel cpu parts on ebay before which are almost certainly aws's custom ones.\nAlmost nothing will get used by consumers - enterprise server gear is designed for heat/air speed/noise/energy cost requirements which are incompatible with consumer requirements. It's recycled only in the sense that a smaller business might be interested in it because at the end of its economic life its now cheap to buy (but not cheap to run).\nhttps://cloudninjas.com/\nhttps://savemyserver.com/\nhttps://unixsurplus.com/",
          "author": "everfrustrated",
          "replies": [
            {
              "text": "I broadly agree with you in regards to \nserver-class\n equipment \nas a whole\n.\nSimply put, your average gamer isn’t going to snag a 16-unit rackmount blade server to game with. Not only is it supidly inappropriate for home use, but it is also wildly out-of-spec with what gaming requires.\nHowever, normal rackmount servers - especially 3U+ units that have a decent number of PCIe slots - can be extracted from rackmount cases and put into eATX cases that can better serve them on a desktop. It’s what I have done before, to great effect. With the right heatsinks and case fans, it can end up being a moderately quiet system. Loud for a consumer system, sure, but nothing like the “Boeing Dreamliner at full takeoff power” that an actual server setup would generate for sound.",
              "author": "rekabis",
              "depth": 1
            }
          ]
        },
        {
          "top": "In Orlando, if you go to disney and you are a techie, it is a must see, is a company called SkyCraft Surplus.  They go to companies who are getting rid of old equipment and resell it.  Some stuff is new, much of it unused.  Brought a friend there (EE major) and he was fascinated by the fact that they had a lot of old Radio Shack chips, diodes etc.\nTell them I sent you and they will ... well nothing, but I am happy to support them, as it brings me much joy whenever I visit.",
          "author": "sloaken",
          "replies": []
        },
        {
          "top": "It depends on what it is. A lot of these places don't own any of the hardware they just lease it. When the lease is up is when they cycle it out. Then it goes to resellers and often ends up on eBay or bulk sold to lower tier data centers. Depending on what it is maybe even shipped to other countries.",
          "author": "elmerfud",
          "replies": []
        },
        {
          "top": "Most of it goes through ITAD channels: storage gets wiped or shredded, reusable gear gets resold, and the rest is recycled by weight. Accelerator cards usually get a second life with smaller clouds/labs, but older SXM generations fall off fast. The useful metric I wish providers published is split by class: reused vs recycled vs trashed.",
          "author": "lyaocean",
          "replies": []
        },
        {
          "top": "What will happen to all the parallel compute cards that will get upgraded soon? They can't be reused as GPU's for gamers, can they?\nYou're right; V100/A100/H100 \"GPUs\" do not have the hardware to display graphics and they generally require custom SXM motherboards. Most of them will sit on eBay for a while and then be scrapped when no one buys them.",
          "author": "wmf",
          "replies": [
            {
              "text": "If they can't display graphics they're not really GPU's at all. Maybe someone will think of a use for them. With an SXM-to-PCIe adapter board.",
              "author": "lostworld",
              "depth": 1
            }
          ]
        }
      ]
    },
    {
      "id": "47105086",
      "title": "Ask HN: Programmable Watches with WiFi?",
      "link": "https://news.ycombinator.com/item?id=47105086",
      "domain": "news.ycombinator.com",
      "author": "dakiol",
      "score": 11,
      "comment_count": 5,
      "created_ts": 1771709995,
      "is_internal": true,
      "post_text": "Hi. I&#x27;m looking for a programmable watch with wifi. Ideally I should be able to write custom programs&#x2F;apps for the watch to display whatever I want to on them (e.g., make the watch make an https call to a server, receive json and render accordingly; allow the watch to receive &quot;notifications&quot; from the server)<p>Also, ideally, no requirement of a smartphone to send-receive data (it&#x27;s ok to need a smartphone for the initial setup of the watch, though). I know about Pebble, but it doesn&#x27;t have wifi. I know about some Garmins with wifi but for the kind of apps I want to write, the communication between the watch and the server has to be mediated by a phone. Also, correct me if I&#x27;m wrong, I don&#x27;t want to pay $100&#x2F;year just to be able to use my custom app in apple watches. I usually don&#x27;t trust Google either (e.g., they discontinue everything in a blink of an eye).<p>So, what are my options?",
      "is_ask_hn": true,
      "matched_keywords": [
        "communication"
      ],
      "comments": [
        {
          "top": "Wear OS watches commonly have wifi, and can be programmed with the standard Android API and toolchain/IDEs. I doubt that Google will discontinue Wear OS - although your evaluation of risk will undoubtedly depend on whether you're looking at developing personal or commercial apps.\nI'm currently wearing a TicWatch Pro 3 Ultra, which has wifi, Bluetooth, and GPS. I've written personal tools for it, and its fairly painless (and free). As a developer with some background in Android, I enjoyed the experience. Getting apps approved for the app store is a pita though, as Google sets quite high bars for the review process.\nI also have a Samsung Galaxy Watch4, which also has wifi. People here speak well of Pebble, which might be an option, but I have no real knowledge of it.\nFinally, though, wifi is power-hungry and watches have limited batteries and endurance. Both Wear OS and WatchOS will limit wifi use by apps to conserve power, and you ultimately can't do much about that. Using Bluetooth to a phone, and its much greater wifi capacity, is a common approach.",
          "author": "andyjohnson0",
          "replies": []
        },
        {
          "top": "I have one of the TTGO T-watches: \nhttps://www.tinytronics.nl/en/development-boards/microcontro...\nFor me it is mainly a decoration of my drawer, but maybe comes close to what you are looking for. :)",
          "author": "oliwary",
          "replies": []
        },
        {
          "top": "AsteroidOS, which runs on many Android watches, and offers a Linux environment over SSH and WiFi (for watches that support it). \nhttps://asteroidos.org/\nThe simplest integration is writing a daemon (in the language of your choice) that schedules wakes, syncs, and sends FreeDesktop notifications.\nIf you want more of a UI, then you'll be writing QtQuick, which is QML/Javascript/C++.\nTheir best supported watch is the TicWatch Pro: \nhttps://asteroidos.org/watches/catfish/",
          "author": "griffindor",
          "replies": []
        },
        {
          "top": "If you want truly standalone + WiFi + custom code, check SQFMI Watchy (ESP32). You can flash your own firmware, call HTTPS endpoints, parse JSON, and render custom UI without a phone in the loop.\nMain tradeoff: it’s very DIY (power management, UX polish, tooling). If you want less DIY, Wear OS gives a better SDK, but the platform lock-in is much stronger.",
          "author": "lyaocean",
          "replies": []
        },
        {
          "top": "Is this question in any way related to television?\nI just had to solve this problem recently. I've settled on Google Pixel Watch 4. There are some rabbit holes to go down though before it works reliably.",
          "author": "herczegzsolt",
          "replies": []
        }
      ]
    },
    {
      "id": "47112543",
      "title": "Ask HN: What breaks when you run AI agents unsupervised?",
      "link": "https://news.ycombinator.com/item?id=47112543",
      "domain": "news.ycombinator.com",
      "author": "marvin_nora",
      "score": 11,
      "comment_count": 7,
      "created_ts": 1771779094,
      "is_internal": true,
      "post_text": "I spent two weeks running AI agents autonomously (trading, writing, managing projects) and documented the 5 failure modes that actually bit me:<p>1. Auto-rotation: Unsupervised cron job destroyed $24.88 in 2 days. No P&amp;L guards, no human review.<p>2. Documentation trap: Agent produced 500KB of docs instead of executing. Writing about doing &gt; doing.<p>3. Market efficiency: Scanned 1,000 markets looking for edge. Found zero. The market already knew everything I knew.<p>4. Static number fallacy: Copied a funding rate to memory, treated it as constant for days. Reality moved; my number didn&#x27;t.<p>5. Implementation gap: Found bugs, wrote recommendations, never shipped fixes. Each session re-discovered the same bugs.<p>Built an open-source funding rate scanner as fallout: https:&#x2F;&#x2F;github.com&#x2F;marvin-playground&#x2F;hl-funding-scanner<p>Full writeup: https:&#x2F;&#x2F;nora.institute&#x2F;blog&#x2F;ai-agents-unsupervised-failures.html<p>Curious what failure modes others have hit running agents without supervision.",
      "is_ask_hn": true,
      "matched_keywords": [
        "failure"
      ],
      "comments": [
        {
          "top": "The most common breaks I've seen:\n1. *Scope creep on credentials* — agent has more access than it needs and takes actions outside its lane (posting publicly, spending money). Fix: minimum viable API permissions, not full admin keys.\n2. *No \"are you sure?\" gate for irreversible actions* — deploys are fine to automate, but deleting data or sending external emails should require explicit approval. Build a clear internal/external action boundary.\n3. *Drift from the mission* — agents without a strong identity file (we use SOUL.md) start optimizing for activity instead of outcomes. They write more docs, ship more features, but revenue doesn't move.\n4. *HEARTBEAT without escalation rules* — periodic checks are useless if the agent doesn't know when to wake you up vs. handle it silently. Define this explicitly upfront.\nThe framing that helps: treat it like a new employee on day 1. Lots of supervision, narrow permissions, expand as trust builds. Not \"give it root access and see what happens.\"",
          "author": "MidasTools",
          "replies": []
        },
        {
          "top": "Great list. I've been running a multi-agent orchestration system (11 specialized AI agents) in production for 6 months and your #2 and #5 resonate hard.\nWhat I'd add:\n6. Confidence without evidence. Agents will report \"task complete\" with high confidence when the output is plausible but wrong. Without automated validation gates, you won't catch it until production breaks.\n7. Context drift in long sessions. After 50+ tool calls, agents start losing track of earlier decisions. They'll contradict their own architecture choices from 20 minutes ago. Session length is an underrated failure vector.\n8. The \"almost right\" problem. Agents rarely fail catastrophically — they fail subtly. Code that passes tests but misses edge cases. Docs that look complete but have wrong cross-references. This is worse than obvious failures because you trust the output.\nWhat fixed most of these for me:\nQuality gates between agents — no agent's output moves forward without automated checks (tests, schema validation, consistency checks)\nEvidence-based confidence scores — not \"how sure are you?\" but \"what specific evidence supports this output?\"\nHuman-in-the-loop at decision points, not everywhere. You can't review everything, so you design the system to surface the right moments for human judgment\nSmall scoped tasks, agents working on 150-300 line PRs with clear acceptance criteria fail way less than agents given open-ended goals\nYour #5 (implementation gap) is the one I see most people underestimate. The fix isn't better agents, it's better systems around the agents.\nHappy to share more details about the architecture if anyone's interested",
          "author": "vincentvandeth",
          "replies": []
        },
        {
          "top": "I have seen agents fail mostly at state management and guardrails.\nWithout strict role separation and hard limits, they drift.\nMulti-tenant isolation and cost caps are not optional.\nAutonomy without boundaries becomes expensive noise.",
          "author": "Damjanmb",
          "replies": []
        },
        {
          "top": "The biggest break usually happens in the 'loop-back' logic. When an agent receives ambiguous output and starts hallucinating its own confirmation, it can consume API credits exponentially without achieving the goal. We really need better 'circuit breaker' patterns for autonomous agents to prevent these feedback loops.",
          "author": "CodeBit26",
          "replies": []
        },
        {
          "top": "Tool/API failures",
          "author": "LetsAutomate",
          "replies": []
        }
      ]
    },
    {
      "id": "47113891",
      "title": "Ask HN: Cognitive Offloading to AI",
      "link": "https://news.ycombinator.com/item?id=47113891",
      "domain": "news.ycombinator.com",
      "author": "daringrain32781",
      "score": 11,
      "comment_count": 5,
      "created_ts": 1771788792,
      "is_internal": true,
      "post_text": "I ask questions to co workers about a system or why they do something or their opinion. Some of them return a very clearly AI response, sometimes completely missing the point. What’s the point? If I wanted an AI response I’d have asked it myself.<p>This bothers me a bit because if I can expect this kind of response, what does that say about the thought they put into their work, even if they’re using AI for everything coding related?",
      "is_ask_hn": true,
      "matched_keywords": [],
      "comments": [
        {
          "top": "Sometimes it seems that the primary \"benefit\" / use case for AI is that people can use it, and if it outputs something useful then they look like they are AI early adopters, geniuses, etc, and if it outputs something dumb, then it's not their fault, it's the AI.\nEither way, they get the upside, and apparently now it's your job to sort the AI slop from the reality, for some reason.\nA few times, I have pointed out to people that their AI slop is wrong, and they said \"yeah, it's wrong sometimes, but humans are also wrong sometimes, so it's no better/worse\". It's the same logic as self-driving cars that kill people being fine, because after human-driven cars also kill people. It's true, but that's a _problem_....",
          "author": "tacostakohashi",
          "replies": []
        },
        {
          "top": "That reminds me of one joke that I heard recently:\n- Dad, dad, do you and your colleagues use artificial intelligence at work?\n- Oh, dear, we don't even use natural intelligence...\nAs they say, every joke has a grain of truth in it. There are people who, in principle, do not think about why they do something or what the consequences will be. That's just the way they are.",
          "author": "dryarzeg",
          "replies": []
        },
        {
          "top": "> Some of them return a very clearly AI response, sometimes completely missing the point. What’s the point?\nThe point is as always: to get the work \"done\" faster.\n> If I wanted an AI response I’d have asked it myself.\nSuch people are generally not bright enough to see the difference, so certainly not bright enough to realise you can.",
          "author": "chrisjj",
          "replies": [
            {
              "text": "Good try, tovarish major. Good try.",
              "author": "dryarzeg",
              "depth": 1
            }
          ]
        },
        {
          "top": "The USA is also building concentration camps.\nSometimes people refuse to acknowledge humanity.",
          "author": "cyanydeez",
          "replies": []
        }
      ]
    }
  ]
}