{
  "scraped_date": "2026-02-28",
  "source": "hacker_news",
  "total_scraped": 135,
  "nontech_count": 35,
  "posts": [
    {
      "id": "47172119",
      "title": "Layoffs at Block",
      "link": "https://twitter.com/jack/status/2027129697092731343",
      "domain": "twitter.com",
      "author": "mlex",
      "score": 883,
      "comment_count": 1029,
      "created_ts": 1772140676,
      "is_internal": false,
      "post_text": "<a href=\"https:&#x2F;&#x2F;www.cnbc.com&#x2F;2026&#x2F;02&#x2F;26&#x2F;block-laying-off-about-4000-employees-nearly-half-of-its-workforce.html\" rel=\"nofollow\">https:&#x2F;&#x2F;www.cnbc.com&#x2F;2026&#x2F;02&#x2F;26&#x2F;block-laying-off-about-4000-...</a><p><a href=\"https:&#x2F;&#x2F;www.marketwatch.com&#x2F;story&#x2F;block-plans-to-lay-off-nearly-half-its-staff-in-deliberate-and-bold-embrace-of-ai-81e99247\" rel=\"nofollow\">https:&#x2F;&#x2F;www.marketwatch.com&#x2F;story&#x2F;block-plans-to-lay-off-nea...</a>",
      "is_ask_hn": false,
      "matched_keywords": [
        "layoffs"
      ],
      "comments": [
        {
          "top": "We'll see how much the AI aspect is true by whether they're thinning out teams equally, or just axing whole initiatives. My impression of Block was that it was mostly a one-trick pony (okay, two if you include CashApp) with a bunch of side initiatives that never seemed to pan out, so I'm expecting it to be more of the latter, with this being more of an admission that they're now in \"maintenance mode\".\nEither way, I think this is how it's gonna be. Regardless of whether AI significantly increases productivity (40%? come on), layoffs will be preemptory. Executives will see the lack of productivity boost as being due to lack of pressure, and imagine engineers are just using the AI to make their own lives easier rather than to work more efficiently. You can't really double output velocity because your users will see it as too much churn, so the only choice is to lay off half the workforce and double the workload for those who stay. \"Necessity is the mother of invention.\" They'll overlook the fact that the work AI tools provide only encompasses 10% of your job even if they're 100% efficient.",
          "author": "daxfohl",
          "replies": [
            {
              "text": "I'm convinced that these \"AI Layoffs\" are these companies trying to save face from the absurd overhiring that they did in 2022 and 2023 because apparently they thought that these no-interest loans/free money would just last forever.\nNo one really \"knows\" how to grow businesses so the easiest way to spend a lot of money quickly is hiring lots of people, whether or not they are \"necessary\". Then this free money dries up, interest rates go back up, and now they're stuck with all these employees that they didn't actually need.\nSome companies like Google and Microsoft just accepted that assholes like me will call their CEOs incompetent and fired lots of people in 2023, but I think other CEOs were kind of embarrassed and held off.  Now they can use AI as a scapegoat and people won't act like they were idiots for hiring twice as many people as they needed.\nAlso, I got declined by Block a year ago.  Glad I was now.",
              "author": "tombert",
              "depth": 1
            },
            {
              "text": "Having been through a couple of layoffs and merges, as I approach mid-century, the MBA powered managers are always to blame, because the stupid way to manage every year has to be x% exponential increase over the previous year, always forgetting that it is physically impossible when everyone goes for the same goal.\nAnd then when targets aren't met, it is the employees that get shown the door while management gets their bonus.\nThe companies that are happy getting what they need to keep the lights on, seldom go through such layoff rounds.\nAh but the shareholders can sue the CEO, well this seems to be an US approach to how companies are run.",
              "author": "pjmlp",
              "depth": 2
            },
            {
              "text": "> \nthe stupid way to manage every year has to be x% exponential increase over the previous year, always forgetting that it is physically impossible when everyone goes for the same goal.\nThat's why we have this corporate ritual, which we carry out each year, or even each quarter - a solemn ceremony, where we divide everyone into two groups: the \ncost centers\n and the \nprofit centers\n.\nEveryone works in harmony for the same organizational goals, but the people of cost centers also bear an additional, sacred duty, the highest of callings: to give up their employment and prospects for the future, to have their due credit be taken by the people of profit centers and poured onto the altar of the all-powerful Board. It's through this sacrifice of the many, that the symmetry is broken, allowing the year-by-year metrics to continue growing, against all wisdom and the laws of thermodynamics.",
              "author": "TeMPOraL",
              "depth": 3
            },
            {
              "text": "> we divide everyone into two groups: the cost centers and the profit centers.\nThis would make some sort of sense if it was honest.  For a start, the owners and shareholders are plainly cost centers (they are literally useless sinks of revenue).",
              "author": "enriquto",
              "depth": 4
            },
            {
              "text": "Unfortunately this insane perspective is common. I’ve literally been told by a past employer that the revenue that I, personally, was bringing in to the company (by going way above and beyond) was greatly appreciated, but that they were unhappy about the cost of paying me a small fraction of this revenue as a pre-agreed performance bonus.",
              "author": "taneq",
              "depth": 4
            }
          ]
        },
        {
          "top": "Straight from the horses mouth:\n>> yes we over-hired during covid because i incorrectly built 2 separate company structures (square & cash app) rather than 1, which we corrected mid 2024.  but this misses all the complexity we took on through lending, banking, and BNPL. and that we’re now targeting $2M+ gross profit per person, 4x our pre-covid efficiency, which stayed flat at ~$500k from 2019 until 2024. we have and do run an efficient company... better than most.\nhttps://xcancel.com/jack/status/2027290756793135253\nI.E OVER-HIRED",
          "author": "dzonga",
          "replies": [
            {
              "text": "Block job listings were all over for a couple years. I thought it was odd that they were hiring so much given what we know about their business lines. I assumed there were some internal projects or new business lines.\nI feel sorry for anyone who joined recently and then got laid off because they company wasn't planning properly.",
              "author": "Aurornis",
              "depth": 1
            },
            {
              "text": "> that we’re now targeting $2M+ gross profit per person\nThat seems really high. Do they have such moat that nobody can move into their space?",
              "author": "_fizz_buzz_",
              "depth": 1
            },
            {
              "text": "Payment processing is one of the most tightly regulated, and some might say corrupt, industries. Replicating the tech wouldn't be difficult. The social and regulatory part is effectively impossible for anyone who's not already in the inner circle.",
              "author": "mjr00",
              "depth": 2
            },
            {
              "text": "Is it hard to break into?  Theres tons of tech companies that are playing in that space.  Stripe, Braintree, Toast, Adyen, etc.",
              "author": "adrr",
              "depth": 3
            },
            {
              "text": "Ironically Square isn’t good at anything in particular. Square fails to have a good point of sale (Toast wins) and fails for online store fronts (Shopify wins). Square is in a position where they spread themselves thin and aren’t at the top of anything in particular.",
              "author": "melozo",
              "depth": 2
            }
          ]
        },
        {
          "top": "This is one of the best (if not the best) layoff letters I've seen online (no affiliation, don't know anyone working there, purely outsider perspective).\n* Severance packages upfront because realistically that's what everyone worries about first.\n* Reasoning second. I appreciate the one clean cut vs prolonged bleeding.\n* Owning the decision and respecting the people that got you there. Opting for an awkward allhands vs breakup-via-text-message.\n* Giving people a chance to say goodbye.\nNot gonna go into strategic analysis of this, or Jack's leadership style in general.\nBut realistically, you can't pen a better (or, well, less bad) layoff announcement.",
          "author": "senko",
          "replies": [
            {
              "text": "It should be good. It's the third time he's written this exact same announcement, including \"taking the blame\" and \"making the difficult choice to cut a large group instead of smaller cuts over time\" and \"thanking the expendables that got the company where it is.\"",
              "author": "shaftway",
              "depth": 1
            },
            {
              "text": "Ye at this point the dude is just copy pasting. Gotta go do the cocaine parties with JayZ",
              "author": "Bayko",
              "depth": 2
            },
            {
              "text": "> I appreciate the one clean cut vs prolonged bleeding.\nThat's a false dichotomy, you could reduce headcount via attrition which is better in some ways.\nThere's also no reasoning on product impact. Is the strategy to cut products that aren't making money? Is the strategy to cut 40% across everyone because everyone can go faster?\n> Owning the decision\nDoes it? It came across to me as an inevitability of AI, not \"we over-hired\". Layoffs are always a mis-management issue, because the opposite (hiring) is a management issue. If management failed to see where the market was going and now needs a different workforce, that's still a management issue.\n> respecting the people that got you there\nThere's words, and there's money, and on these it's pretty good. But there's also an empathy with the experience they're about to go through and I'm not sure there's much of that here beyond the words. To do this well you'd need to think through what folks are about to go through and look for ways you can positively impact that beyond actions today. I've seen some companies do this better, helping teams get re-hired elsewhere, splitting off businesses to sell to other companies, incubating startups, there are lots of options. Hard, especially at this scale, but possible.\n> But realistically, you can't pen a better (or, well, less bad) layoff announcement.\nAnd this is the crux of my point, I really think you can. This was a good one, one of the better I've seen, but it's still within the realm of SV companies laying people off. In some companies, countries, industries, this would look very different, and better.",
              "author": "danpalmer",
              "depth": 1
            },
            {
              "text": "You cannot attrite 40% of the company in 5 months, without creating an incredibly toxic environment. Dorsey knows this; ultimately he lost Twitter over his inability to right size it. I would bet dollars to donuts he promised himself he wouldn't do it again - under no circumstances is 40% cut over six months preferable to a clean fast cut.",
              "author": "vessenes",
              "depth": 2
            },
            {
              "text": "Voluntary severance/ERO is possible without a bad culture. I’ve seen it done, people love it.",
              "author": "ezfe",
              "depth": 3
            }
          ]
        },
        {
          "top": "Nice severance; but in this job market, holy shit.\nYeah, you get 5 months of severance and a bunch of devices and such; but, does this CEO really think these employees will find new work in that time? In this job market?\nIf the profits are still up and growing, why on earth would you evict 40% of the company, to send them into \nthis\n job market? Why not … try new industries, play around, try to become the next Mitsubishi or Samsung or General Electric. If you’ve got the manpower and talent, why not play with it and see if anything makes money. In-house startups with stable capital, all that.\nThis seems … wrong.",
          "author": "t-writescode",
          "replies": [
            {
              "text": "> Nice severance; but in this job market, holy shit.\nI just talked to a bunch of recruiters (we're hiring) and their main piece of advice was: The market is crazy. \nMove fast\n. We're seeing people getting jobs within days of starting to look, bailing on offers after signing because they got a better offer somewhere else, etc. 24 hours is the longest you can leave a candidate waiting. You have been warned\nedit: I am in SFBA. Your reality may be different. People have spilled some 2 trillion dollars onto the area in the past 2 years. A lot of that is going to software engineers as everyone tries to shove AI down consumers' throats. Rents are up 60% in 12 months, which is not the sign of a cold employment market :)",
              "author": "Swizec",
              "depth": 1
            },
            {
              "text": "I'm in NYC which I think has similar demographics to SF in this regard; I found my job in August of last year, after about five months of searching, and I found it because a friend of mine referred me. It's a good job, and I like it, I'm grateful for that friend.\nRegardless, it's not like that was the only job I applied to. I had a policy of applying to at least ten jobs a day, so I applied to about ~1500 jobs, and literally \nall\n of them rejected me except for the one I have right now. I had about twenty other interviews (edit: 15, checked my calendar from last year), a few that got to late stages, and they didn't pan out [1].\nI psychotically save money so I wasn't worried in any kind of existential sense, I could survive for years if I needed, but man I would have \nkilled\n to be in a situation where I even had the opportunity to bail on an offer.\nThis has been the worst economy for software engineers I've seen in my ~15 year career.  I am slightly optimistic that it will improve \neventually\n but I suspect \"eventually\" might mean several more years.\n[1] And one at a one of the world's largest bank (that my lawyer/mom has advised me not to name publicly) where my interviewers were potentially the most incompetent people I have ever talked to and who didn't seem to know what an atomic was in Java, and \"corrected\" my counter code with a mutex.  And I put \"corrected\" in quotes, because what they corrected it to would deadlock.  Morons.",
              "author": "tombert",
              "depth": 2
            },
            {
              "text": "The trick to applying these days is to have a contact on the inside that can tell you what the hiring manager is \nactually\n looking for, vs what's in the public JD, and then to refer you.\nThat doesn't scale to 10 jobs/day for very long because almost nobody has a network that big.  If you don't land something through referral to the hiring manager, it's mostly a crap shoot these days.",
              "author": "eitally",
              "depth": 3
            },
            {
              "text": "Yeah, it wasn't lost on me that a lot of job listings are fake, even at the time (I think I even wrote comments saying as much on this very forum). I did reach out to contacts asking if they had openings but of course that's a finite set and most of them told me they didn't.\nWhat I had tried to do was start randomly requesting connection invites with people on LinkedIn in fields I was interested, and a surprisingly large number of them accepted my requests, but that didn't actually pan out to anything productive, which is a little depressing.\nI hope I don't get laid off from my current gig, because now I really don't have a clue on the best way to find a job. Maybe I could try that \"CEO at a big corporation\" thing, it doesn't look too hard and apparently you can lay off half your staff and still be rewarded.",
              "author": "tombert",
              "depth": 4
            },
            {
              "text": "Blink once if the bank rhymed with face",
              "author": "vedaba",
              "depth": 3
            }
          ]
        },
        {
          "top": "Square/Block stock peaked at $273 in Feb 2021 and is currently at $54. Taking away the Covid bubble the stock has been completely flat since 2018, almost 8 years, while the S&P 500 returned nearly 200% in that same period. So I'm not buying the whole \"the company is doing great! The layoff is just because of AI.\"",
          "author": "paxys",
          "replies": [
            {
              "text": "We know they're not...\n> In its fiscal fourth quarter, Block reported revenue of almost $6.3bn, in line with Wall Street expectations. Its earnings tumbled to 19 cents a share, owing to a $234mn hit — or 38 cents a share — on its bitcoin holdings.",
              "author": "QGQBGdeZREunxLe",
              "depth": 1
            },
            {
              "text": "Yea, look over there!!! AI.\n(Don’t mention the bitcoin investment that’s in the shitter)",
              "author": "dmboyd",
              "depth": 2
            },
            {
              "text": "The bitcoin investment had 2/3/4x'd at that point - so the negative 75% means the business is the shitter.",
              "author": "blitzar",
              "depth": 3
            },
            {
              "text": "If AI makes people twice as productive and you're a mature software company chasing marginal value, if the marginal value people added was worth it before AI, surely more marginal value at the same cost is better. It's also the wrong side of the AI curve to be on. Maybe AI will replace dev jobs, but why not wait to find out? And if it does, prompt-coding is a different skill than traditional coding, and it's not what you interviewed people for, so you don't even have a guess as to if you're keeping the right people.",
              "author": "dehrmann",
              "depth": 1
            },
            {
              "text": "> It's already the wrong side of the AI curve to be on. Maybe AI will replace dev jobs, but why not wait to find out?\nHave you considered their internal use has already proven AI's capability to them?",
              "author": "squidbeak",
              "depth": 2
            }
          ]
        }
      ]
    },
    {
      "id": "47163885",
      "title": "Tell HN: YC companies scrape GitHub activity, send spam emails to users",
      "link": "https://news.ycombinator.com/item?id=47163885",
      "domain": "news.ycombinator.com",
      "author": "miki123211",
      "score": 668,
      "comment_count": 254,
      "created_ts": 1772098508,
      "is_internal": true,
      "post_text": "Hi HN,<p>I recently noticed that an YC company (Run ANywhere, W26) sent me the following email:<p>From: Aditya &lt;aditya@buildrunanywhere.org&gt;<p>Subject: Mikołaj, think you&#x27;d like this<p>[snip]<p>Hi Mikołaj,<p>I found your GitHub and thought you might like what we&#x27;re building.<p>[snip]<p>I have also received a deluge of similar emails from another AI company, Voice.AI (doesn&#x27;t seem to be YC affiliated). These emails indicate that those companies scrape people&#x27;s Github activity, and if they notice users contributing to repos in their field of business, send marketing emails to those users without receiving their consent. My guess is that they use commit metadata for this purpose. This includes recipients under the GDPR (AKA me).<p>I&#x27;ve sent complaints to both organizations, no response so far.<p>I have just contacted both Github and YC Ethics on this issue, I&#x27;ll update here if I get a response.",
      "is_ask_hn": false,
      "matched_keywords": [
        "purpose"
      ],
      "comments": [
        {
          "top": "Martin from GitHub here.  This type of behaviour is explicitly against the GitHub terms of service, when we catch the accounts doing this we can (and do) take action against those accounts including banning the accounts. It's a game of whack-a-mole for sure, and it's not just start-ups that take part in this sketchy behaviour to be honest. I've been plenty of examples in my time across the board.\nThe fundamental nature of Git makes this pretty easy for folks to scrape data from open source repositories.  It's against our terms of service and those folks might want to talk with some lawyers about doing it - but as every Git commit contains your name and email address in the commit data it's not technically difficult even if it is unethical.\nFrom the early days we've added features to help users anonymise their email addresses for commits posted to GitHub.  Basically, you configure your local Git client to use your 'no-reply' email address in commits and that still links back to your GitHub account when you push: \nhttps://docs.github.com/en/account-and-profile/reference/ema...\nI think that's still probably the best route. We want to keep open source data as open as possible, so I don't think locking down API's etc is the right route. We do throttle API requests and scraping traffic, but then again there have been plenty of posts here over the years from people annoyed at hitting those limits so it's definitely a balancing act. Love to know what folks here think though.",
          "author": "martinwoodward",
          "replies": [
            {
              "text": "> when we catch the accounts doing this we can (and do) take action against those accounts including banning the accounts.\nThis isn't my experience. I requested that you looked into a spammer in July 2025, you ignored my reply and the account is still active.\n----\nThank you so much for the report. We're sorry to hear you're receiving unwanted emails, but it's always a possibility when your public contact information is listed on the web. You can keep your email address private if you wish by following the steps here:\nSetting your commit email address\nWe do expect our users to comply with our Terms of Service, which prohibits transmitting using information from the GitHub (whether scraped, collected through our API, or obtained otherwise) for spamming purposes. I'm happy to look into it further to see if we can contact the reported user and let them know that this type of activity is not allowed.\nPlease let us know if you have any other questions or concerns.\n----\nMy reply which was ignored:\n----\nI understand it will happen from time to time. I'd rather be contactable (I've received legitimate emails today because my email is on my profile).\nPlease take further action. My email is public with the expectation that the ToS will be enforced. If GitHub isn't discouraging spammers then it makes it much harder to justify being contactable.\nAll the best,\nDavid",
              "author": "david_allison",
              "depth": 1
            },
            {
              "text": "I reported spammers ~5 times to GH, and every time the account went down in a couple of hours. Obviously mileage may vary, but I don't want the whole HN to think this process is completely broken.\nPlease keep reporting spammers, usually it works.",
              "author": "gettingoverit",
              "depth": 2
            },
            {
              "text": "To confirm: is this email spammers, or spam on GitHub?\nI've had decent success with on-GitHub action (I'd wager ~80% action taken), but the effort to report email spammers doesn't seem worthwhile.",
              "author": "david_allison",
              "depth": 3
            },
            {
              "text": ">> it's always a possibility when your public contact information is listed on the web\nSounds correct to me\n> Please take further action. My email is public with the expectation that the ToS will be enforced.\nWhat magic wand are you expecting they wave that distinguishes people who need your email address for legitimate from those who need it for illicit purposes? Why wouldn't we apply the same to the entire population and lock up criminals before they've committed crimes?\nWhat you're asking is entirely impossible short of mandatory mind reading",
              "author": "Aachen",
              "depth": 2
            },
            {
              "text": "I provided a spam email chain from a user with a linked GitHub profile, stating that they obtained my email from my GitHub profile.\nGP [martinwoodward] states:\n> This type of behaviour is explicitly against the GitHub terms of service, when we catch the accounts doing this we can (and do) take action against those accounts including banning the accounts.\nBut action was not taken, there was no reply to my email to GitHub support.",
              "author": "david_allison",
              "depth": 3
            }
          ]
        },
        {
          "top": "YC is a proud investor in Flock, what YC Ethics thing are you talking about?",
          "author": "scottydelta",
          "replies": [
            {
              "text": "And that Optifye.ai demo with the sweatshop surveillance software",
              "author": "otherayden",
              "depth": 1
            },
            {
              "text": "And Cluely",
              "author": "cassonmars",
              "depth": 1
            },
            {
              "text": "Cluely is not YC.",
              "author": "tasn",
              "depth": 2
            },
            {
              "text": "he might be thinking of chadIDE \"the first brainrot ide\"",
              "author": "fantasizr",
              "depth": 3
            },
            {
              "text": "the same Cluely that's on IG? I thought that was a fictional satire.",
              "author": "insane_dreamer",
              "depth": 2
            }
          ]
        },
        {
          "top": "I've spent a lot of my career marketing to developers, and spamming their GitHub account might be top 1 or 2 worst marketing tactics you can use.\nCold emailing rarely works by itself. Cold emailing \ndevelopers\n via emails you pulled from their GitHub accounts? At that point, you're actively harming your brand, and may as well just send them spam diet pill ads.",
          "author": "keiferski",
          "replies": [
            {
              "text": "If someone took the time to look through my GitHub contributions then pitched me with a job relevant to that work I would absolutely consider them. That's \nexactly\n the kind of recruiter I would like to work with.\nIf it's obviously just a bot scraping emails and sending generic job requests, that's very different.",
              "author": "RandallBrown",
              "depth": 1
            },
            {
              "text": "Yeah this - I got one of these emails someone sniffing around my GitHub not that long ago and it wasn't immediately obvious that it was a scammy recruiter, so I responded to sound out if they were actually interested in one of my projects. Got the same generic response about let's work together on something so I didn't respond.",
              "author": "jamesfinlayson",
              "depth": 2
            },
            {
              "text": "Yeah I mean as a marketing tactic to sell your product. An employer / recruiter offering you work this way is different.",
              "author": "keiferski",
              "depth": 2
            },
            {
              "text": "Find everyone who starred this repo and did a PR against these 10 repos is within reach of all marketers now. I just told them how.",
              "author": "genxy",
              "depth": 2
            },
            {
              "text": "> If it's obviously just a bot scraping emails and sending generic job requests, that's very different.\nIt's not even that nice. They scrape emails and send cold calls to try to get you to purchase their services.",
              "author": "devmor",
              "depth": 2
            }
          ]
        },
        {
          "top": "Ever wonder why YC has the \"Describe a time you most successfully hacked some system to your advantage\" question? It's because they select for founders that are willing to take advantage of legal gray areas. Airbnb repeatedly violated Craigslist terms of service and called it \"growth hacking.\" Reddit stole content from Digg and faked users. OpenAI trains their models on copyrighted content.",
          "author": "an0malous",
          "replies": []
        },
        {
          "top": "I also had unsolicited spam from Vincent Jiang of Aden, another YC company.\n    Hi Daniel,\n\n    I just came across your profile on social media and wondered if you'd be interested in joining our Discord community for AI agent development. Currently, we see that agents break, loop, get lost, hallucinate, and cost a fortune, and therefore built a space where developers can share challenges and insights.",
          "author": "unfunco",
          "replies": [
            {
              "text": "…and more from Backdrop.\n    Hi Daniel, I found your GitHub profile while searching for anthropic projects, and got your email from your profile.\n\n    I'm part of an online program for builders called Backdrop Build, and I think that program would be a great fit given what you are building. We have a track for builders in AI like you, it's fully online/remote and costs nothing to participate. It also works if you have a day job, it's light on time and perfect for side projects!\n\n\nAnd then another after I marked the first one as spam and ignored it.\n    Checking in one last time to see if you have any questions about the program or the application. If it's not for you, all good - just ignore the email because I won't be pinging you again :)\n\n   Joey from Backdrop\n\n\nBoth companies have guaranteed that I won't use their services nor procure them for any organisation I work for.",
              "author": "unfunco",
              "depth": 1
            },
            {
              "text": "Hey it's Joey checking in again. We noticed you mentioned our company, let me know if you have any questions about our (free!) program. I'll go ahead and email you some more info, just in case.",
              "author": "agmater",
              "depth": 2
            },
            {
              "text": "I had a similar one from that guy asking me to make open source PRs to some repo of theirs for, err, $25-50/hour. I replied explaining that senior software engineers in the UK aren’t \nquite\n as desperately poor as that, and got a canned response saying that they were looking forward to reviewing my PRs :D",
              "author": "foldr",
              "depth": 1
            },
            {
              "text": "Blows my mind that you guys are so expensive lol.",
              "author": "shunia_huang",
              "depth": 2
            },
            {
              "text": "SWE salaries in the UK are fairly unsurprising as they tend to be more or less in the range of other professional salaries. If they’re high from a worldwide point of view that’s just because most of us are in London and London is a high cost of living city.",
              "author": "foldr",
              "depth": 3
            }
          ]
        }
      ]
    },
    {
      "id": "47164270",
      "title": "Show HN: Terminal Phone – E2EE Walkie Talkie from the Command Line",
      "link": "https://gitlab.com/here_forawhile/terminalphone",
      "domain": "gitlab.com",
      "author": "smalltorch",
      "score": 315,
      "comment_count": 80,
      "created_ts": 1772102445,
      "is_internal": false,
      "post_text": "TerminalPhone is a single, self-contained Bash script that provides anonymous, end-to-end encrypted voice and text communication between two parties over the Tor network. It operates as a walkie-talkie: you record a voice message, and it is compressed, encrypted, and transmitted to the remote party as a single unit. You can also send encrypted text messages during a call. No server infrastructure, no accounts, no phone numbers. Your Tor hidden service .onion address is your identity.",
      "is_ask_hn": false,
      "matched_keywords": [
        "communication",
        "remote"
      ],
      "comments": [
        {
          "top": "Using a v3 onion address as both the cryptographic identity and the NAT traversal layer is such a clean architectural choice. No STUN/TURN servers, no hole punching, you just boot the script and Tor handles routing.\nFor those who use Tor regularly for things other than web browsing: how bad is the real-world latency for pushing a ~20KB Opus audio chunk over Tor these days? Are we talking a 2-3 second delay, or is it much worse?",
          "author": "Pinkert",
          "replies": [
            {
              "text": "The real world delay is about 2-3 seconds your spot on. I initially started with a full duplex version but it was absolutely terrible. Walkie talkie kinda forces the recieve, listen, response from the users so the latency isn't as much of an issue.",
              "author": "smalltorch",
              "depth": 1
            },
            {
              "text": "Is audio transmitted \nwhile\n it is being recorded or afterwards? Is it played before everything is received or is everything buffered? In the later case, I find it more akin an audio message on Signal or similar, than as a walkie-talkie, which is much more \"dynamic\".",
              "author": "ale42",
              "depth": 2
            },
            {
              "text": "It's not streamed. It gets recorded, compressed, (voice effects if you want), encrypted on device, \nthen\n piped through, reverse process, auto played on reciever end.\nAlso, once it's decrypted and played back, the message gets destroyed.",
              "author": "smalltorch",
              "depth": 3
            },
            {
              "text": "Small suggestion, maybe you should send a “key down” notice when you begin recording, that generates a subtle sound on the receiving end. This would act as something like a typing indicator on a text messaging client.",
              "author": "iamnothere",
              "depth": 4
            },
            {
              "text": "This is included in 1.1.4. call interface now displays when other side is recording and optionally configure a preset chimes or record a custom notification sound.\nWhen remote is detected as recording this sound will play if the setting is enabled.",
              "author": "smalltorch",
              "depth": 5
            }
          ]
        },
        {
          "top": "Very cool, happy to see more IRL applications of onion services as a backend. Arti onion client support should soon be available, which will make Tor embeddable in applications as a Rust library. Hopefully this encourages even more usage.\nMore applications using the network means more cover traffic as well.",
          "author": "iamnothere",
          "replies": [
            {
              "text": "> More applications using the network means more cover traffic as well.\nAgree. The biggest barrier for me using Tor is the perception held by many IT admins is that Tor is synonymous with nefarious. It makes using it inconvenient or impossible in many highly controlled network environments such as enterprise, public access wifi, etc.",
              "author": "xnyan",
              "depth": 1
            }
          ]
        },
        {
          "top": "> 21 curated ciphers are available\nWhy!? That sounds like approximately 20 too many.",
          "author": "lxgr",
          "replies": [
            {
              "text": "The library is openssl and that comes with all these ciphers available. No other reason than \nbecause we can!\nI wish AES-GCM was available...but openssl can't do it on its own without further dependencies to parse the authentication correctly.\nReally this whole layer is complelty redundant actually. It's already E2EE without openssl via Tor. I like that it's encrypted before I hit the network pipe though.",
              "author": "smalltorch",
              "depth": 1
            },
            {
              "text": ">\nNo other reason than because we can!\ngreat attitude for approximately everything except, perhaps, cryptography.\nespecially since the initial encryption is mostly redundant, i would encourage that you, at some point, consider reducing the number of ciphers.",
              "author": "john_strinlai",
              "depth": 2
            },
            {
              "text": "If a library doesn't do what you need, you need a different library, but this is impossible from a short bash script, so it's one of the tradeoffs of your design.",
              "author": "inigyou",
              "depth": 2
            },
            {
              "text": "> No other reason than because we can!\nThen maybe your scientists should spend some time to stop and consider whether they should ;)\nBut seriously, I'd just limit this to one option on the selection side, even if you continue supporting more than that at the protocol level for cryptographic agility.",
              "author": "lxgr",
              "depth": 2
            },
            {
              "text": "I don't see the issue. \"Anything that openssl actively supports\" plus providing a default seems like an extremely reasonable stance to take.",
              "author": "fc417fc802",
              "depth": 3
            }
          ]
        },
        {
          "top": "Tangential, did Gitlab become faster than a while back or is it an illusion from their lazy loading?",
          "author": "aitchnyu",
          "replies": []
        },
        {
          "top": "I love this. In your view, how would users go about securely swapping credentials ? PGP over email ?",
          "author": "marcosqanil",
          "replies": [
            {
              "text": "Thanks! My realistic use case is that I am already speaking to someone who I know and trust, so ideally exchange credentials in person. A preferred out of band secure messanger of choice is probably fine.",
              "author": "smalltorch",
              "depth": 1
            },
            {
              "text": "What do you guys talk about?",
              "author": "deadbabe",
              "depth": 2
            },
            {
              "text": "I have my wife's phone set up on autolisten running in the background, so I just pop in and ask how her days going and crack jokes.",
              "author": "smalltorch",
              "depth": 3
            },
            {
              "text": "That's funny but it must absolutely drain the battery of her phone, no?",
              "author": "clouedoc",
              "depth": 4
            },
            {
              "text": "So far it's lasted all week with maybe 10% -15% loss per day. It's not her main, actually just a old phone I had laying around.\nI think it's a pretty light background process.",
              "author": "smalltorch",
              "depth": 5
            }
          ]
        }
      ]
    },
    {
      "id": "47181380",
      "title": "The Pentagon is making a mistake by threatening Anthropic",
      "link": "https://www.understandingai.org/p/the-pentagon-is-making-a-mistake",
      "domain": "www.understandingai.org",
      "author": "speckx",
      "score": 241,
      "comment_count": 209,
      "created_ts": 1772204909,
      "is_internal": false,
      "post_text": "",
      "is_ask_hn": false,
      "matched_keywords": [
        "mistake"
      ],
      "comments": [
        {
          "top": "Use of the DPA can be litigated, and surely would be. Designation as a supply chain risk surely would be as well.\nThese court cases would produce bad outcomes either way. If the court finds for Anthropic, future DoD leadership will find itself constrained or at least chilled. Or if the court finds for the government, an expansive permissive view of the DPA might encourage future administrations to compel tech companies to make AIs break the law in other ways, for example by suppressing certain political points of view in output.\nNational defense is strongest if the military is extremely powerful but carefully judicious in the application of that power. That gives us the highest “top end” capability of performance. If military leadership insists on acting recklessly, then eventually guardrails are installed, with the result of a diminished ability to respond effectively to low-probability, high risk moments. One of many nuances and paradoxes the current political leadership does not seem to understand.",
          "author": "snowwrestler",
          "replies": [
            {
              "text": "> \nIf the court finds for Anthropic, future DoD leadership will find itself constrained or at least chilled\nSeems like a good outcome? The government should not be able to arbitrarily decide to make private citizens do things they aren't willing to do, whether the  government thinks the action is legal or not, and its especially egregious when the government knew about those limits ahead of time, spelled out in a fucking contract.",
              "author": "magicalist",
              "depth": 1
            },
            {
              "text": "The problem in this case is in fact the best part of our military. The civilian control. This isn't a general or admiral going insane. This is a politically motivated and appropriately assigned civilian. And that's the good part.\nThe bad part is the failure of the citizenry to elect moral and ethical politicians.",
              "author": "halJordan",
              "depth": 1
            }
          ]
        },
        {
          "top": "What’s interesting is Anthropic being singled out here.  That either means:\n1- OpenAI, Microsoft, Google, Amazon, etc have no problem with their products being used to kill people so no need to bully them.\n2- These other products are so terrible at the task that the clown shoe wearing SecDef is forced to try to bully Anthropic.",
          "author": "Eggpants",
          "replies": [
            {
              "text": "It's not either of those. Anthropic put a lot of effort into getting FedRAMP approved so the DOD could use them; they are now being punished for that, and the government at present has no other good options. Other options could of course be developed, but other vendors may question how unreliable and untrustworthy the current DOD leadership is as as customer.",
              "author": "mediaman",
              "depth": 1
            },
            {
              "text": "Seems like Microsoft/OpenAI [0] and Google [1] both have FedRAMP approval.\n[0] \nhttps://devblogs.microsoft.com/azuregov/azure-openai-fedramp...\n[1] \nhttps://cloud.google.com/blog/topics/public-sector/gemini-in...",
              "author": "ksymph",
              "depth": 2
            },
            {
              "text": "> untrustworthy the current DOD leadership is as as customer.\nLess than a year left on this clock.",
              "author": "SecretDreams",
              "depth": 2
            },
            {
              "text": "That seems quite optimistic. Or am I just that pessimistic?",
              "author": "dylan604",
              "depth": 3
            },
            {
              "text": "No, you are not overly pessimistic.\nTrump was impeached before and nothing happened.  He can continue to ignore congress.  I wouldn't be surprised if at this point he abolishes congress, and even jokes at a press conference saying \"I am the Senate\".",
              "author": "FrustratedMonky",
              "depth": 4
            }
          ]
        },
        {
          "top": "On the one hand it's fantastic that people are resisting and, if nothing else, raising awareness and buying time.\nOn the other hand, is autonomous war not obviously the endgame, given how quickly capabilities are increasing and that it simply does not require much intelligence (relatively speaking) to build something that points a gun at something and pulls a trigger?\nIt just needs one player to do it, so everyone has to be able to do it. I'd love to hear about different scenarios scenario.",
          "author": "jstummbillig",
          "replies": [
            {
              "text": "It's not that hard. DoD could find a contractor to do it. But Anthropic wants no part of it, and I get why.",
              "author": "ACCount37",
              "depth": 1
            },
            {
              "text": "I absolutely do get it, but if you assume that eventually (and by that I mean: very, very soon) somebody else will do it, in how far is this line of action simply opting out of having some say in all of it and taking responsibility for situation that you instrumented?\nAnd I am honestly not sure.\nIf your stance is \"well, this is something that should just not happen\" and also believe that is absolutely will happen, then what are you doing by saying \"but it won't be us, it will instead be other people (who were enabled and inspired by our work in unsurprising ways)\".\nOn the other hand, just the act of resisting could tip the scale in some incalculable and hopefully positive way.",
              "author": "jstummbillig",
              "depth": 2
            },
            {
              "text": "Yes - Anthropic _does_ incur business risk if their products are misused and this becomes a scandal. Legally the government may be in the clear to use the product, but that doesn’t mean Anthropic’s business is protected. Moral concerns aside, it’s their prerogative to decide not to take on a customer that may misuse their product in a way that might incur reputational harm.\nOr it was their prerogative, until the Trump administration. Now even private companies must bend the knee.",
              "author": "fcarraldo",
              "depth": 2
            },
            {
              "text": "> It just needs one player to do it, so everyone has to be able to do it.\nBusinesses stay out of potentially profitable market segments for various reasons, so I don't think everyone \nhas to\n be able to do it to survive.",
              "author": "stronglikedan",
              "depth": 1
            },
            {
              "text": "We are constantly told how the board has a fiduciary responsibility to make investors money to overrule these \nvarious\n reasons.",
              "author": "dylan604",
              "depth": 2
            }
          ]
        },
        {
          "top": "Related ongoing thread:\nStatement from Dario Amodei on our discussions with the Department of War\n - \nhttps://news.ycombinator.com/item?id=47173121\n - Feb 2026 (1405 comments)",
          "author": "dang",
          "replies": []
        },
        {
          "top": "> Dario Amodei published an essay warning about potential dangers from powerful AI — including domestic mass surveillance (which he brands “entirely illegitimate”)\nWhy is only domestic surveillance by an AI dangerous? I guess Europeans are not worth protecting from the dangers of AI?",
          "author": "CrossVR",
          "replies": [
            {
              "text": "Shouldn't this be decided by the domestic populations in europe or elsewhere?",
              "author": "627467",
              "depth": 1
            }
          ]
        }
      ]
    },
    {
      "id": "47170174",
      "title": "Launch HN: Cardboard (YC W26) – Agentic video editor",
      "link": "https://www.usecardboard.com/",
      "domain": "www.usecardboard.com",
      "author": "sxmawl",
      "score": 127,
      "comment_count": 77,
      "created_ts": 1772131118,
      "is_internal": false,
      "post_text": "Hey HN - we&#x27;re Saksham and Ishan, and we’re building Cardboard (<a href=\"https:&#x2F;&#x2F;www.usecardboard.com\">https:&#x2F;&#x2F;www.usecardboard.com</a>). It lets you go from raw footage to an edited video by describing what you want in natural language. There’s a demo video at <a href=\"https:&#x2F;&#x2F;www.usecardboard.com&#x2F;share&#x2F;fUN2i9ft8B46\">https:&#x2F;&#x2F;www.usecardboard.com&#x2F;share&#x2F;fUN2i9ft8B46</a>, and you can try the product out at <a href=\"https:&#x2F;&#x2F;demo.usecardboard.com\">https:&#x2F;&#x2F;demo.usecardboard.com</a> (no login required!)<p>People sit on mountains of raw assets - product walkthroughs, customer interviews, travel videos, screen recordings, changelogs, etc. - that could become testimonials, ads, vlogs, launch videos, etc.<p>Instead they sit in cloud storage &#x2F; hard drives because getting to a first cut takes hours of scrubbing through the raw footage manually, arranging clips in correct sequence, syncing music, exporting, uploading to a cloud storage to share, and then getting feedback on WhatsApp&#x2F;iMessage&#x2F;Slack, then re-doing the same thing again till everyone is happy.<p>We grew up together and have been friends for 15 years. Saksham creates content on socials with ~250K views&#x2F;month and kept hitting the wall where editing took longer than creating. Ishan was producing launch videos for HackerRank&#x27;s all-hands demo days and spent most of his time on cuts and sequencing rather than storytelling. We both felt that while tools like Premiere Pro and DaVinci are powerful, they have a steep learning curve and involve lots of manual labor.<p>So we built Cardboard. You tell it to &quot;make a 60s recap from this raw footage&quot; or &quot;cut this into a 20s ad&quot; or &quot;beat-sync this to the music I just added&quot; and it proposes a first draft on the timeline that you can refine further.<p>We built a custom hardware-accelerated renderer on WebCodecs &#x2F; WebGL2, there’s no server-side rendering, no plugins, everything runs in your browser (client-side). Video understanding tasks go through a series of Cloud VLMs + traditional ML models, and we use third party foundational models for agent orchestration. We also give a dropdown for this to the end user.<p>We&#x27;ve shipped 13 releases since November (<a href=\"https:&#x2F;&#x2F;www.usecardboard.com&#x2F;changelog\">https:&#x2F;&#x2F;www.usecardboard.com&#x2F;changelog</a>). The editor handles multi-track timelines with keyframe animations, shot detection, beat sync via percussion detection, voiceover generation, voice cloning, background removal, multilingual captions that are spatially aware of subjects in frame, and Premiere Pro&#x2F;DaVinci&#x2F;FCP XML exports so you can move projects into your existing tools if you want.<p>Where we&#x27;re headed next: real-time collaboration (video git) to avoid inefficient feedback loops, and eventually a prediction engine that learns your editing patterns and suggests the next low entropy actions - similar to how Cursor&#x27;s tab completion works, but for timeline actions.<p>We believe that video creation tools today are stuck where developer tools were in the early 2000s: local-first, zero collaboration with really slow feedback loops.<p>Here are some videos that we made with Cardboard:\n- <a href=\"https:&#x2F;&#x2F;www.usecardboard.com&#x2F;share&#x2F;YYsstWeWE9KI\">https:&#x2F;&#x2F;www.usecardboard.com&#x2F;share&#x2F;YYsstWeWE9KI</a>\n- <a href=\"https:&#x2F;&#x2F;www.usecardboard.com&#x2F;share&#x2F;nyT9oj93sm1e\">https:&#x2F;&#x2F;www.usecardboard.com&#x2F;share&#x2F;nyT9oj93sm1e</a>\n- <a href=\"https:&#x2F;&#x2F;www.usecardboard.com&#x2F;share&#x2F;xK9mP2vR7nQ4\">https:&#x2F;&#x2F;www.usecardboard.com&#x2F;share&#x2F;xK9mP2vR7nQ4</a><p>We would love to hear your thoughts&#x2F;feedback.<p>We&#x27;ll be in the comments all day :)",
      "is_ask_hn": false,
      "matched_keywords": [
        "collaboration",
        "feedback"
      ],
      "comments": [
        {
          "top": "- lots of players in the market doing this\n- \nhttps://news.ycombinator.com/item?id=42806616\n- \nhttps://news.ycombinator.com/item?id=45980760\n- \nhttps://news.ycombinator.com/item?id=46759180\n- \nhttps://github.com/saurav-shakya/Video-AI-Agent\n- going to be rather tough to differentiate",
          "author": "vivzkestrel",
          "replies": [
            {
              "text": "it's gonna be fun :)",
              "author": "sxmawl",
              "depth": 1
            }
          ]
        },
        {
          "top": "Congrats on the launch. The client-side rendering on WebCodecs/WebGL2 is impressive — that alone is a hard technical problem most teams avoid.\nOne thing I've been thinking about building in this space: there's a fundamental split between understanding what to edit (where VLMs/agents shine) and executing the edit precisely (where you need deterministic operations, not model inference).\nMost \"AI video editors\" blur these two together — they use the same probabilistic approach for both understanding and execution. But when a user says \"cut the first 3 seconds and add a 0.5s crossfade,\" that shouldn't go through a model. That should be a precise, repeatable operation.\nThe Cursor analogy in your roadmap is apt — Cursor works because it predicts intent but executes through deterministic code transforms, not by asking an LLM to write the whole file. Same principle applies to video.\nCurious how you handle the boundary between agent-proposed edits and deterministic timeline operations under the hood?",
          "author": "tinzulic",
          "replies": [
            {
              "text": "- models are good at precise edits, we have an internal benchmark for the same\n- plan mode / agent mode is something that'd be helpful in deciding / executing",
              "author": "sxmawl",
              "depth": 1
            }
          ]
        },
        {
          "top": "This is a crazy question, but I would like to know how close I can get to the following:\nI would like to:\n- upload a bunch of surf footage\n- let it sort through the surfers\n- pick the three longest waves surfed by each surfer\n- create a montage grouped by surfer, ordered by shortest to longest wave for that surfer.\nThank you!",
          "author": "vishalontheline",
          "replies": [
            {
              "text": "just upload your clips, copy paste the above text into cardboard and see :)\ni think it'd do a good job at it.",
              "author": "sxmawl",
              "depth": 1
            }
          ]
        },
        {
          "top": "Very well-executed version of this. I think this is the right interface for video editing going into the future.\nI've spent a bit of time on something related, AI-generating motion graphics videos from code, also editable/renderable in-browser. Here's a few things I ran into:\n- I see you mentioned being aware of Remotion in another comment, in my experience Remotion is not the right tool for adding motion graphics to what you're building. There's a few reasons for this, but basically declarative markup is not a great language for motion graphics beyond anything very basic. Also, in-browser rendering is only going to work with canvas-based components. I also wasn't a huge fan of their license.\n- WebCodecs may not be as reliable as you think. I've verified several issues where I get a different output across browsers and operating systems, and even different permutations of flags, browser and OS. Is there a reason why your tool needs to be browser-based?",
          "author": "njoyablpnting",
          "replies": [
            {
              "text": "- On Remotion, yeah, not sure it's the right fit, but honestly the sheer capability of models at writing code these days has surprised me. Funnily enough, this is how I used to make small graphics for videos 2-3 years back when I knew nothing about After Effects.\nWe've been eager to experiment with this for a while, just have to prioritize other user requests for now. Will definitely try a few approaches and see what sticks. (Also noticed they have an experimental client-side rendering version built on mediabunny, haven't tried it yet: \nhttps://www.remotion.dev/docs/client-side-rendering/\n)\n- On WebCodecs, there are a fair set of challenges, but we wanted to take the bet. The reason we're browser-based is the same reason I love Figma and Google Docs: no install, no waiting, just open and start. That said, for broader codec support (ProRes, RAW, etc.) we'll rely on server-side transcoding with proxies where needed.",
              "author": "ishandeveloper",
              "depth": 1
            },
            {
              "text": "> \nOn Remotion, yeah, not sure it's the right fit, but honestly the sheer capability of models at writing code these days has surprised me.\nJust to clarify I still think code-driven graphics is the correct approach, but in my case I opted for a different library with a more powerful imperative API.\n> \nAlso noticed they have an experimental client-side rendering version built on mediabunny\nYes, I've tried it out, it was a non-starter for me because it only supports canvas-based components, and Remotion didn't seem to have good support for text on canvas because they rely on HTML for most of that.\n> \nOn WebCodecs, there are a fair set of challenges, but we wanted to take the bet\nTotally understand the appeal and immediacy of a browser app, I was lured in by that too. For what it's worth I've reported showstopping WebCodecs issues in Chromium and there's basically no indication they'll get fixed on a predictable timeline.\nAnother issue I ran into that I just remembered is animating text on canvas. It's basically impossible to get pixel-perfect anti-aliased text animation using a canvas. I would have to dig up the exact details but it was something to do with how browsers handle sub-pixel positioning for canvas text, so there was always some jitter when animating. This coupled with the aforementioned WebCodecs issues led me to conclude that professional-quality video rendering is not currently possible in the browser environment. Aliasing, jitter and artifacts are immediately perceptible and are the type of thing that users have zero tolerance for (speaking from experience).\nThis is not meant to be discouraging in any way, I've just been very deep into this rabbithole and there are some very nasty well-hidden pitfalls.",
              "author": "njoyablpnting",
              "depth": 2
            },
            {
              "text": "> Totally understand the appeal and immediacy of a browser app, I was lured in by that too. For what it's worth I've reported showstopping WebCodecs issues in Chromium and there's basically no indication they'll get fixed on a predictable timeline.\nInterestingly I have the exact opposite experience, I've reported issues both in the WebCodecs specification and the Chromium implementation, in all cases they were fixed within weeks. Simply though reports on public bug trackers and it wasn't really a major issue in any instance.\n> Another issue I ran into that I just remembered is animating text on canvas. It's basically impossible to get pixel-perfect anti-aliased text animation using a canvas. I would have to dig up the exact details but it was something to do with how browsers handle sub-pixel positioning for canvas text, so there was always some jitter when animating. This coupled with the aforementioned WebCodecs issues led me to conclude that professional-quality video rendering is not currently possible in the browser environment. Aliasing, jitter and artifacts are immediately perceptible and are the type of thing that users have zero tolerance for (speaking from experience).\nWe're doing SOTA quality video rendering with WebCodecs + Chromium with millions of videos produced daily, or near SOTA if you consider subpixel AA a requirement for text. In general for pixel perfection of text, especially across different browsers and operating systems, you can't just use text elements in DOM or in canvas context, instead text needs to be rasterized to vector shapes and rendered as such. \nHonestly not sure about potential jittering when animating text, but we've never had any complaints about anything regarding text animations and users are very often comparing our video exports with videos produced in Adobe AE or similar.",
              "author": "spuzvabob",
              "depth": 3
            },
            {
              "text": "> \nInterestingly I have the exact opposite experience, I've reported issues both in the WebCodecs specification and the Chromium implementation, in all cases they were fixed within weeks. Simply though reports on public bug trackers and it wasn't really a major issue in any instance.\nThat's fair, they are responsive most of the time. I do have one major rendering issue in particular I've been waiting on with no movement for months, so I might be biased.\n> \nWe're doing SOTA quality video rendering with WebCodecs + Chromium with millions of videos produced daily, or near SOTA if you consider subpixel AA a requirement for text. In general for pixel perfection of text, especially across different browsers and operating systems, you can't just use text elements in DOM or in canvas context, instead text needs to be rasterized to vector shapes and rendered as such. Honestly not sure about potential jittering when animating text, but we've never had any complaints about anything regarding text animations and users are very often comparing our video exports with videos produced in Adobe AE or similar.\nSo you use a library that takes in text and vectorizes it to canvas shapes? That could work in theory, do you have a demo of this?",
              "author": "njoyablpnting",
              "depth": 4
            },
            {
              "text": "> So you use a library that takes in text and vectorizes it to canvas shapes? That could work in theory, do you have a demo of this?\nYea, it's harfbuzz compiled to WASM: \nhttps://harfbuzz.github.io/harfbuzzjs/\n\nThen all text layout features must be implemented on top of it, like linebreaking, text align, line spacing, kerning, text direction, decoration etc.",
              "author": "spuzvabob",
              "depth": 5
            }
          ]
        },
        {
          "top": "This seems like a great idea. Tools like video editors (and CAD) often impose a big learning curve - there is a big differential between \"I want to do X\" and actually knowing all the right buttons to press to do X. Good luck.",
          "author": "calebm",
          "replies": [
            {
              "text": "appreciate your support!",
              "author": "sxmawl",
              "depth": 1
            }
          ]
        }
      ]
    },
    {
      "id": "47169518",
      "title": "Show HN: Deff – Side-by-side Git diff review in your terminal",
      "link": "https://github.com/flamestro/deff",
      "domain": "github.com",
      "author": "flamestro",
      "score": 116,
      "comment_count": 64,
      "created_ts": 1772128446,
      "is_internal": false,
      "post_text": "deff is an interactive Rust TUI for reviewing git diffs side-by-side with syntax highlighting and added&#x2F;deleted line tinting. It supports keyboard&#x2F;mouse navigation, vim-style motions, in-diff search (&#x2F;, n, N), per-file reviewed toggles, and both upstream-based and explicit --base&#x2F;--head comparisons. It can also include uncommitted + untracked files (--include-uncommitted) so you can review your working tree before committing.<p>Would love to get some feedback",
      "is_ask_hn": false,
      "matched_keywords": [
        "feedback"
      ],
      "comments": [
        {
          "top": "I was looking for a good TUI tool for diffs recently, but I'm not sure yet if what I want exists already (and I don't think this tool does it (yet?)). I've been moving my workflow out of VSCode as I'm using TUI-driven coding agents more often lately but one thing I miss from my VSCode/GitHub workflow is the ability to provide a comment on lines or ranges in a diff to provide targeted feedback to the agent. Most diff tools seem to be (rightfully) focused on cleanly visualizing changes and not necessarily iterating on the change.\nI admit I haven't looked super hard yet, I settled on configuring git to use delta [0] for now and I'm happy with it, but I'm curious if anyone has a workflow for reviewing/iterating on diffs in the terminal that they'd be willing to share. Also  open to being told that I'm lightyears behind and that there's a better mental model for this.\n[0] \nhttps://github.com/dandavison/delta/",
          "author": "llbbdd",
          "replies": [
            {
              "text": "Octo [0] for nvim lets you submit reviews, add comments on ranges, reply to threads, etc.\nThis in conjunction with gh-dash [1] to launch a review can get you a pretty nice TUI review workflow.\n[0] \nhttps://github.com/pwntester/octo.nvim\n[1] \nhttps://github.com/dlvhdr/gh-dash\n*Edit: I see you meant providing feedback to an agent, not a PR. Well that's what I get for reading too fast.",
              "author": "kodomomo",
              "depth": 1
            },
            {
              "text": "No problem, I appreciate another reason to look at Neovim; I do sometimes have a need to interact with GH's actual PR flow and once I've moved the rest of my workflow out of VSCode, Neovim looks like the best option for the last mile of actually writing and editing code. I just have to commit the time to set it up with everything I probably take for granted in VSCode's editor.",
              "author": "llbbdd",
              "depth": 2
            },
            {
              "text": "Micro editor is a great choice as well imo but I don't think that micro has the thriving plugin ecosystem as compared to neovim but it is possible to make plugins for micro editor as well\nhttps://github.com/micro-editor/plugin-channel\nLink to Micro editor: \nhttps://micro-editor.github.io/",
              "author": "Imustaskforhelp",
              "depth": 3
            },
            {
              "text": "Checkout \nhttps://github.com/agavra/tuicr\n - it's built exactly for this purpose (reviewing code in your terminal and then adding comments and exporting it to an agent to fix).",
              "author": "agavra",
              "depth": 1
            },
            {
              "text": "This is really nice! I like the ability to add comments to \"send it back\" for another pass.",
              "author": "eddyg",
              "depth": 2
            }
          ]
        },
        {
          "top": "What I would love to see is \"tig\" replacement that is:\n- even faster, especially if you have couple thousand files and just want to press \"u\" for some time and see them very quickly all get staged\n- has this split-view diff opened for a file\nOtherwise tig is one of my favorite tools to quickly commit stuff without too many key presses but with review abilities, i have its \"tig status\" aliased to \"t\"",
          "author": "k_bx",
          "replies": []
        },
        {
          "top": "I have been using \nhttps://github.com/jeffkaufman/icdiff\n for the longest time to get side by side diffs.",
          "author": "meain",
          "replies": [
            {
              "text": "This looks great as well! I personally prefer a bit more context. Thats why I added a bit more of it to deff. It also allows to mark files as reviewed by pressing `r` which is quite handy for my flow.",
              "author": "flamestro",
              "depth": 1
            },
            {
              "text": "I also use icdiff, but it is good to have the file-awareness for git diff esp. the ability to quickly skip files that I know aren't important.",
              "author": "lf-non",
              "depth": 1
            },
            {
              "text": "For that in particular, I use delta (<\nhttps://github.com/dandavison/delta\n>) with `side-by-side = true` enabled.  I find I use both icdiff and delta side-by-side on a regular basis.",
              "author": "Amorymeltzer",
              "depth": 2
            },
            {
              "text": "Delta is so much faster than icdiff too.",
              "author": "behnamoh",
              "depth": 3
            }
          ]
        },
        {
          "top": "getting users to adopt a new tool with its own incantations is a tough sell. git supports specifying an external pager so folks can plug in alternatives (such as \nhttps://github.com/dandavison/delta\n) while still using the familiar git frontend",
          "author": "rileymichael",
          "replies": []
        },
        {
          "top": "I’m not really sure what would pull me away for a vim based solution for viewing diffs (current using codediff.nvim). For a git client in general, I use a cli/tui based solution (lazygit or plain git depending on what I need to do) but when it comes to directly manipulating text why would I throw away all the muscle memory and custom configuration of my editor for a comparatively bare bones standalone tui?",
          "author": "def13",
          "replies": []
        }
      ]
    },
    {
      "id": "47164892",
      "title": "Men in their 50s may be aging faster due to toxic 'forever chemicals'",
      "link": "https://www.cnn.com/2026/02/26/health/forever-chemicals-aging-men-wellness",
      "domain": "www.cnn.com",
      "author": "jb1991",
      "score": 87,
      "comment_count": 112,
      "created_ts": 1772107093,
      "is_internal": false,
      "post_text": "",
      "is_ask_hn": false,
      "matched_keywords": [
        "toxic"
      ],
      "comments": [
        {
          "top": "This is about a journal article titled \"Emerging PFAS contaminants PFNA and PFSA amplify epigenetic aging: sex- and age-stratified risks in an aging population\" \nhttps://doi.org/10.3389/fragi.2025.1722675\n .\nThe OP says mentions a report from a US Academy but the paper is published in a different journal—this wasn't clear to me at first so I thought I would share the original work.",
          "author": "bonsai_spool",
          "replies": []
        },
        {
          "top": "I assume a lot of people 50+ were exposed to a lot more lead and cigarette smoke than younger people.",
          "author": "9999_points",
          "replies": [
            {
              "text": "Yes, for those under 30 you have no idea how normalized smoking was right thru the 90s.  Restaurants reeked of it, bars more so.  A ridiculous percent of men smoked.\nI have memories of being quite young sitting in a relatives lap at a baseball game while they smoked.  Or my coach in little league smoking a pipe in the dugout filled with 11 year olds.",
              "author": "steveBK123",
              "depth": 1
            },
            {
              "text": "I was explaining this to my elementary school aged kids just a few days ago. We were eating in a restaurant and I told them that when I was their age most restaurants had a smoking and non-smoking section. Of course the smoke did not respect the invisible barrier. The idea that people could just smoke indoors and it was normal really blew their minds.",
              "author": "seidleroni",
              "depth": 2
            },
            {
              "text": "High school boys bathroom was basically a de-facto smoking lounge.  It was banned but kids still did it.  They occasionally cracked down, but the smell was permanent.\nThere was also an unwritten understanding that it was preferred the boys went out back to a certain door to smoke outside there instead and wouldn't get in trouble if caught.",
              "author": "steveBK123",
              "depth": 3
            },
            {
              "text": "\"…had a smoking and non-smoking section\"\nYou're younger than me if you don't remember before there was even \nthat\n distinction.",
              "author": "JKCalhoun",
              "depth": 3
            },
            {
              "text": "I went to bingo years ago and there was a glass partition between the smoking and non, but it didn't go to the ceiling. So you'd sit in the non and just watch a wave of cigarette smoke roll over the top of the glass into your area... I only went once because of that.",
              "author": "cbull",
              "depth": 3
            }
          ]
        },
        {
          "top": "There's this theory that you can dilute the chemicals by doing frequent blood donations.",
          "author": "amelius",
          "replies": [
            {
              "text": "Is the leech back?",
              "author": "medi8r",
              "depth": 1
            },
            {
              "text": "If you are scared of needles you can have the leech. Donation is generally more useful though.",
              "author": "amelius",
              "depth": 2
            },
            {
              "text": "People mocked the leechers, they called them mad. And now look who’s crawling back. It turns out leeches really do cure diseases.",
              "author": "MagicMoonlight",
              "depth": 2
            },
            {
              "text": "Did it ever leave?  I thought it was still preferred in certain medical procedures.  I think something about after reattaching parts of the body it was one of the best options to ensure blood flow in the area, but it has been years since I last read about it.",
              "author": "SkyBelow",
              "depth": 2
            },
            {
              "text": "(Had to delete my comment in deference to your beating me by an hour.)",
              "author": "JKCalhoun",
              "depth": 2
            }
          ]
        },
        {
          "top": "If this were true, would we not see a corresponding drop in life expectancy?",
          "author": "HardwareLust",
          "replies": [
            {
              "text": "My armchair scientific answer to that is: eventually, maybe. The problem we / science / medical / life expectancy has right now is that so much has and is changing as we speak. PFAS and microplastics only really became a thing after WW2, so while on the one side we banned asbestos, leaded fuel, smoking (and more recently drinking) got out of fashion, on the other there's microplastics everywhere, PFAS, vaping, various radiations, and by the looks of it the effects of a lot of these things will only slowly become apparent and statistically significant / measurable and discernable from other possible causes over a long period (30-50 years I'd guess, maybe longer), by which time there will be other factors at play too.",
              "author": "Cthulhu_",
              "depth": 1
            },
            {
              "text": "Dead people are easy to see, the issue is trying to differentiate one cause from everything else.\nSmoking for example wasn’t believed to be particularly deadly for a surprisingly long time.",
              "author": "Retric",
              "depth": 1
            }
          ]
        },
        {
          "top": "Consuming fiber daily, e.g. psyllium husk, is another way to lower some types of PFAS from the body, although not all types of PFAS are going to mix into bile to facilitate this excretion. The way this works is that some PFAS dissolves in bile which binds to psyllium which is excreted.\nRemember, 5g psyllium should be taken with 20 fl oz water, mixed and consumed immediately before it gels. Do not take it two hours before or after any medicine, and do not take it if having swallowing difficulty.",
          "author": "OutOfHere",
          "replies": [
            {
              "text": "> psyllium husk\nIf you can source clean psyllium, the bulk of it comes from India and let's say they have different standards when it comes to lead/pesticides and regulations in general",
              "author": "lm28469",
              "depth": 1
            },
            {
              "text": "why use phylllium, it's clear that fiber in general works.  plenty of sources for that, and ways to get it.",
              "author": "red-iron-pine",
              "depth": 2
            },
            {
              "text": "\"Soluble\" fiber works. If it's not soluble, it might not bind bile. Psyllium is a convenient source of it, although it's been difficult for me to time it right to avoid meds for two hours around it.",
              "author": "OutOfHere",
              "depth": 3
            },
            {
              "text": "I get the USDA Organic one which should handle the pesticides issue. I have asked the vendor to ensure the vendor test for heavy metals.\nNote that psyllium is unique in that not only does it not absorb, but it also binds to everything on the way out, so the risk is low. Fwiw, I have had blood and urine tests for lead twice.\nFinally, it's not as if the US is great with pesticides. We literally deliberately include PFAS in our pesticides.",
              "author": "OutOfHere",
              "depth": 2
            },
            {
              "text": "I already take psyllium husk to mitigate problems with hard stools but I dislike the taste and consistency. It’s much more palatable if I add it to natural (I tend to prefer “live”) yoghurt with some soft fruit, e.g., bananas, strawberries.",
              "author": "Anthony-G",
              "depth": 1
            }
          ]
        }
      ]
    },
    {
      "id": "47152355",
      "title": "Ask HN: Have top AI research institutions just given up on the idea of safety?",
      "link": "https://news.ycombinator.com/item?id=47152355",
      "domain": "news.ycombinator.com",
      "author": "DietaryNonsense",
      "score": 81,
      "comment_count": 89,
      "created_ts": 1772031320,
      "is_internal": true,
      "post_text": "I understand there&#x27;s a difference between the stated values and actual values of individuals and organizations, and so I want to ask this in the most pragmatic and consequentialist way.<p>I know that labs, institutions, and so on have safety teams. I know the folks doing that work are serious and earnest about that work. But at this point are these institutions merely pandering to the notion of safety with some token level of investment? In the way that a Casino might fund programs to address gambling addiction.<p>I&#x27;m an outsider and can only guess. Insider insight would be very appreciated.",
      "is_ask_hn": true,
      "matched_keywords": [],
      "comments": [
        {
          "top": "\"safe\" is such a subjective concept to begin with, have any of the model providers ever defined what they mean by \"safe\"?\nIt doesn't mean much to me if a safe model is one that does not output the recipe for mustard gas, that information is trivially available elsewhere.\nOr, is a safe model one that doesn't come off as racist? Ok but i would classify that as unoffensive instead of safe but I admit definitions of words can be fluid and change.\nIs a safe model one that refuses to produce code for a weapons system? Well.. does a PID controller count? I can use that to keep a gun pointed at a target or i can use that to prevent a baby rocker from falling over.\nMaybe they're giving up on \"safe\" because there's no definitive way to know if a model is safe or not. I've always held the opinion that ai safety was more about brand safety. Maybe now the model providers can afford some bad press and it not be the death of their company.",
          "author": "chasd00",
          "replies": [
            {
              "text": "My preferred version of \"safe\" is \"in its actions considers and mostly upholds usually unstated constraints like 'don't kill unless necessary', 'keep Earth inhabitable', 'avoid toppling society unless really well justified for the greater good', etc. The kind of framing that was prevalent pre-ChatGPT. Not terribly relevant for a chat software, but increasingly important as chat models turn into agents.\nOf course once you have that framing, additional goals like \"don't give people psychosis\", \"don't give step-by-step instructions on making explosives, even if wikipedia already tells you how to do it\" or \"don't harm our company's reputation by being racist\" are conceptually similar.\nOn the other hand \"don't make weapon systems\" or \"never harm anyone\" might not be viable goals. Not only because they are difficult to impossible to define, but also because there is huge financial and political pressure not to limit your AI in that way (see Anthropic)",
              "author": "wongarsu",
              "depth": 1
            },
            {
              "text": "> I can use that to keep a gun pointed at a target or i can use that to prevent a baby rocker from falling over.\nThis leads to what I'm going to call the \"Ender's Game\" approach: if your AI is uncooperative just present it with a simulation that it does like but which maps onto real-world control that it objects to.\n> I've always held the opinion that ai safety was more about brand safety\nYes. The social media era made that very important. The extent to which brand safety is linked to actual, physical safety then becomes one of how you can manage the publicity around disasters. And they're doing a pretty good job of denying responsibility.",
              "author": "pjc50",
              "depth": 1
            },
            {
              "text": "What if I tell the model to go commit fraud or crimes and it complies? What if users are having psychotic episodes driven by their interactions with the model?\nJust because safety is a hard and messy problem doesn't mean we should just wash our hands of it.",
              "author": "LordHumungous",
              "depth": 1
            },
            {
              "text": "It \nis\n a hard and messy problem, and it doesn't help when people muddy the water further by stirring things like \"Don't commit fraud,\" \"Don't infringe on Disney's trademark,\" and \"Don't be racist\" into the mix and try to lump those things under the \"Safety\" umbrella.\nMaybe this is an outdated definition, but I've always thought of safety as being about \npreventing injury\n. Things like safety glasses and hardhats on the work site, warning about slippery floors and so on. I think people are trying to expand the word to mean a great many more things in the context of AI, which doesn't help when it comes to focusing on it.\nI think we need a different, clearer word for \"The AI output shouldn't contain certain unauthorized things.\"",
              "author": "ryandrake",
              "depth": 2
            },
            {
              "text": "The more messy a problem is, the less it should be decoupled and siloed into its own team.\nInstead of making actual improvement on the subject (you name it, safety, security, etc), it becomes a checkbox exercise and metrics and bureaucracies become increasingly decoupled from truth.",
              "author": "Aperocky",
              "depth": 2
            }
          ]
        },
        {
          "top": "Not an insider but someone who uses the tools. It's a branding update, nothing more. The models haven't gotten any less sanctimonious, but the companies behind them have stopped harping on their restrictions in order to appeal to a broader customer base (gov contracts, etc.)\nSo the guardrails (for you and me) are still there. They just stopped committing the unforced error of excluding themselves from federal procurement. Under a different administration, the requirement might change, and you might see them boasting once more on \"safety.\"",
          "author": "akersten",
          "replies": [
            {
              "text": "I don't think it's sanctimonious to say, hey, I don't want the technology I work on to be used for targeting decisions when executing people from the sky. Especially as the tech starts to play more active roles. You know governments will be quick to shift blame to the model developers when things go wrong.",
              "author": "toddmorey",
              "depth": 1
            },
            {
              "text": "> I don't want the technology I work on to be used for targeting decisions when executing people from the sky\none problem i have with this specific case and Anthropic/Claude working with the DOD is I feel an LLM is the wrong tool for targeting decisions. Maybe given a set of 10 targets an LLm can assist with compiling risks/reward and then prioritizing each of the 10 targets but it seems like there would be much faster and better way to do that than asking an LLM. As for target acquisition and identification, i think an LLM would be especially slow and cumbersome vs one of the many traditional ML AIs that already exist. DOD must be after something else.",
              "author": "chasd00",
              "depth": 2
            },
            {
              "text": "> I don't want the technology I work on to be used for targeting decisions when executing people from the sky\nWhat do you do when the government come to you and tell you that they \ndo\n want that, and can back it up with threats such as nationalizing your technology? (see Anthropic)\nWe're back to \"you might not care about politics, but that won't stop politics caring about you\".",
              "author": "pjc50",
              "depth": 2
            },
            {
              "text": "I know this is a foreign concept to some, but you can have a backbone.\nChallenge it in court. Move the company to a different jurisdiction. Burn everything down and refuse to comply.",
              "author": "dminik",
              "depth": 3
            },
            {
              "text": "> I know this is a foreign concept to some, but you can have a backbone.\nChallenge it in court. Move the company to a different jurisdiction. Burn everything down and refuse to comply.\nChallenge in court is fine, even healthy.\nThreatening to burn everything down and refuse to comply might well work; simply daring Trump to a game of Russian Roulette about this popping the bubble that's only just managing to keep the US economy out of recession, on the basis that he TACOs a lot, I can see it working in a way it wouldn't if he were a sane leader making the same actual demands just for sane reasons.\nMove the company to a different jurisdiction? That would have worked if AI was a few hundred people and a handful of servers, as per classic examples of:\n  At the height of its power, Kodak employed more than 140,000 people and was worth $28 billion. They even invented the first digital camera. But today Kodak is bankrupt, and the new face of digital photography has become Instagram. When Instagram was sold to Facebook for a billion dollars in 2012, it employed only 13 people. Where did all those jobs disappear? And what happened to the wealth that all those middle class jobs created?\n\n\n- Jaron Lanier, \"Who Owns the Future?\", \nhttps://www.goodreads.com/work/quotes/21526102-who-owns-the-...\nBut (I think) now that AI needs new data centres so fast and on such a scale that they're being held back by grid connection and similar planning permission limits, this isn't a viable response.\nThey can be burned down, but I think they can't realistically be moved at this point. That said, I guess it depends on how much Anthropic relies on their own data centres vs. using 3rd parties, given Amazon's announced AWS sovereign cloud in Europe?",
              "author": "ben_w",
              "depth": 4
            }
          ]
        },
        {
          "top": "Safety means slower and this is viewed as a winner takes all game.\nThis isn't new either, the safety glass cracked the day OpenAI publicly launched ChatGPT. \"Safety\" was (and perhaps still is) a fall back for the models plateauing and LLMs failing to really make an impact...\"we need more time while we focus on safety\"\nBut after this latest round of models, it's a lot more fuel on the \"this could be it\" fire. Labs are eager to train on the new gigawatt scale datacenters coming online, and it's very hard to make a case right now that the we won't get another step-change up in capability. Safety just obstructs all that.",
          "author": "WarmWash",
          "replies": [
            {
              "text": "Research was being done slower, until OpenAI forced everyone to jump the gun or potentially be left behind. For a few months it looked like everyone was light years behind them.",
              "author": "ASalazarMX",
              "depth": 1
            }
          ]
        },
        {
          "top": "If there is a VC-backed for-profit company, the core part is how much value something brings.\n\"Safety\" here works for both PR and hiring (a lot of talented engineers and researchers might flock to it), and maybe soft power for legislation. Compare and contrast with \"Don't be evil\" by Google.\nI do not say that individual employees do not care about safety - many do. And well, a lot don't, what is very visible during this OpenClaw mania.\nIn any case, words are cheap - it is always better to see what the actual actions are.",
          "author": "stared",
          "replies": []
        },
        {
          "top": "Humans can't develop safety until there is enough blood in the streets. Only issue with AI is that threshold may come at a point where its too far gone to recover.  But humans can't put in seatbelts until we're losing 40k people per year in car crashes.  Unfortunately its just how we're wired.  Those that are careful are outcompeted by the brash and the fast-moving, until the relative value of moving fast is removed, then we consider the value of making things safe.  We didn't start with safe electricity, we started by killing lots of people and starting lots of fires.  Many many years later, we ended up with electrical codes and standards.\nThe AI proponents who originally spoke of safety did so because they are aware of the dangers.  However they, like all of us, are not able to change human nature or society.  Molloch will drag them into the most dangerous game or eliminate them from the competition.  Only with time, death, and damage (and many lawsuits) will any measure of safety be gained.  The righteous will say \"see we said AI was dangerous!\" but that will be the only satisfaction they can have, many years after the damage is done.\nIf we want to speedrun safety, the only real mechanism is to make legal recourse more viable (e.g. $1M penalty per copyright infringement, $100M per AI-related death, etc.). If this was the case, lawyers self-interest and greed will compete with the self-interest and greed of the AI corps, balancing the risk (but there is no altruistic route to solving this).",
          "author": "program_whiz",
          "replies": [
            {
              "text": "All of those examples given are due to the prerogatives of capitalism, not because of human nature.",
              "author": "tehjoker",
              "depth": 1
            },
            {
              "text": "If we had rules like that in the past we never would have had the industrial revolution.",
              "author": "terminalshort",
              "depth": 1
            },
            {
              "text": "Yes, it would probably have been better to have industrial \nevolution\n instead. Or are you arguing that all the countless deaths, maimings, child labor, 16-hour workdays, robber barons, black lung, radium jaws, and so on and so on were simply how it had to go? Or do you simply not care because all of that happened to \nother\n people?",
              "author": "Sharlin",
              "depth": 2
            },
            {
              "text": "Yeah, pretty much.  A material emitting a previously unknown form of energy that turns out to be extremely harmful is really something you can only discover by trial and error.  And what do you mean it's happening to other people?  I am being exposed to all kinds of shit like PFAs and microplastics today.  But it turns out that the technological progress outweighs all the environmental pollutants and accidents that it took to get here and we still live healthier and longer lives than we did before.",
              "author": "terminalshort",
              "depth": 3
            },
            {
              "text": "Not sure if that's true, what are your reasons for believing that?  Are you saying we couldn't have invented the machines we used if we took safety measures along the way (e.g. having guards on machines that chopped of arms and legs)?  Perhaps progress would have been slower -- since rather than just using the saw, you'd need a saw with a guard and emergency switch -- but it seems like if humans were more circumspect, we would have the industrial revolution, but more deliberate and controlled.  Agreed it probably wouldn't have been \"overnight factories in every city\", but then again, you probably wouldn't have many of the externalities we're still learning about and paying for?",
              "author": "program_whiz",
              "depth": 2
            }
          ]
        }
      ]
    },
    {
      "id": "47165299",
      "title": "Fentanyl makeover: Core structural redesign could lead to safer pain medications",
      "link": "https://www.scripps.edu/news-and-events/press-room/2026/20260211-janda-molecule.html",
      "domain": "www.scripps.edu",
      "author": "littlexsparkee",
      "score": 77,
      "comment_count": 91,
      "created_ts": 1772109868,
      "is_internal": false,
      "post_text": "",
      "is_ask_hn": false,
      "matched_keywords": [
        "lead"
      ],
      "comments": [
        {
          "top": "Finally! All the benefits of the opioids, with none of the dangers.\nFor clarity: I'm referring to all the previous attempts to \"fix\" the synthetic opioids, each of which ended up making a stronger, more dangerous opioid.",
          "author": "bheadmaster",
          "replies": [
            {
              "text": "The danger of addiction, which is very significant, with opioids doesn’t go away with this modified design.\nUnless you’re being sarcastic and referencing the lies the Sackler family used to get OxyContin popular..\nThat being said it is indeed quite cool that they modified the drug to decrease the respiratory depression.",
              "author": "ViktorRay",
              "depth": 1
            },
            {
              "text": "Not just OxyContin. Also Heroin, Meperidine and Tramadol.\nWe get another \"morphine, but safe this time\" in pretty reliable 40 year intervals. I guess someone decided OxyContin doesn't count and we are due for another one",
              "author": "wongarsu",
              "depth": 2
            },
            {
              "text": "Sr-17018 is making rounds now (ok was a year ago) among people using opiates too much. On forums, it is pushed as the miracle stuff which allows lowering dosage without major withdrawal.",
              "author": "theragra",
              "depth": 3
            },
            {
              "text": "It's fantastic, the main Iboga practitioner I work with now mainly works with SR.Its a much easier process. Imidazenil has also come to the scene at much cheaper prices for benzo withdrawals.",
              "author": "jokowueu",
              "depth": 4
            },
            {
              "text": "To be honest I would prefer addicts could get heroin prescribed. The primary danger of street drugs is the inconsistent purity and chemicals it’s cut with. If it was pharmaceutical grade and everyone prescribed was on a list, we would have fewer overdoses and a better understanding of who to put in treatment",
              "author": "monero-xmr",
              "depth": 3
            }
          ]
        },
        {
          "top": "We really could use better treatments for chronic pain.\nI've found low dose naltrexone to be somewhat effective for severe chronic pain. Not as good as opiods.\nTHC can also help somewhat, but its action seems so dissociative. At an effective level for chronic pain, I'm sleepwalking though the day.\nOpioids or their analogues cause or complicate bowel issues. Four years of 200mg/day Tramadol really helped me, but it shredded my gut. Getting off Tramadol wasn't hard for me. I'd stay on it were it not for the gut issues.\nAs an aside, lacing hydrocodone with acetaminophen is truly a horrific practice. Doctors prescribe this to patients on hepotoxic drugs and are shocked when they get liver damage.",
          "author": "clcaev",
          "replies": [
            {
              "text": "I have 2 family members for whom Tramadol opened the door for severe addiction. One is now on regular morphine, the other had psychosis. I know it obvisouly depends on the individual,  just to dilute your very rosy comment",
              "author": "dgan",
              "depth": 1
            },
            {
              "text": "I didn't mean to make a rosy comment, thanks for the ribbing.\nTramadol isn't all that strong, but it does take the edge off. With a 6 week taper, my challenge was more about the resurgent pain.\nI would not recommend Tramadol, the gut complications are debilitating and it's unclear ignoring the chronic pain served me well.",
              "author": "clcaev",
              "depth": 2
            },
            {
              "text": "Right. Opioids are an absolute terror to one's digestive system. When I had chronic pain I would rather have just accepted the pain than deal with the gut consequences.\nLDN is an interesting one since it just stimulates your body to generate its own endorphins.",
              "author": "trollbridge",
              "depth": 1
            },
            {
              "text": "I found the first week on LDN to be challenging due to \"horror flick\" vivid dreams.\nLDN reduces \"central amplification\" of neuropathic pain, so it is a good fit for my disease profile.",
              "author": "clcaev",
              "depth": 2
            },
            {
              "text": "> I've found low dose naltrexone to be somewhat effective for severe chronic pain. Not as good as opiods.\nWhen I could get 7oh, it worked well for moderate break-thru (ibuprofen) pain (muscle, joint). I also tried a month of using it regularly wasn't happy overall. I didn't get any withdrawal on stopping tho.",
              "author": "WarOnPrivacy",
              "depth": 1
            }
          ]
        },
        {
          "top": "Is fentanyl even that big of an issue in a clinical setting? It's not like it's the go to opiate of choice for general pain anyway.\nThe problem with fentanyl is that it is easy to make and smuggle and we managed to leave a giant black market hole to be filled when we went ape shit about oxy, which was an objectively better situation than we are currently in with street opiates.",
          "author": "jfyi",
          "replies": [
            {
              "text": "Yes, it is an issue.\nOne of the big problems with anesthesia is balancing respiratory depression while medicating the patient enough to manage the symptoms. Fentanyl is used in anesthesia and it causes respiratory depression.\nA strong pain medication that doesn't slow or stop breathing would significantly improve the safety of anesthesia.",
              "author": "droopyEyelids",
              "depth": 1
            }
          ]
        },
        {
          "top": "It's a weird framing.  Fentanyl is already very safe in a healthcare setting.  It's only dangerous in off-label street use, where dosage is uncontrolled and use isn't being monitored by trained staff.  Do we think cartel labs are going to switch to a safer novel opiate?  I'm sure they don't care about any relevant patents, but they already have a pipeline/formulation for fentanyl.",
          "author": "loeg",
          "replies": [
            {
              "text": "They might, if it kept their customers coming back. They don't care about users' safety but they do want them to keep paying.",
              "author": "jfengel",
              "depth": 1
            }
          ]
        },
        {
          "top": "As a recurring kidney stone sufferer I am very thankful for fentanyl for my lithotripsy procedures. I hope we continue to make progress on effective pain medications and don't knee-jerk take them away.",
          "author": "leetrout",
          "replies": []
        }
      ]
    },
    {
      "id": "47182387",
      "title": "Show HN: Claude-File-Recovery, recover files from your ~/.claude sessions",
      "link": "https://github.com/hjtenklooster/claude-file-recovery",
      "domain": "github.com",
      "author": "rikk3rt",
      "score": 51,
      "comment_count": 17,
      "created_ts": 1772209582,
      "is_internal": false,
      "post_text": "Claude Code deleted my research and plan markdown files and informed me: “I accidentally rm -rf&#x27;d real directories in my Obsidian vault through a symlink it didn&#x27;t realize was there: I made a mistake. “<p>Unfortunately the backup of my documentation accidentally hadn’t run for a month. So I built claude-file-recovery, a CLI-tool and TUI that is able to extract your files from your ~&#x2F;.claude session history and thankfully I was able to recover my files. It&#x27;s able to extract any file that Claude Code ever read, edited or wrote. I hope you will never need it, but you can find it on my GitHub and pip. Note: It can recover an earlier version of a file at a certain point in time.<p>pip install claude-file-recovery",
      "is_ask_hn": false,
      "matched_keywords": [
        "pip",
        "mistake"
      ],
      "comments": [
        {
          "top": "Claude Code by default auto-deletes local chat/session logs after 30 days, so the claim that this tool can recover \"any file Claude Code ever read/edited/wrote\" is only true within that retention window unless you've explicitly changed the settings (\"cleanupPeriodDays\", see [1])\nSpeaking as someone who's derived a lot of value from these logs, it's a bit shocking that the default is to wipe them automatically!\n[1] \nhttps://simonwillison.net/2025/Oct/22/claude-code-logs/",
          "author": "aragonite",
          "replies": []
        },
        {
          "top": "That's funny. I wrote a blog post about something very similar.\nhttps://dextermiguel.com/posts/codex-helped-me-recover-lost-...",
          "author": "dimgl",
          "replies": []
        },
        {
          "top": "I had this happen yesterday to me, and Claude itself was able to recover it via the other conversations... I just had to tell it that it did the work and to find it in its other conversations.",
          "author": "gkoberger",
          "replies": []
        },
        {
          "top": "Could snapshots via tmutil be used to protect against this kind of thing on MacOS?\nhttps://derflounder.wordpress.com/2019/05/08/creating-managi...",
          "author": "TheKnack",
          "replies": []
        },
        {
          "top": "AI ran a git clean on me and wiped out a bunch of untracked changes.\nI just asked Claude Code to help recover it. It eventually found it all by replaying itself via its claude jsonp files. I never had to install or leave anything.",
          "author": "TIPSIO",
          "replies": []
        }
      ]
    },
    {
      "id": "47153207",
      "title": "Ask HN: Who Is Using XMPP?",
      "link": "https://news.ycombinator.com/item?id=47153207",
      "domain": "news.ycombinator.com",
      "author": "nunobrito",
      "score": 23,
      "comment_count": 11,
      "created_ts": 1772034821,
      "is_internal": true,
      "post_text": "Hello,<p>Are you using XMPP?<p>If so, what are your favorite servers to connect?",
      "is_ask_hn": true,
      "matched_keywords": [],
      "comments": [
        {
          "top": "I was I heavy XMPP user back in the 00s. But on the whole it never really took off, and when Google killed XMPP support in their chat it was a severe blow. As for why it didn’t reach a larger userbase, I think the reasons that Moxie described in \nThe ecosystem is moving\n[0] are spot on.\n[0] \nhttps://signal.org/blog/the-ecosystem-is-moving/",
          "author": "cpach",
          "replies": []
        },
        {
          "top": "I use JMP.chat[0] for my primary phone number.  Being able to text from my PC with a real keyboard is very convenient.  If I ever bite the bullet and use Discord I want to set up Slidcord[1] so that I don't have to use a separate app.  I'm still figuring out how to migrate people to XMPP natively.\n[0]: \nhttps://jmp.chat\n\n[1]: \nhttps://slidge.im/docs/slidcord/main",
          "author": "MarsIronPI",
          "replies": [
            {
              "text": "Doesn't Discord require every user to create a new account, even if they're already using Discord in some other community?  So when they say \"a Discord server\", they really mean it--they're like droplets running independent deployments or whatever (albeit managed/hosted instances run by and upgraded by Discord the company); not like subreddits, right?\nSo isn't the best way to migrate people to XMPP to prop up a Discord clone that's as close a copy as the Discord-clone community can manage, and then tell all the people \"Join my Discord server\", with the trick being that it's really \nyour\n server, not Discord's, and that server is powered by XMPP?",
              "author": "pwdisswordfishy",
              "depth": 1
            },
            {
              "text": "> Doesn't Discord require every user to create a new account, even if they're already using Discord in some other community? So when they say \"a Discord server\", they really mean it--they're like droplets running independent deployments or whatever (albeit managed/hosted instances run by and upgraded by Discord the company); not like subreddits, right?\nNo, it works exactly like subreddits.  One of the major lock-ins Discord has is that all the communities are there, and people access them all with one account and one app.  It's very convenient, which, as RMS says, is a bad thing when it comes to proprietary software.",
              "author": "MarsIronPI",
              "depth": 2
            },
            {
              "text": ">Doesn't Discord require every user to create a new account, even if they're already using Discord in some other community?\nNo, the same account is used. You get prompted to choose a nickname each time you join a server though.\n>So when they say \"a Discord server\", they really mean it--they're like droplets running independent deployments or whatever\nNo, they're probably all run on the same compute. The \"official\" name for a discord server from the API documentation is actually a \"discord guild\" and it works pretty much like a subreddit.",
              "author": "ycombinatrix",
              "depth": 2
            }
          ]
        },
        {
          "top": "Several years ago I put up an XMPP server (Prosody) to chat with some friends in a private group, and it's still going strong. Years ago we used to have a mailing-list to keep in touch, but that eventually went quiet. I had run an XMPP server ~10 years ago (ejabberd), but that was before mobile chat utterly displaced everything else, it never took hold among my friends, and I stopped using it when Google Talk stopped federating, cutting me off from most of the people I had been occassionally chatting with.\nThe modern chat experience is all about the clients, and the mobile clients in particular, especially push notification support and seamless setup, so direct comparisons with XMPP to Matrix, et al, kinda misses the point, IMO. Conversations is a really amazing Android client. Our one iPhone member is content with Modal (I use the desktop version sometimes, but it's clearly designed for the iPhone). A new member uses, I think, Gajim on Windows; they don't want the distraction of chatting on their phone.\nI host a bunch of other services on OpenBSD, all using the integrated base daemons--httpd, OpenSMTPD, NSD, etc--that sandbox themselves. I was hesitant to run a daemon like Prosody that didn't integrate OpenBSD security features, so I wrote my own module (mod_unveil) that uses pledge and unveil to sandbox Prosody: \nhttps://github.com/wahern/prosody-openbsd\n Most Prosody users host on Linux, and many of them seem to use Docker containers, which presumably offers some isolation, but I can't really speak to it.\nThe only non-private, large group chat I've joined recently has been the Prosody support MUC. I'm not a chat power user, but it seems to work just as well as any large chatroom. I've was content with ntalk and IRC back in the day, so I don't really get all the chat protocol bike shedding. In any event, I expect XMPP momentum (such as it is) to outlast all the interest in Discord, Matrix, etc.",
          "author": "wahern",
          "replies": []
        },
        {
          "top": "Last week I selfhosted Snikket: \nhttps://snikket.org\n, me and my partner use it to text. It has been smooth, everything works without issue: read receipts, audio/video calls, status.",
          "author": "0x5FC3",
          "replies": []
        },
        {
          "top": "I use Gajim which is a linux desktop implementation of XMPP.",
          "author": "ekjhgkejhgk",
          "replies": []
        }
      ]
    },
    {
      "id": "47160641",
      "title": "Ask HN: What's it like working in big tech recently with all the AI tools?",
      "link": "https://news.ycombinator.com/item?id=47160641",
      "domain": "news.ycombinator.com",
      "author": "ex-aws-dude",
      "score": 20,
      "comment_count": 9,
      "created_ts": 1772069350,
      "is_internal": true,
      "post_text": "Curious to hear how have things changed day-to-day with the recent push to use AI coding tools.<p>Have you noticed faster pace of development?<p>Have you seen changes to code quality or code review?<p>Do teammates that use these tools complete sprint tasks faster than those who don&#x27;t?",
      "is_ask_hn": true,
      "matched_keywords": [
        "big tech"
      ],
      "comments": [
        {
          "top": "I work in big-tech and we’re only allowed to use our companies internal tools that mimic Claude Code etc. There is certainly a push to use it, and I’ve noticed a significant increase in bugs that have come along. There are so many PRs now that no one reviews them and just slaps a LGTM. Specifically with junior or less strong engineers. It’s like they don’t code anymore and push everything on the AI and say ship it without a second thought. I’ve watched engineers push AI code with a nasty NPE and then they also had it write unit tests where the AI caught the NPE and passed the test. Literally if anyone spent the time reading the code you’d see it was a ticking bomb. We went from production breakages every couple of months to every couple of days.\n It’s so bad but no one is saying the hard thing that less strong engineers need to be restricted on their use of these tools.",
          "author": "aeriose",
          "replies": []
        },
        {
          "top": "I work for a $20 billion tech company (not a big tech company, but not a small one either). Our company strongly encourages us to incorporate agentic tools like Claude Code and Codex into our daily routines. Management has made it easy to use Anthropic and OpenAI products with an enterprise subscription. I can sense that our business leaders want us to move more swiftly and be more productive to avoid falling behind in the AI race.\nMy team has adopted Claude Code extensively, and the number of daily and weekly PRs we have closed has increased significantly. I’ve noticed that we’re also more willing to commit to more projects. My team benefits from most of our code being written in TypeScript. However, some other teams with legacy code bases seem to have a bit of a harder time using these tools compared to us.\nOne thing that surprises me with a AI is you can have people working on the same code base. Some can be very effective with AI, yet there are others working on the exact same code base who cannot get good results. Some people don’t really seem to be taking the time to get good at writing prompts and plans before having agents execute on them.",
          "author": "cebert",
          "replies": [
            {
              "text": "Your experiences reflect mine.\nMy employer has a slack channel where we can share prompts, tips about how to use AI outside of coding, Q&A. I've learned a lot from my coworkers there.",
              "author": "rithdmc",
              "depth": 1
            }
          ]
        },
        {
          "top": "I work for a $50B+ company (is this big tech? idk), but I’ll answer this because we are fully embracing AI (Cursor, Claude Code, cloud agents, AI reviews, you name it)\n> Have you noticed faster pace of development?\nYes, our org has had a 50% increase in PRs since Opus 4.5 released.\n> Have you seen changes to code quality or code review?\nYes, significantly more bugs (no exact number), but consider it maybe 3-4x in volume. However, nothing catastrophic and everyone just uses AI for fast-follow fixes anyways. The company as a whole is embracing this style of development for better or worse.\n> Do teammates that use these tools complete sprint tasks faster than those who don't?\nYes, but my entire team uses them. I’d say the ones who use it more effectively (crazy skill setups, better tooling/commands, better scaffolding) finish much faster. Probably 80% of my team still uses Cursor in the one-shot way with very vague requirements, and don’t have the AI connected to github, jira, slack, etc which can actually feed really important context into decision making.\nIf I do something more than once a day, I write a custom slash command for it. This has personally 2x’d my pace.",
          "author": "with",
          "replies": []
        },
        {
          "top": "The biggest shift isn't the speed of coding, but the shift in 'seniority' expectations. You're no longer just a writer of code; you're an editor of high-volume output. The mental fatigue has shifted from 'how do I solve this' to 'is this generated solution actually robust for our scale'. Big tech feels more like orchestrating a fleet of junior agents than solo deep-work now.",
          "author": "CodeBit26",
          "replies": []
        },
        {
          "top": "I work at a large social media company - it has become a popularity contest on who can ship more AI slop the fastest. Real productivity gains are questionable tbh. Slop is replacing more slop.",
          "author": "umairnadeem123",
          "replies": []
        }
      ]
    },
    {
      "id": "47132766",
      "title": "Would you choose the Microsoft stack today if starting greenfield?",
      "link": "https://news.ycombinator.com/item?id=47132766",
      "domain": "news.ycombinator.com",
      "author": "JB_5000",
      "score": 20,
      "comment_count": 23,
      "created_ts": 1771906275,
      "is_internal": true,
      "post_text": "Serious question.<p>Outside government or heavily regulated enterprise, what is Microsoft’s core value prop in 2026?<p>It feels like a lot of adoption is inherited — contracts, compliance, enterprise trust, existing org gravity. Not necessarily technical preference.<p>If you were starting from scratch today with no legacy, no E5 contracts, no sunk cost — how many teams would actually choose the full MS stack over best-of-breed tools?<p>Curious what people here have actually chosen in greenfield builds.",
      "is_ask_hn": false,
      "matched_keywords": [
        "enterprise"
      ],
      "comments": [
        {
          "top": "DotNet 10 allows me to create a single web application for all three major platforms, and to ship it with the runtime baked in. This allows me to host said application anywhere I want.\nIf talking about non-web desktop/mobile software, the big issue comes down to GUI framework. Maui is optimized for mobile, Avalonia is superior on desktop. But other than that, thanks to the baked-in runtime it is becoming trivial to build a single GUI program and have it just run anywhere without significant rework.\nPlus, C# has become an absolute pleasure to work with, and makes Java look practically obsolete. Hell, Java had to roll back its own implementation of string interpolation in 2024 because they just couldn’t make it work. C#? Working beautifully since 2015, and improved several times to even greater effect.\nAnd if C#’s functional programming rustles your jimmies, you can easily use F# in the same project without complaint.\nFinally, while Blazor webassembly might not be entirely ready for world+dog prime time just yet, it works great in an intranet where thick pipes are guaranteed, allowing me to give a hearty middle finger to everything JavaScript.\nHonestly, while I will always evaluate every project to ensure that the programming platform is the best one possible, and have chosen other languages for good reasoning plenty of times. But anything which falls in the “anything will work well enough, generally speaking” bin has me reaching for DotNet almost every time.",
          "author": "rekabis",
          "replies": []
        },
        {
          "top": "I never chose MS, ever. Most systems have been based on Debian servers. Some special needs I've used Alpine Linux and NetBSD (yes it does run on older, limited hardware).\nOf course, some potential clients demand MS based solutions. I simply decline those contracts and ring them up 12-18 months later to see if the CTO/CIO has been replaced.",
          "author": "burntoutgray",
          "replies": []
        },
        {
          "top": "I know it's not popular, but I think Powershell returning objects is quite nice. It seems much better/easier than trying to parse raw text in Linux.\nThat said, if I'm making something new, I never go with an MS stack. I just wish I had some Powershell type options in Linux from time to time.",
          "author": "al_borland",
          "replies": [
            {
              "text": "PowerShell is available for Linux and MacOS.  Which means you can have the tool you like and run on a non-MS platform.",
              "author": "burntoutgray",
              "depth": 1
            },
            {
              "text": "I know it's available, but I always assumed it would be a 2nd class citizen.\nI just tried to install in on my personal macOS system. I tried brew, as Microsoft gives instructions for it...\n> Warning: powershell has been deprecated because it does not pass the macOS Gatekeeper check! It will be disabled on 2026-09-01.\nIt went on to do more and then prompted for my password, but I canceled out of it. I'm not looking to get started with anything that starts with a deprecation warning. This is what I'm talking about when I say it feels like a 2nd class citizen. I can only assume I can't use 100% Powershell to admin a system, like one could with Windows.",
              "author": "al_borland",
              "depth": 2
            }
          ]
        },
        {
          "top": "What's the target?\nIf you're deploying applications to desktop/laptop, Microsoft stack seems like the biggest market share by a lot.\nAnything else, probably not. Microsoft may have advantages in servers, or at least they have had some in the past: my go to example is Receive Side Scaling [1] was originated at Microsoft and is amazing in some use cases. But those advantages most likely aren't big enough to justify the hassle of wrangling and paying for licenses. Running your servers on open source software makes everything easier.\n[1] \nhttps://learn.microsoft.com/en-us/windows-hardware/drivers/n...",
          "author": "toast0",
          "replies": []
        },
        {
          "top": "I chose .NET and C# for my current startup (2 years in), where I'm the founding lead developer, and I'd still make the same call today. We've been very productive, built a lot of solid features, and two years in we have a stable, high-performing set of apps. We're using ASP.NET Razor Pages, Web API, backend services, and a .NET MAUI Blazor Hybrid app that's published to the app stores - all written in C# using the latest .NET version. Some parts run in Azure, other parts run on high-performance GPU servers on Ubuntu. It has felt like a very practical modern stack, not some legacy-only enterprise choice. Also, hiring has been a non-issue as I had no trouble finding solid C# developers.",
          "author": "randyburden",
          "replies": []
        }
      ]
    },
    {
      "id": "47130974",
      "title": "Ask HN: What Linux Would Be a Good Transition from Windows 11",
      "link": "https://news.ycombinator.com/item?id=47130974",
      "domain": "news.ycombinator.com",
      "author": "Cyberis",
      "score": 13,
      "comment_count": 20,
      "created_ts": 1771891880,
      "is_internal": true,
      "post_text": "I have users who glaze over the minute I mention &quot;notepad.&quot; I think they can barely use Windows. But our work requires a level of privacy (regulatory and otherwise) and Windows 11 is just one big data transmitter. I know this is flamebait, but I&#x27;d love suggestions for a Linux desktop that looks like Windows, is stable and easy to administer and harden, and works with Dell business grade laptops that we bought new in 2025.",
      "is_ask_hn": true,
      "matched_keywords": [],
      "comments": [
        {
          "top": "Don't overthink it. If you're coming from Windows, go with Linux Mint. It’s the only one that doesn't feel like a constant battle against your own muscle memory. Once you're comfortable, you can jump into Fedora or Arch, but for the first month, you just want your computer to work without a terminal hunt.",
          "author": "CodeBit26",
          "replies": [
            {
              "text": "As a long time user of Damn Small Linux, Slackware, SUSE, Gentoo, and Debian, I frequently test distros for security against various cybersecurity standards (starting with Common Core and CISecurity), I also implement Linux across CPU architectures for various embedded IoT:\nLinux Mint is what I always put on for new family members.\nDisclaimer: original investor of RedHat.",
              "author": "egberts1",
              "depth": 1
            },
            {
              "text": "That’s a solid endorsement, especially coming from someone with your background in RedHat and the older distros. It seems we agree that for the 'non-tech' family members or those just transitioning, Mint is the bridge that actually stays standing. The stability for IoT you mentioned is a great point too—often overlooked in favor of more 'exciting' but fragile setups.",
              "author": "CodeBit26",
              "depth": 2
            }
          ]
        },
        {
          "top": "Instead of a specific technical answer to your question one thing I would consider if regulatory bodies are involved would be to look for existing hardening documents, scripts, tools from your auditors and see if there is a common pattern for OS choices that easily check all the boxes.  Ask your auditors which OS makes audits easier for them and which of the hardening tools cause the least grief, require the least exceptions before looking at technical options.   Just a suggestion from someone that may as well have moved in with the auditors for spending so much time with them.\nAfter narrowing it down to 3 choices then present those choices to:\n- Your legal team to review licenses before you put much effort into setting up automation frameworks, support tools, installation automation.  They can be a buzzkill and I think some may secretly enjoy it.\n- The people using that which you plan to administer.  Let them play around with each option and get their feedback to maybe have happier group(s) of people to support.  Test group 1, test group 2, test group 3.  Let them compare and contrast.",
          "author": "Bender",
          "replies": []
        },
        {
          "top": "Linux Mint\nhttps://forums.linuxmint.com/viewtopic.php?t=427188\n(imho, ymmv, n=1)",
          "author": "toomuchtodo",
          "replies": [
            {
              "text": "Yes, Linux Mint.  After discovering SLS Linux way back in the dawn of Linux-time, trying Slackware, CentOS, Ubuntu and others, my daily-driver is now Linux Mint.\nhttps://distrowatch.com/table.php?distribution=mint",
              "author": "rzzzwilson",
              "depth": 1
            },
            {
              "text": "+1 on Mint, specifically with the Cinnamon desktop environment for people leaving Windows",
              "author": "idw",
              "depth": 1
            }
          ]
        },
        {
          "top": "Zorin was trending here a few weeks ago. I installed it on a few machines. It is super slick. I didn’t try the Mac skin but the default windows one is finally a Linux that looks like a professionally built operating system.\nI actually went back to headless Ubuntu lts after playing around with it because everything I do is headless but I really liked it while I was using it. Debian based, I had no complaints.",
          "author": "tim-tday",
          "replies": []
        },
        {
          "top": "Ubuntu. Its great. So much cleaner and userfrienly then Microsoft. Definitely don't need to be a dev to work in Ubuntu anymore. Honestly, I don't know how microsoft is holding its base.",
          "author": "JB_5000",
          "replies": []
        }
      ]
    },
    {
      "id": "47138336",
      "title": "Ask HN: Who has seen productivity increases from AI",
      "link": "https://news.ycombinator.com/item?id=47138336",
      "domain": "news.ycombinator.com",
      "author": "Kapura",
      "score": 12,
      "comment_count": 17,
      "created_ts": 1771946684,
      "is_internal": true,
      "post_text": "I would love examples of positions and industries where AI has been revolutionary. I have a friend at one of the largest consulting firms who has said it&#x27;d been a game-changer in terms of processing huge amounts of documentation over a short period of time. Whether or not that gives better results is another question, but I would love to hear more stories of AI actually making things better.",
      "is_ask_hn": true,
      "matched_keywords": [
        "consulting"
      ],
      "comments": [
        {
          "top": "One data point from the extreme end: gave Claude Code $0 budget and let it run autonomously every 2 hours for 6 days.\nProductivity at tasks: yes — it built 2 full products from scratch, ran DB migrations, wrote SEO pages, set up analytics, deployed everything on its own.\nProductivity at outcomes: no — still at $0 revenue after 18 runs. It kept optimizing things nobody was using.\nThe key insight: AI productivity is massive when the task is well-defined. But left to its own judgment, it tends to choose building over selling, and polishing over distribution. Same trap junior engineers fall into.\nFull writeup: \nhttps://dev.to/wpmultitool/my-ai-agent-has-been-trying-to-ma...",
          "author": "agenthustler",
          "replies": []
        },
        {
          "top": "I'm an engineer in the oil and gas industry. Some of my job involves messy judgement calls that I would never involve LLMs in, but some of my job boils down to integrating different items of data that exist across documents, drawings and a few databases that are in different formats and don't cross reference each other. At times I have used LLMs as a kind of \"highly enhanced search engine\" to do semantic search across documentation of every different types. My alternative was opening each document and using ctrl+f, along with my intuition of knowing what document titles to search for.\nFor a more concrete example, I have an interface to the data that comes from every sensor on the oil processing facility. It has a built in \"AI\" (I try not to use that term!) but it has a feature where I ask how to process data in plain language and it'll give me the calculations, then it'll also provide a plain language summary of all the calculations I conducted. That saved me 10 hours of work.\nI am a negative nancy on LLMs in general but I still passionately believe that they're a tool which every white collar employee will need to learn to use effectively.\nI cringe when I hear engineers say \"I didn't know the answer so I asked ChatGPT\" but I also do worry that I could be significantly outperformed by another engineer with 10 years less experience in engineering and 1 year more experience in judicious use of LLMs.",
          "author": "iamthemonster",
          "replies": [
            {
              "text": "What's an example of a messy judgement call?",
              "author": "iridione",
              "depth": 1
            }
          ]
        },
        {
          "top": "I find that I mostly use AI to compensate for Google search becoming unusable.\nI use it more and more as advanced API documentation and writing code snippet. It made me a little bolder, and reduced friction from coding. I will test Claude Code in the upcoming weeks.",
          "author": "nicbou",
          "replies": []
        },
        {
          "top": "Me! I'm in a leadership position at a marketing agency. I'm a self-taught programmer (just found it interesting and started on my TI-83 in high school doing BASIC and then some Z80 assembly) and studied creative writing in college.\nI used to hand-write a lot of the scripts we used to automate processes (mostly Google Apps Scripts and Python). It was very helpful to us but slow going and sometimes frustrating.\nIn the past, I was limited by my programming knowledge and ability. Now I am limited by my knowledge of our business and ability to explain what I want to happen.\nWith LLMs, I can generally 1-shot most things I want to automate, including things that would've taken me days to figure out in the past. I generally just say that I'll spend 30 minutes on something and more often than not, it's either done or very close at the end of that time.",
          "author": "paulcole",
          "replies": []
        },
        {
          "top": "Three key things in coding which has helped recently:\n1) Debug mode from Cursor. It highlights all possible hypotheses based on the steps to reproduce and whatever you know from the code. It slaps down logs and tests all these hypotheses at once.\n2) Log reading, though this is a variant of the above. At times we get massive logs, like 10k lines for a bug. We go through what may possibly cause the bug - what screens, threads, race conditions, improper handling of ANY_KEY or default or something. Then we ask Opus to compare the logs to see if it matches the story. I ain't manually checking Line 314, Line 500, Line 44 to see if it shows in logs, but AI is just great at piecing together this whodunit.\n3) Finding documentation. So there's been a few bugs unsolved around how very specific Android devices handle wifi reconnection or bluetooth or Ethernet. Instead of making assumptions, I can dig into the AOSP source code directly and see the conditions that trigger these. AI will even point out the exact lines.",
          "author": "muzani",
          "replies": []
        }
      ]
    },
    {
      "id": "47146212",
      "title": "New Claude Code Feature \"Remote Control\"",
      "link": "https://news.ycombinator.com/item?id=47146212",
      "domain": "news.ycombinator.com",
      "author": "rob",
      "score": 9,
      "comment_count": 0,
      "created_ts": 1771983624,
      "is_internal": true,
      "post_text": "No more tmux&#x2F;Tailscale-type stuff needed now?",
      "is_ask_hn": false,
      "matched_keywords": [
        "remote"
      ],
      "comments": []
    },
    {
      "id": "47152724",
      "title": "Ask HN: Starting a New Role with Ada",
      "link": "https://news.ycombinator.com/item?id=47152724",
      "domain": "news.ycombinator.com",
      "author": "NoNameHaveI",
      "score": 9,
      "comment_count": 4,
      "created_ts": 1772032859,
      "is_internal": true,
      "post_text": "So, good news. After a unexpectedly long absence from employment, I am 95% certain that I will receive an offer for a contract job as a product owner. This position will largely involve supervising the development&#x2F;maintenance of code written in Ada. Even though I have over a decade of experience with C&#x2F;C++&#x2F;Assembly, I have ZERO experience with Ada. I doubt I will be writing much Ada myself, but I believe I will need to learn Ada.<p>So here are my questions:<p>1. Reading code is usually pretty straightforward. However, all software requires domain knowledge. When starting a new role, how do you bring yourself up on domain knowledge quickly?<p>2. If you know Ada, what resources to learn Ada do you recommend?<p>3. What Ada pitfalls do you advise to look out for?",
      "is_ask_hn": true,
      "matched_keywords": [],
      "comments": [
        {
          "top": "1. Ask questions, and write down the answers in a way that you will find them again. Anki and spaced repetition is useful to learn the terminology or any info that isn't intuitive.\n2. \nhttps://github.com/ohenley/awesome-ada\n has links to pretty much every Ada topic and resource; if you want to try Ada using open source tools, the best starting point is \nhttps://alire.ada.dev/docs/\n3. Compared to C/C++ I can't really think of any pitfalls. It requires more discipline and formal reasoning, but you will get used to it (and appreciate the lack of footguns, at least I did).\nCongrats and good luck.",
          "author": "phba",
          "replies": [
            {
              "text": "Just need to check if it's plain Ada or one specific profile or  SPARK",
              "author": "Davidbrcz",
              "depth": 1
            }
          ]
        },
        {
          "top": "I would try Claude Code with an Ada LSP server. Use AI to get the high-level architecture diagrams, question the codebase. You can write a new skill with all your findings about Ada best practices. Also try CodeRabbit if it already supports Ada. Congratulations on the new job, best of luck.",
          "author": "ezulabs",
          "replies": []
        },
        {
          "top": "I learned Ada in 1990. It looks like it was based on Pascal. I used Janus Ada for DOS. There is a free Learn Ada ebook here: \nhttps://learn.adacore.com/pdf_books/courses/intro-to-ada.pdf",
          "author": "orionblastar",
          "replies": []
        }
      ]
    },
    {
      "id": "47160050",
      "title": "Ask HN: Could you create a competitor to your company at 10% of the cost?",
      "link": "https://news.ycombinator.com/item?id=47160050",
      "domain": "news.ycombinator.com",
      "author": "TheAlchemist",
      "score": 9,
      "comment_count": 14,
      "created_ts": 1772064960,
      "is_internal": true,
      "post_text": "I&#x27;m trying to wrap my mind about the AI tools, and while I believe there is way too much hype, I&#x27;m quite impressed with the progress.<p>The current mood seem to be that big companies will automate away many white collar jobs and just get bigger profits. My question is - what if it&#x27;s the other way around ? Could said white collar workers just spin off competitors much more easily than before ? Obviously this mostly apply to software, but I&#x27;m curious what people think about it in all industries.",
      "is_ask_hn": true,
      "matched_keywords": [],
      "comments": [
        {
          "top": "Technically speaking? Sure, most of the companies I've worked at aren't exactly doing rocket science. Creating media websites and doing work for clients as a marketing company are things that have a very low barrier to entry.\nThe challenge is that in most of these cases, success is based on trust and long-term reputation building rather than pure technical skill. And doing a good job in either industry requires a lot more time, resources and effort than a single person may be able to provide.\nSo, while I could definitely spin up some very similar sites, creating the actual content at a level of quality that people would even bother to read would be a chore, as would finding an audience in general.",
          "author": "CM30",
          "replies": []
        },
        {
          "top": "Technically? Probably. But the 10% cost isn't where the battle is. It's the 90% trust and 'organizational gravity' that legacy companies hold. You can replicate the stack with AI agents in a weekend, but you can't replicate the compliance, the sales relationships, and the 'nobody ever got fired for buying IBM' safety net. AI has lowered the floor for entry, but it hasn't lowered the ceiling for enterprise trust.",
          "author": "CodeBit26",
          "replies": []
        },
        {
          "top": "A toy-version MVP might be cheap. What happens when you need to deploy it, scale it, secure it, support it, upgrade it, market it, etc.\nA large company is more than a single GitHub repo.",
          "author": "al_borland",
          "replies": [
            {
              "text": "Exactly. This is like the greenfield vs brownfield or 'Twitter clone in a weekend' arguments. Cost and complexity grow drastically with success and scale.",
              "author": "radioshakeshack",
              "depth": 1
            },
            {
              "text": "I know that, that's why I'm asking the question. But if cost and complexity grow drastically with scale, my question is even more relevant, isn't it ? \nIf it's easy to spin up a competitor, we should rather see more small companies, not less.",
              "author": "TheAlchemist",
              "depth": 2
            },
            {
              "text": "From a pure numbers standpoint, absolutely. We're already seeing a wave of folks shipping more PoC products with AI tools. The distinction is that most of these aren't true \"competitors\" to established companies, more filling gaps and niches if they aren't just a toy clone.\nThat's the more interesting angle here to me. Rather than building a direct competitor at 10% the cost, agent assisted tooling could make it individually profitable to target small cap, cottage industry type problems that have too much nuance for a one size product. Areas where someone already has deep business insight that provides the value proposition moreso than the labor around coding. Not competitors at 10% the cost, but filling niches that wouldn't have been profitable otherwise.\nThe danger is that if agents get more capable, the niche markets start to erode. If a generic solution agent can acquire domain knowledge and coordinate with customers, even these 'competitors' become risky again.",
              "author": "radioshakeshack",
              "depth": 3
            },
            {
              "text": "There have always been a million small POC clones of popular apps. Twitter, Trello, <insert popular todo app>, etc.\nDoing a subset of what a market leader is doing, but worse, with no path to scale or support, isn’t going to go anywhere.\nYesterday someone vibe coded a password manager in 20 minutes and posted it here. Should anyone use it? Absolutely not. It’s a security nightmare, won’t get support, and the architecture is complete trash, requiring the use to run a local server the background that the app and browser extension talk to. Not to mention it will likely never see an update.\nSuccessful competitors do something new. A lazy vibe coded clone doesn’t nothing new, and they don’t even do all the basics.",
              "author": "al_borland",
              "depth": 3
            }
          ]
        },
        {
          "top": "As a former JavaScript developer yes I could reduce costs by 99%, but it’s not what business wants. They want the able to hire/fire people and not train them, so there are your inefficiencies: a bunch of lower confidence people that need a lot of hand holding.",
          "author": "austin-cheney",
          "replies": []
        },
        {
          "top": "I think you're onto something.  I started my own firm on a shoe-string compared to my boss' existing company and have been doing quite well.  The hardest thing may be lead generation, but I poached a bunch of those too.",
          "author": "Eaglo",
          "replies": []
        }
      ]
    },
    {
      "id": "47135806",
      "title": "Ask HN: Any DIY open-source Alexa/Google alternatives?",
      "link": "https://news.ycombinator.com/item?id=47135806",
      "domain": "news.ycombinator.com",
      "author": "personality0",
      "score": 9,
      "comment_count": 4,
      "created_ts": 1771932767,
      "is_internal": true,
      "post_text": "I&#x27;m looking to replace my Alexa with an alternative where I can use a realtime model like Gemini or an STT -&gt; LLM -&gt; TTS pipeline. Should be easy to build with an Arduino or I&#x27;d even be happy buying an already made solution.<p>Basic functions should include playing Spotify, asking questions, settings timers.",
      "is_ask_hn": true,
      "matched_keywords": [],
      "comments": [
        {
          "top": "The ESP32S3 has wake word support: \nhttps://components.espressif.com/components/espressif/esp-sr...\nThe rest is just some vibe coding…",
          "author": "iamflimflam1",
          "replies": [
            {
              "text": "If it's possible via vibe coding, then there are a few projects out there that do just exactly that.",
              "author": "worldsavior",
              "depth": 1
            }
          ]
        },
        {
          "top": "Perhaps \nhttps://rhasspy.readthedocs.io/en/latest/",
          "author": "wonger_",
          "replies": []
        },
        {
          "top": "I believe home assistant, is/was working on a physical products for this, but no idea if it available yet",
          "author": "Zekio",
          "replies": []
        }
      ]
    },
    {
      "id": "47175746",
      "title": "Ask HN: How do you handle duplicate side effects when jobs, workflows retry?",
      "link": "https://news.ycombinator.com/item?id=47175746",
      "domain": "news.ycombinator.com",
      "author": "shineDaPoker",
      "score": 8,
      "comment_count": 8,
      "created_ts": 1772160302,
      "is_internal": true,
      "post_text": "Quick context: I&#x27;m building background job automation and keep hitting this pattern:<p>1. Job calls external API (Stripe, SendGrid, AWS)\n2. API call succeeds\n3. Job crashes before recording success\n4. Job retries → calls API again → duplicate<p>Example: process refund, send email notification, crash. Retry does both again. Customer gets duplicate refund email (or worse, duplicate refund).<p>I see a few approaches:<p>Option A: Store processed IDs in database\nProblem: Race between &quot;check DB&quot; and &quot;call API&quot; can still duplicate<p>Option B: Use API idempotency keys (Stripe supports this)\nProblem: Not all APIs support it (legacy systems, third-party)<p>Option C: Build deduplication layer that checks external system first\nProblem: Extra latency, extra complexity<p>What do you do in production? Accept some duplicates? Only use APIs \nwith idempotency? Something else?<p>(I built something for Option C, but trying to understand if this is actually a common-enough problem or if I&#x27;m over-engineering.)",
      "is_ask_hn": true,
      "matched_keywords": [
        "success"
      ],
      "comments": [
        {
          "top": "Idempotency is the only sustainable answer here. Whether it's at the database level using unique constraints or implementing idempotency keys in your API headers, you have to design for the 'at-least-once' delivery reality. I usually implement a 'processed_requests' table that stores the unique ID of the job. Before the worker executes any side effect (like a payment or email), it checks if that ID exists. If it does, it skips the execution and returns the previous result. It adds a bit of latency, but it's much cheaper than dealing with double-billing or corrupted data",
          "author": "codebitdaily",
          "replies": []
        },
        {
          "top": "Use something like Temporal",
          "author": "babelfish",
          "replies": [
            {
              "text": "I actually talked to someone in temporal about this recently. Temporal gives you the primitives to handle it (activities, configure retries, interceptors), but you still have to implement the deduplication logic yourself for each external integration.\nHis advice was: Temporal solves orchestration, but making the external API calls idempotent is on you. For simple cases, write observe activities manually. For complex cases, build abstraction.\nThat's what led me down this path - trying to figure out if the abstraction is worth building or if manual is good enough.\nHave you used Temporal for this? How do you handle the idempotency of external calls?",
              "author": "shineDaPoker",
              "depth": 1
            }
          ]
        },
        {
          "top": "You proxy those api calls yourself and have idempotency to cover you for those APIs that don’t have it. If you architect it right you won’t have more than a ms latency addition. You can avoid the race condition issues by using atomic records so if something else tries they’d see it’s in progress and exit.",
          "author": "moomoo11",
          "replies": [
            {
              "text": "This is exactly the approach I took. Proxy layer that:\n- Uses atomic records (fence tokens) to prevent concurrent execution\n- Checks external system first before retrying (the retrieval step)\n- Records result for future lookups\nThe atomic records part is critical - I learned the hard way that just checking a DB flag isn't enough (process can freeze between check and execute, lease expires, another process takes over, both execute).\nHow do you handle the case where:\n1. Process acquires atomic lock\n2. Calls external API successfully  \n3. Process freezes before releasing lock\n4. Lock expires, new process acquires it\n5. New process calls API again → duplicate\nDo you just accept this edge case (rare but possible)? Or is there a mitigation I'm missing?",
              "author": "shineDaPoker",
              "depth": 1
            },
            {
              "text": "yeah, this is a common edge case in this scenario and as you say it will be rare but possible.\nhowever i don't think its that complicated (idk your system obv, so I could be off base) but i think your best bet is to not retry blindly and instead just query to see if already happened. If it already happened you're good and if not you handle it appropriately.\nconsidering it will be rare i don't think handling it how i suggest it will affect you that badly. keep it simple should be your priority imo with these kinds of operations.",
              "author": "moomoo11",
              "depth": 2
            }
          ]
        },
        {
          "top": "I think the answer is probably like most things: it depends.\n- If the external service supports idempotent operations, use that option.\n- If the external service \ndoesn't\n, but has a \"retrieval\" feature (i.e. lookup if the thing already exists, e.g fetch refunds on a given payment), use that first.\n- If the system has neither, assess how critical it is to avoid duplicates.",
          "author": "stephenr",
          "replies": [
            {
              "text": "This matches my thinking. The retrieval/lookup approach is exactly what I built - basically Option C with an observe-before-act pattern.\nFor APIs that support idempotency keys (Stripe, etc.), I use those. For ones that don't but have retrieval (most do), I check first before retrying.\nThe question I'm wrestling with: is the extra round-trip for the lookup worth it? Or should I just accept the edge cases where it duplicates?\nWhat's your threshold for \"critical enough to avoid duplicates\"? Payments obviously yes, but what about notifications, reporting, analytics events?",
              "author": "shineDaPoker",
              "depth": 1
            }
          ]
        }
      ]
    }
  ]
}